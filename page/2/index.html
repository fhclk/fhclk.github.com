<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"fhclk.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="拾荒者">
<meta property="og:url" content="http://fhclk.github.io/page/2/index.html">
<meta property="og:site_name" content="拾荒者">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="fhclk">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://fhclk.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>拾荒者</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">拾荒者</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">昨夜西风凋碧树，独上高楼，望尽天涯路</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/03/18/Kafka%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F%E9%9B%86%E6%88%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/18/Kafka%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F%E9%9B%86%E6%88%90/" class="post-title-link" itemprop="url">Kafka外部系统集成</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-18 21:02:14" itemprop="dateCreated datePublished" datetime="2022-03-18T21:02:14+08:00">2022-03-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-02-02 17:54:53" itemprop="dateModified" datetime="2023-02-02T17:54:53+08:00">2023-02-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="集成Flume"><a href="#集成Flume" class="headerlink" title="集成Flume"></a>集成Flume</h1><p><img src="/2022/03/18/Kafka%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F%E9%9B%86%E6%88%90/image-20230202132247145.png" alt="image-20230202132247145"></p>
<h2 id="Flume生产者"><a href="#Flume生产者" class="headerlink" title="Flume生产者"></a>Flume生产者</h2><p><img src="/2022/03/18/Kafka%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F%E9%9B%86%E6%88%90/image-20230202132414125.png" alt="image-20230202132414125"></p>
<ol>
<li><p>启动kafka集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# ./bin/zk.sh start</span><br><span class="line">(base) [root@node1 ~]# ./bin/kf.sh start</span><br></pre></td></tr></table></figure>


</li>
<li><p>启动kafka消费者</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic first</span><br></pre></td></tr></table></figure>


</li>
<li><p>配置flume</p>
<p>文件名file_to_kafka.conf</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 组件定义</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 2 配置 source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">TAILDIR</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups</span> = <span class="string">f1</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.f1</span> = <span class="string">/root/data/applog/app.*</span></span><br><span class="line"><span class="attr">a1.sources.r1.positionFile</span> = <span class="string">/root/data/flume/taildir_position.json</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 3 配置 channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 4 配置 sink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">org.apache.flume.sink.kafka.KafkaSink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.topic</span> = <span class="string">first</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.flumeBatchSize</span> = <span class="string">20</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.acks</span> = <span class="string">1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.linger.ms</span> = <span class="string">1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 5 拼接组件</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>启动flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 flume]# bin/flume-ng agent -c conf/ -n a1 -f job/file_to_kafka.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>


</li>
<li><p>向&#x2F;root&#x2F;data&#x2F;app.log 里追加数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# echo hello &gt;&gt; /root/data/applog/app.log</span><br><span class="line">(base) [root@node1 ~]# echo hello flume &gt;&gt; /root/data/applog/app.log</span><br></pre></td></tr></table></figure>


</li>
<li><p>查看 kafka 消费者消费情况</p>
<p><img src="/2022/03/18/Kafka%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F%E9%9B%86%E6%88%90/image-20230202160913784.png" alt="image-20230202160913784"></p>
</li>
</ol>
<h2 id="Flume消费者"><a href="#Flume消费者" class="headerlink" title="Flume消费者"></a>Flume消费者</h2><p><img src="/2022/03/18/Kafka%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F%E9%9B%86%E6%88%90/image-20230202161059358.png" alt="image-20230202161059358"></p>
<ol>
<li><p>配置flume</p>
<p>文件名：kafka_to_file.conf</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 组件定义</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 2 配置 source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">org.apache.flume.source.kafka.KafkaSource</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">50</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchDurationMillis</span> = <span class="string">200</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.bootstrap.servers</span> = <span class="string">node1:9092</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.topics</span> = <span class="string">first</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.consumer.group.id</span> = <span class="string">custom.g.id</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 3 配置 channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 4 配置 sink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>启动flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 flume]# bin/flume-ng agent -c conf/ -n a1 -f job/kafka_to_file.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>


</li>
<li><p>启动kafka生产者</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-producer.sh --bootstrap-server node1:9092 --topic first</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello flume</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello hadoop</span></span><br><span class="line"><span class="meta prompt_">&gt;</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>观察控制台输出</p>
<p><img src="/2022/03/18/Kafka%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F%E9%9B%86%E6%88%90/image-20230202163426516.png" alt="image-20230202163426516"></p>
</li>
</ol>
<h1 id="集成Flink"><a href="#集成Flink" class="headerlink" title="集成Flink"></a>集成Flink</h1><p><img src="/2022/03/18/Kafka%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F%E9%9B%86%E6%88%90/image-20230130220442486.png" alt="image-20230130220442486"></p>
<h1 id="集成SpringBoot"><a href="#集成SpringBoot" class="headerlink" title="集成SpringBoot"></a>集成SpringBoot</h1><p><img src="/2022/03/18/Kafka%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F%E9%9B%86%E6%88%90/image-20230130220559057.png" alt="image-20230130220559057"></p>
<h1 id="集成Spark"><a href="#集成Spark" class="headerlink" title="集成Spark"></a>集成Spark</h1><p><img src="/2022/03/18/Kafka%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F%E9%9B%86%E6%88%90/image-20230130220639873.png" alt="image-20230130220639873"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/03/17/Kafka-Kraft%E6%A8%A1%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/17/Kafka-Kraft%E6%A8%A1%E5%BC%8F/" class="post-title-link" itemprop="url">Kafka-Kraft模式</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-17 21:10:01" itemprop="dateCreated datePublished" datetime="2022-03-17T21:10:01+08:00">2022-03-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-30 21:55:16" itemprop="dateModified" datetime="2023-01-30T21:55:16+08:00">2023-01-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Kafka-Kraft架构"><a href="#Kafka-Kraft架构" class="headerlink" title="Kafka-Kraft架构"></a>Kafka-Kraft架构</h1><p>Kafka 现有架构中，元数据在 zookeeper 中，运行时动态选举 controller，由controller 进行 Kafka 集群管理。</p>
<p>Kafka-kraft 模式架构（实验性），不再依赖 zookeeper 集群，而是用三台 controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进行 Kafka 集群管理。</p>
<p>这样做的好处有以下几个：</p>
<ul>
<li>Kafka 不再依赖外部框架，而是能够独立运行；</li>
<li>controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升；</li>
<li>由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制；</li>
<li>controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策。</li>
</ul>
<h1 id="Kafka-Kraft-集群部署"><a href="#Kafka-Kraft-集群部署" class="headerlink" title="Kafka-Kraft 集群部署"></a>Kafka-Kraft 集群部署</h1><ol>
<li><p>再次解压一份 kafka 安装包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 software]# tar -zxvf kafka_2.12-3.3.2.tgz </span><br></pre></td></tr></table></figure>


</li>
<li><p>重命名为 kafka2</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 software]# mv kafka_2.12-3.3.2 ../server/kafka2</span><br></pre></td></tr></table></figure>


</li>
<li><p>在 node1上修改&#x2F;export&#x2F;server&#x2F;kafka2&#x2F;config&#x2F;kraft&#x2F;server.properties 配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kraft]# vim server.properties </span><br></pre></td></tr></table></figure>

<p>内容</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#kafka 的角色（controller 相当于主机、broker 节点相当于从机，主机类似 zk 功能）</span></span><br><span class="line"><span class="attr">process.roles</span>=<span class="string">broker, controller</span></span><br><span class="line"><span class="comment">#节点 ID</span></span><br><span class="line"><span class="attr">node.id</span>=<span class="string">2</span></span><br><span class="line"><span class="comment">#controller 服务协议别名</span></span><br><span class="line"><span class="attr">controller.listener.names</span>=<span class="string">CONTROLLER</span></span><br><span class="line"><span class="comment">#全 Controller 列表</span></span><br><span class="line"><span class="attr">controller.quorum.voters</span>=<span class="string">2@node1:9093,3@node2:9093,4@node3:9093</span></span><br><span class="line"><span class="comment">#不同服务器绑定的端口</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://:9092,CONTROLLER://:9093</span></span><br><span class="line"><span class="comment">#broker 服务协议别名</span></span><br><span class="line"><span class="attr">inter.broker.listener.name</span>=<span class="string">PLAINTEXT</span></span><br><span class="line"><span class="comment">#broker 对外暴露的地址</span></span><br><span class="line"><span class="attr">advertised.Listeners</span>=<span class="string">PLAINTEXT://node1:9092</span></span><br><span class="line"><span class="comment">#协议别名到安全协议的映射</span></span><br><span class="line"><span class="attr">listener.security.protocol.map</span>=<span class="string">CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span></span><br><span class="line"><span class="comment">#kafka 数据存储目录</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/export/server/kafka2/data</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>分发 kafka2</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 server]# scp -r kafka2 root@node2:$PWD/</span><br><span class="line">(base) [root@node1 server]# scp -r kafka2 root@node3:$PWD/</span><br></pre></td></tr></table></figure>

<ul>
<li><p>在node2和node3上需要对 node.id 相应改变 ， 值需要和controller.quorum.voters 对应。</p>
</li>
<li><p>在 node2和node3上需要根据各自的主机名称，修改相应的advertised.Listeners 地址。</p>
</li>
</ul>
</li>
<li><p>初始化集群数据目录</p>
<p>a. 首先生成存储目录唯一ID</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka2]# ./bin/kafka-storage.sh random-uuid</span><br><span class="line">y0XwsF6dSOyAQoRyAYssJw</span><br></pre></td></tr></table></figure>

<p>b. 用该 ID 格式化 kafka 存储目录（三台节点）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka2]# ./bin/kafka-storage.sh format -t y0XwsF6dSOyAQoRyAYssJw -c ./config/kraft/server.properties </span><br><span class="line">(base) [root@node2 kafka2]# ./bin/kafka-storage.sh format -t y0XwsF6dSOyAQoRyAYssJw -c ./config/kraft/server.properties </span><br><span class="line">(base) [root@node3 kafka2]# ./bin/kafka-storage.sh format -t y0XwsF6dSOyAQoRyAYssJw -c ./config/kraft/server.properties </span><br></pre></td></tr></table></figure>


</li>
<li><p>启动 kafka 集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka2]# ./bin/kafka-server-start.sh -daemon ./config/server.properties </span><br><span class="line">(base) [root@node2 kafka2]# ./bin/kafka-server-start.sh -daemon ./config/server.properties </span><br><span class="line">(base) [root@node3 kafka2]# ./bin/kafka-server-start.sh -daemon ./config/server.properties </span><br></pre></td></tr></table></figure>


</li>
<li><p>停止 kafka 集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka2]# ./bin/kafka-server-stop.sh</span><br><span class="line">(base) [root@node2 kafka2]# ./bin/kafka-server-stop.sh</span><br><span class="line">(base) [root@node3 kafka2]# ./bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="Kafka-Kraft-集群启动停止脚本"><a href="#Kafka-Kraft-集群启动停止脚本" class="headerlink" title="Kafka-Kraft 集群启动停止脚本"></a>Kafka-Kraft 集群启动停止脚本</h1><ol>
<li><p>在~&#x2F;bin 目录下创建文件 kf2.sh 脚本文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# vim bin/kf2.sh </span><br></pre></td></tr></table></figure>

<p>内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">! /bin/bash</span></span><br><span class="line">echo &quot;--------&gt;&gt; kafka2 基于Kraft,不依赖于Zookeeper&quot;</span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line"> for i in node1 node2 node3</span><br><span class="line"> do</span><br><span class="line"> echo &quot; --------启动 $i Kafka2-------&quot;</span><br><span class="line"> ssh $i &quot;/export/server/kafka2/bin/kafka-server-start.sh -daemon /export/server/kafka2/config/kraft/server.properties&quot;</span><br><span class="line"> done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line"> for i in node1 node2 node3</span><br><span class="line"> do</span><br><span class="line"> echo &quot; --------停止 $i Kafka2-------&quot;</span><br><span class="line"> ssh $i &quot;/export/server/kafka2/bin/kafka-server-stop.sh &quot;</span><br><span class="line"> done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br><span class="line">~</span><br></pre></td></tr></table></figure>


</li>
<li><p>添加执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# chmod u+x bin/kf2.sh </span><br></pre></td></tr></table></figure>


</li>
<li><p>启动集群命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# ./bin/kf2.sh start</span><br></pre></td></tr></table></figure>


</li>
<li><p>停止集群命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# ./bin/kf2.sh stop</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/03/16/Kafka-Eagle%E7%9B%91%E6%8E%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/16/Kafka-Eagle%E7%9B%91%E6%8E%A7/" class="post-title-link" itemprop="url">Kafka-Eagle监控</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-16 20:19:41" itemprop="dateCreated datePublished" datetime="2022-03-16T20:19:41+08:00">2022-03-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-30 17:51:05" itemprop="dateModified" datetime="2023-01-30T17:51:05+08:00">2023-01-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>Kafka-Eagle 框架可以监控 Kafka 集群的整体运行情况，在生产环境中经常使用。Kafka-Eagle 的安装依赖于 MySQL，MySQL 主要用来存储可视化展示的数据。</p>
<h1 id="Kafka环境准备"><a href="#Kafka环境准备" class="headerlink" title="Kafka环境准备"></a>Kafka环境准备</h1><ol>
<li><p>关闭Kafka集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# ./bin/kf.sh stop</span><br></pre></td></tr></table></figure>


</li>
<li><p>修改&#x2F;export&#x2F;server&#x2F;kafka&#x2F;bin&#x2F;kafka-server-start.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# vim bin/kafka-server-start.sh </span><br></pre></td></tr></table></figure>

<p>修改如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then</span><br><span class="line">    export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>为</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then</span><br><span class="line">    export KAFKA_HEAP_OPTS=&quot;-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70&quot;</span><br><span class="line">    export JMX_PORT=&quot;9999&quot;</span><br><span class="line">    #export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>修改之后在启动 Kafka 之前要同步其他节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# scp bin/kafka-server-start.sh root@node2:$PWD/bin/</span><br><span class="line">(base) [root@node1 kafka]# scp bin/kafka-server-start.sh root@node3:$PWD/bin/</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="Kafka-Eagle安装"><a href="#Kafka-Eagle安装" class="headerlink" title="Kafka-Eagle安装"></a>Kafka-Eagle安装</h1><ol>
<li><p>官网</p>
<p><a target="_blank" rel="noopener" href="https://www.kafka-eagle.org/">https://www.kafka-eagle.org/</a></p>
</li>
<li><p>下载解压安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 software]# tar -zxvf kafka-eagle-bin-3.0.1.tar.gz</span><br><span class="line">(base) [root@node1 software]# cd kafka-eagle-bin-3.0.1/</span><br><span class="line">(base) [root@node1 kafka-eagle-bin-3.0.1]# tar -zxvf efak-web-3.0.1-bin.tar.gz </span><br><span class="line">(base) [root@node1 kafka-eagle-bin-3.0.1]# mv efak-web-3.0.1 /export/server/efak</span><br></pre></td></tr></table></figure>


</li>
<li><p>修改配置文件</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># multi zookeeper &amp; kafka cluster list</span></span><br><span class="line"><span class="comment"># Settings prefixed with &#x27;kafka.eagle.&#x27; will be deprecated, use &#x27;efak.&#x27; instead</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">efak.zk.cluster.alias</span>=<span class="string">cluster1</span></span><br><span class="line"><span class="attr">cluster1.zk.list</span>=<span class="string">node1:2181,node2:2181,node3:2181/kafka</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka offset storage</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">cluster1.efak.offset.storage</span>=<span class="string">kafka</span></span><br><span class="line"><span class="attr">cluster2.efak.offset.storage</span>=<span class="string">zk</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka mysql jdbc driver address</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="attr">efak.driver</span>=<span class="string">com.mysql.cj.jdbc.Driver</span></span><br><span class="line"><span class="attr">efak.url</span>=<span class="string">jdbc:mysql://node1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></span><br><span class="line"><span class="attr">efak.username</span>=<span class="string">root</span></span><br><span class="line"><span class="attr">efak.password</span>=<span class="string">hadoop</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>添加环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 conf]# vim /etc/profile</span><br></pre></td></tr></table></figure>

<p>在末尾添加</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#KAFKA-EFAK</span></span><br><span class="line"><span class="attr">export</span> <span class="string">KE_HOME=/export/server/efak</span></span><br><span class="line"><span class="attr">export</span> <span class="string">PATH=$PATH:$KE_HOME/bin</span></span><br></pre></td></tr></table></figure>

<p>使生效</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 conf]# source /etc/profile</span><br></pre></td></tr></table></figure>


</li>
<li><p>启动</p>
<p>启动之前需要先启动 ZK 以及 Kafka。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# ./bin/zk.sh start</span><br><span class="line">(base) [root@node1 ~]# ./bin/kf.sh start</span><br></pre></td></tr></table></figure>

<p>启动Efak</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 efak]# ./bin/ke.sh start</span><br><span class="line">[2023-01-30 17:44:02] INFO: [Job done!]</span><br><span class="line">Welcome to</span><br><span class="line">    ______    ______    ___     __ __</span><br><span class="line">   / ____/   / ____/   /   |   / //_/</span><br><span class="line">  / __/     / /_      / /| |  / ,&lt;   </span><br><span class="line"> / /___    / __/     / ___ | / /| |  </span><br><span class="line">/_____/   /_/       /_/  |_|/_/ |_|  </span><br><span class="line">( Eagle For Apache Kafka )</span><br><span class="line"></span><br><span class="line">Version v3.0.1 -- Copyright 2016-2022</span><br><span class="line">*******************************************************************</span><br><span class="line">* EFAK Service has started success.</span><br><span class="line">* Welcome, Now you can visit &#x27;http://192.168.88.151:8048&#x27;</span><br><span class="line">* Account:admin ,Password:123456</span><br><span class="line">*******************************************************************</span><br><span class="line">* &lt;Usage&gt; ke.sh [start|status|stop|restart|stats] &lt;/Usage&gt;</span><br><span class="line">* &lt;Usage&gt; https://www.kafka-eagle.org/ &lt;/Usage&gt;</span><br><span class="line">*******************************************************************</span><br></pre></td></tr></table></figure>

<p>关闭Efak</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 efak]# ./bin/ke.sh stop</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="Kafka-Eagle页面"><a href="#Kafka-Eagle页面" class="headerlink" title="Kafka-Eagle页面"></a>Kafka-Eagle页面</h1><p><img src="/2022/03/16/Kafka-Eagle%E7%9B%91%E6%8E%A7/image-20230130175035021.png" alt="image-20230130175035021"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/" class="post-title-link" itemprop="url">Kafka消费者</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-14 19:50:49" itemprop="dateCreated datePublished" datetime="2022-03-14T19:50:49+08:00">2022-03-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-30 15:53:58" itemprop="dateModified" datetime="2023-01-30T15:53:58+08:00">2023-01-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Kafka消费方式"><a href="#Kafka消费方式" class="headerlink" title="Kafka消费方式"></a>Kafka消费方式</h1><p><strong>pull（拉）模 式：</strong></p>
<p>consumer采用从broker中主动拉取数据。<br>Kafka采用这种方式。</p>
<p><strong>push（推）模式：</strong></p>
<p>Kafka没有采用这种方式，因为由broker决定消息发送速率，很难适应所有消费者的消费速率。例如推送的速度是50m&#x2F;s，Consumer1、Consumer2就来不及处理消息。</p>
<blockquote>
<p>pull模式不足之处是，如 果Kafka没有数据，消费者可能会陷入循环中，一直返回空数据。</p>
</blockquote>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230129170544843.png" alt="image-20230129170544843"></p>
<h1 id="Kafka-消费者工作流程"><a href="#Kafka-消费者工作流程" class="headerlink" title="Kafka 消费者工作流程"></a>Kafka 消费者工作流程</h1><h2 id="Kafka-消费者总体工作流程"><a href="#Kafka-消费者总体工作流程" class="headerlink" title="Kafka 消费者总体工作流程"></a>Kafka 消费者总体工作流程</h2><p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230129170857457.png" alt="image-20230129170857457"></p>
<h2 id="消费者组原理"><a href="#消费者组原理" class="headerlink" title="消费者组原理"></a>消费者组原理</h2><h3 id="消费者组"><a href="#消费者组" class="headerlink" title="消费者组"></a>消费者组</h3><p>Consumer Group（CG）：消费者组，由多个consumer组成。形成一个消费者组的条件，是所有消费者的groupid相同。</p>
<ul>
<li>消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费。</li>
<li>消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li>
</ul>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230129173612911.png" alt="image-20230129173612911"></p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230129173806525.png" alt="image-20230129173806525"></p>
<h3 id="消费者组初始化流程"><a href="#消费者组初始化流程" class="headerlink" title="消费者组初始化流程"></a>消费者组初始化流程</h3><p>coordinator：辅助实现消费者组的初始化和分区的分配。<br>coordinator节点选择 &#x3D; groupid的hashcode值 % 50（ <em>consumer_offsets的分区数量）</em><br>例如： groupid的hashcode值 &#x3D; 1，1% 50 &#x3D; 1，那么consumer_offsets 主题的1号分区，在哪个broker上，就选择这个节点的coordinator作为这个消费者组的老大。消费者组下的所有的消费者提交offset的时候就往这个分区去提交offset。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230129174335217.png" alt="image-20230129174335217"></p>
<h3 id="消费者组详细消费流程"><a href="#消费者组详细消费流程" class="headerlink" title="消费者组详细消费流程"></a>消费者组详细消费流程</h3><p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230129174512599.png" alt="image-20230129174512599"></p>
<h3 id="消费者重要参数"><a href="#消费者重要参数" class="headerlink" title="消费者重要参数"></a>消费者重要参数</h3><table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>bootstrap.servers</td>
<td>向 Kafka 集群建立初始连接用到的 host&#x2F;port 列表。</td>
</tr>
<tr>
<td>key.deserializer和value.deserializer</td>
<td>指定接收消息的 key 和 value 的反序列化类型。一定要写全类名。</td>
</tr>
<tr>
<td>group.id</td>
<td>标记消费者所属的消费者组。</td>
</tr>
<tr>
<td>enable.auto.commit</td>
<td>默认值为 true，消费者会自动周期性地向服务器提交偏移量。</td>
</tr>
<tr>
<td>auto.commit.interval.ms</td>
<td>如果设置了 enable.auto.commit 的值为 true， 则该值定义了消费者偏移量向 Kafka 提交的频率，默认 5s。</td>
</tr>
<tr>
<td>auto.offset.reset</td>
<td>当 Kafka 中没有初始偏移量或当前偏移量在服务器中不存在（如，数据被删除了），该如何处理？ earliest：自动重置偏移量到最早的偏移量。 latest：默认，自动重置偏移量为最新的偏移量。 none：如果消费组原来的（previous）偏移量不存在，则向消费者抛异常。 anything：向消费者抛异常。</td>
</tr>
<tr>
<td>offsets.topic.num.partitions</td>
<td>__consumer_offsets 的分区数，默认是 50 个分区。</td>
</tr>
<tr>
<td>heartbeat.interval.ms</td>
<td>Kafka 消费者和 coordinator 之间的心跳时间，默认 3s。该条目的值必须小于 session.timeout.ms ，也不应该高于session.timeout.ms 的 1&#x2F;3。</td>
</tr>
<tr>
<td>session.timeout.ms</td>
<td>Kafka 消费者和 coordinator 之间连接超时时间，默认 45s。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>max.poll.interval.ms</td>
<td>消费者处理消息的最大时长，默认是 5 分钟。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>fetch.min.bytes</td>
<td>默认 1 个字节。消费者获取服务器端一批消息最小的字节数。</td>
</tr>
<tr>
<td>fetch.max.wait.ms</td>
<td>默认 500ms。如果没有从服务器端获取到一批数据的最小字节数。该时间到，仍然会返回数据。</td>
</tr>
<tr>
<td>fetch.max.bytes</td>
<td>默认 Default: 52428800（50 m）。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值（50m）仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。一批次的大小受 message.max.bytes （broker config）or max.message.bytes （topic config）影响。</td>
</tr>
<tr>
<td>max.poll.records</td>
<td>一次 poll 拉取数据返回消息的最大条数，默认是 500 条。</td>
</tr>
</tbody></table>
<h1 id="消费者API"><a href="#消费者API" class="headerlink" title="消费者API"></a>消费者API</h1><h2 id="独立消费者案例（订阅主题）"><a href="#独立消费者案例（订阅主题）" class="headerlink" title="独立消费者案例（订阅主题）"></a>独立消费者案例（订阅主题）</h2><ol>
<li><p>需求</p>
<p>创建一个独立消费者，消费 first 主题中数据。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230129222637115.png" alt="image-20230129222637115"></p>
<p><strong>注意：</strong>在消费者 API 代码中必须配置消费者组 id。命令行启动消费者不填写消费者组id 会被自动填写随机的消费者组 id。</p>
</li>
<li><p>实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建消费者配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化，必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注册要消费的主题（可以消费多个主题）</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拉取数据打印</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置1s中消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord: consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>测试</p>
<p>a. 在Idea中执行消费者程序。</p>
<p>b. 在 Kafka 集群控制台，创建 Kafka 生产者，并输入数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-producer.sh --bootstrap-server node1:9092 --topic first</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello kafka</span></span><br><span class="line"><span class="meta prompt_">&gt;</span></span><br></pre></td></tr></table></figure>



<p>c. 在 IDEA 控制台观察接收到的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRecord(topic = first, partition = 2, leaderEpoch = 17, offset = 16, CreateTime = 1675002175897, serialized key size = -1, serialized value size = 11, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = hello kafka)</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="独立消费者案例（订阅分区）"><a href="#独立消费者案例（订阅分区）" class="headerlink" title="独立消费者案例（订阅分区）"></a>独立消费者案例（订阅分区）</h2><ol>
<li><p>需求</p>
<p>创建一个独立消费者，消费 first 主题 0 号分区的数据。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230129222802804.png" alt="image-20230129222802804"></p>
</li>
<li><p>实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerPartition</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建消费者配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化，必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费某个主题的某个分区数据</span></span><br><span class="line">        ArrayList&lt;TopicPartition&gt; topicPartitions = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topicPartitions.add(<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;first&quot;</span>, <span class="number">0</span>));</span><br><span class="line">        kafkaConsumer.assign(topicPartitions);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拉取数据打印</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置1s中消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord: consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>测试</p>
<p>a. 在 IDEA 中执行消费者程序。</p>
<p>b. 在 IDEA 中执行生产者程序 CustomProducerCallbackPartitions（见Kafka生产者）在控制台观察生成几个 0 号分区的数据。</p>
<p>c. 在 IDEA 控制台，观察接收到的数据，只能消费到 0 号分区数据表示正确。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 0, offset = 2, CreateTime = 1675005390152, serialized key size = -1, serialized value size = 6, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = kafka0)</span><br><span class="line">ConsumerRecord(topic = first, partition = 0, leaderEpoch = 0, offset = 3, CreateTime = 1675005390165, serialized key size = -1, serialized value size = 6, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = kafka3)</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="消费者组案例"><a href="#消费者组案例" class="headerlink" title="消费者组案例"></a>消费者组案例</h3><ol>
<li><p>需求</p>
<p>测试同一个主题的分区数据，只能由一个消费者组中的一个消费。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230129232001641.png" alt="image-20230129232001641"></p>
</li>
<li><p>实现</p>
<p>a. 复制一份基础消费者的代码，在 IDEA 中同时启动，即可启动同一个消费者组中的两个消费者。</p>
<p>b. 启动代码中的生产者发送消息，在 IDEA 控制台即可看到两个消费者在消费不同分区的数据（如果只发生到一个分区，可以在发送时增加延迟代码 Thread.sleep(2)）。</p>
<p>c. 重新发送到一个全新的主题中，由于默认创建的主题分区数为 1，可以看到只能有一个消费者消费到数据。</p>
</li>
</ol>
<h1 id="经验"><a href="#经验" class="headerlink" title="经验"></a>经验</h1><h2 id="分区的分配以及再平衡"><a href="#分区的分配以及再平衡" class="headerlink" title="分区的分配以及再平衡"></a>分区的分配以及再平衡</h2><p>1、一个consumer group中有多个consumer组成，一个 topic有多个partition组成，现在的问题是，到底由哪个consumer来消费哪个partition的数据。<br>2、Kafka有四种主流的分区分配策略： Range、RoundRobin、Sticky、CooperativeSticky。可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range + CooperativeSticky。Kafka可以同时使用多个分区分配策略。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130091307148.png" alt="image-20230130091307148"></p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>heartbeat.interval.ms</td>
<td>Kafka 消费者和 coordinator 之间的心跳时间，默认 3s。该条目的值必须小于 session.timeout.ms，也不应该高于session.timeout.ms 的 1&#x2F;3。</td>
</tr>
<tr>
<td>session.timeout.ms</td>
<td>Kafka 消费者和 coordinator 之间连接超时时间，默认 45s。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>max.poll.interval.ms</td>
<td>消费者处理消息的最大时长，默认是 5 分钟。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>partition.assignment.strategy</td>
<td>消费者分区分配策略 ， 默认策略是 Range + CooperativeSticky。Kafka 可以同时使用多个分区分配策略。可以选择的策略包括： Range 、 RoundRobin 、 Sticky 、CooperativeSticky</td>
</tr>
</tbody></table>
<h3 id="Range以及再平衡"><a href="#Range以及再平衡" class="headerlink" title="Range以及再平衡"></a>Range以及再平衡</h3><h4 id="Range分区策略原理"><a href="#Range分区策略原理" class="headerlink" title="Range分区策略原理"></a>Range分区策略原理</h4><p>Range 是对每个 topic 而言的。<br>首先对同一个 topic 里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。<br>假如现在有 7 个分区，3 个消费者，排序后的分区将会是0,1,2,3,4,5,6；消费者排序完之后将会是C0,C1,C2。<br>例如，7&#x2F;3 &#x3D; 2 余 1 ，除不尽，那么消费者 C0 便会多消费 1 个分区。 8&#x2F;3&#x3D;2余2，除不尽，那么C0和C1分别多消费一个。<br>通过 partitions数&#x2F;consumer数 来决定每个消费者应该消费几个分区。如果除不尽，那么前面几个消费者将会多消费 1 个分区。</p>
<p>注意：如果只是针对 1 个 topic 而言，C0消费者多消费1个分区影响不是很大。但是如果有 N 多个 topic，那么针对每个 topic，消费者 C0都将多消费 1 个分区，topic越多，C0消费的分区会比其他消费者明显多消费 N 个分区。</p>
<blockquote>
<p>容易产生数据倾斜！ </p>
</blockquote>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130093639216.png" alt="image-20230130093639216"></p>
<h4 id="Range分区分配策略案例"><a href="#Range分区分配策略案例" class="headerlink" title="Range分区分配策略案例"></a>Range分区分配策略案例</h4><ol>
<li><p>修改first主题的分区数为7</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --alter --topic first --partitions 7</span><br><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --describe --topic first</span><br><span class="line">Topic: first    TopicId: lF9HqKnNSB2dlBesNt48lA PartitionCount: 7       ReplicationFactor: 3    Configs: </span><br><span class="line">        Topic: first    Partition: 0    Leader: 0       Replicas: 0,1,2 Isr: 2,1,0</span><br><span class="line">        Topic: first    Partition: 1    Leader: 2       Replicas: 2,0,1 Isr: 2,0,1</span><br><span class="line">        Topic: first    Partition: 2    Leader: 2       Replicas: 1,2,0 Isr: 2,0,1</span><br><span class="line">        Topic: first    Partition: 3    Leader: 0       Replicas: 0,2,1 Isr: 0,2,1</span><br><span class="line">        Topic: first    Partition: 4    Leader: 1       Replicas: 1,0,2 Isr: 1,0,2</span><br><span class="line">        Topic: first    Partition: 5    Leader: 2       Replicas: 2,1,0 Isr: 2,1,0</span><br><span class="line">        Topic: first    Partition: 6    Leader: 0       Replicas: 0,1,2 Isr: 0,1,2</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：分区数可以增加，但是不能减少。</p>
</blockquote>
</li>
<li><p>复制 CustomConsumer 类，创建 CustomConsumer2。这样可以由三个消费者CustomConsumer、CustomConsumer1、CustomConsumer2 组成消费者组，组名都为“test”，同时启动 3 个消费者。</p>
</li>
<li><p>启动 CustomProducer 生产者，发送 500 条消息，随机发送到不同的分区。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建kafka生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.配置bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key,value 序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.调用send发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">7</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, i, <span class="string">&quot;test&quot;</span>, <span class="string">&quot;msg&quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>说明：Kafka 默认的分区分配策略就是 Range + CooperativeSticky，所以不需要修改策略。</p>
</li>
<li><p>观看 3 个消费者分别消费哪些分区的数据。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130114829286.png" alt="image-20230130114829286"></p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130114942375.png" alt="image-20230130114942375"></p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130115021210.png" alt="image-20230130115021210"></p>
</li>
</ol>
<h4 id="Range分区分配再平衡案例"><a href="#Range分区分配再平衡案例" class="headerlink" title="Range分区分配再平衡案例"></a>Range分区分配再平衡案例</h4><ul>
<li><p>停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。<br>1 号消费者：消费到 6、5 号分区数据。<br>2 号消费者：消费到 0、2、1 号分区数据。<br>0 号消费者的任务会整体被分配到 1 号消费者或者 2 号消费者。45s后，1号消费者消费到4号分区的数据，2号消费者消费到3号分区的数据。</p>
<blockquote>
<p>说明：0 号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。 </p>
</blockquote>
</li>
<li><p>再次重新发送消息观看结果（45s 以后）。<br>1 号消费者：消费到 6、4、5 号分区数据。<br>2 号消费者：消费到 0、3、2、1 号分区数据。</p>
<blockquote>
<p>说明：消费者 0 已经被踢出消费者组，所以重新按照 range 方式分配。</p>
</blockquote>
</li>
</ul>
<h3 id="RoundRobin以及再平衡"><a href="#RoundRobin以及再平衡" class="headerlink" title="RoundRobin以及再平衡"></a>RoundRobin以及再平衡</h3><h4 id="RoundRobin分区策略原理"><a href="#RoundRobin分区策略原理" class="headerlink" title="RoundRobin分区策略原理"></a>RoundRobin分区策略原理</h4><p>RoundRobin 针对集群中所有Topic而言。<br>RoundRobin 轮询分区策略，是把所有的 partition 和所有的consumer 都列出来，然后按照 hashcode 进行排序，最后通过轮询算法来分配 partition 给到各个消费者。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130131058021.png" alt="image-20230130131058021"></p>
<h4 id="RoundRobin分区分配策略案例"><a href="#RoundRobin分区分配策略案例" class="headerlink" title="RoundRobin分区分配策略案例"></a>RoundRobin分区分配策略案例</h4><ol>
<li><p>依次在 CustomConsumer、CustomConsumer1、CustomConsumer2 三个消费者代码中修改分区分配策略为 RoundRobin。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改分区分配策略</span></span><br><span class="line">properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, <span class="string">&quot;org.apache.kafka.clients.consumer.RoundRobinAssignor&quot;</span>);</span><br></pre></td></tr></table></figure>


</li>
<li><p>重启 3 个消费者，重复发送消息的步骤，观看分区结果。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130132304831.png" alt="image-20230130132304831"></p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130132336762.png" alt="image-20230130132336762"></p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130132406113.png" alt="image-20230130132406113"></p>
</li>
</ol>
<h4 id="RoundRobin分区分配再平衡案例"><a href="#RoundRobin分区分配再平衡案例" class="headerlink" title="RoundRobin分区分配再平衡案例"></a>RoundRobin分区分配再平衡案例</h4><ul>
<li><p>停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。<br>1 号消费者：消费到 2、5 号分区数据<br>2 号消费者：消费到 4、1 号分区数据<br>0 号消费者的任务会按照 RoundRobin 的方式，把数据轮询分成 0 、6 和 3 号分区数据，分别由 1 号消费者或者 2 号消费者消费。</p>
<blockquote>
<p>说明：0 号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p>
</blockquote>
</li>
<li><p>再次重新发送消息观看结果（45s 以后）。<br>1 号消费者：消费到 0、2、4、6 号分区数据<br>2 号消费者：消费到 1、3、5 号分区数据</p>
<blockquote>
<p>说明：消费者 0 已经被踢出消费者组，所以重新按照 RoundRobin 方式分配。</p>
</blockquote>
</li>
</ul>
<h3 id="Sticky以及再平衡"><a href="#Sticky以及再平衡" class="headerlink" title="Sticky以及再平衡"></a>Sticky以及再平衡</h3><h4 id="粘性分区定义"><a href="#粘性分区定义" class="headerlink" title="粘性分区定义"></a>粘性分区定义</h4><p>可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。<br>粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，首先会尽量均衡的放置分区到消费者上面，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分区不变化。</p>
<h4 id="Sticky分区分配案例"><a href="#Sticky分区分配案例" class="headerlink" title="Sticky分区分配案例"></a>Sticky分区分配案例</h4><ol>
<li><p>需求</p>
<p>设置主题为 first，7 个分区；准备 3 个消费者，采用粘性分区策略，并进行消费，观察消费分配情况。然后再停止其中一个消费者，再次观察消费分配情况。</p>
</li>
<li><p>修改分区分配策略为粘性。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改分区分配策略</span></span><br><span class="line">ArrayList&lt;String&gt; startegys = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">startegys.add(<span class="string">&quot;org.apache.kafka.clients.consumer.StickyAssignor&quot;</span>);</span><br><span class="line"></span><br><span class="line">properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, startegys);</span><br></pre></td></tr></table></figure>


</li>
<li><p>使用同样的生产者发送 500 条消息。</p>
<p>可以看到会尽量保持分区的个数近似划分分区。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130134128847.png" alt="image-20230130134128847"></p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130134159190.png" alt="image-20230130134159190"></p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130134229764.png" alt="image-20230130134229764"></p>
</li>
</ol>
<h4 id="Sticky分区分配再平衡案例"><a href="#Sticky分区分配再平衡案例" class="headerlink" title="Sticky分区分配再平衡案例"></a>Sticky分区分配再平衡案例</h4><ul>
<li><p>停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。<br>1 号消费者：消费到 6、4、5 号分区数据。<br>2 号消费者：消费到 3、2 号分区数据。<br>0 号消费者的任务会按照粘性规则，尽可能均衡的随机分成 0 和 1 号分区数据，分别由 2号消费者或者 1 号消费者消费。</p>
<blockquote>
<p>说明：0 号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p>
</blockquote>
</li>
<li><p>再次重新发送消息观看结果（45s 以后）。<br>1 号消费者：消费到 6、4、1、5 号分区数据。<br>2 号消费者：消费到 0、3、2 号分区数据。</p>
<blockquote>
<p>说明：消费者 0 已经被踢出消费者组，所以重新按照粘性方式分配。</p>
</blockquote>
</li>
</ul>
<h1 id="offset-位移"><a href="#offset-位移" class="headerlink" title="offset 位移"></a>offset 位移</h1><h2 id="offset的默认维护位置"><a href="#offset的默认维护位置" class="headerlink" title="offset的默认维护位置"></a>offset的默认维护位置</h2><p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130135755657.png" alt="image-20230130135755657"></p>
<p>__consumer_offsets 主题里面采用 key 和 value 的方式存储数据。key 是 group.id+topic+分区号，value 就是当前 offset 的值。每隔一段时间，kafka 内部会对这个 topic 进行compact，也就是每个 group.id+topic+分区号就保留最新数据。</p>
<h3 id="消费offset案例"><a href="#消费offset案例" class="headerlink" title="消费offset案例"></a>消费offset案例</h3><ol>
<li><p>思想：__consumer_offsets 为 Kafka 中的 topic，那就可以通过消费者进行消费。</p>
</li>
<li><p>在配置文件 config&#x2F;consumer.properties 中添加配置 exclude.internal.topics&#x3D;false，默认是 true，表示不能消费系统主题。为了查看该系统主题数据，所以该参数修改为 false。</p>
</li>
<li><p>采用命令行方式，创建一个新的 topic。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --create --topic five --partitions 2 --replication-factor 2</span><br><span class="line">Created topic five.</span><br></pre></td></tr></table></figure>


</li>
<li><p>启动生产者往five生产数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-producer.sh --bootstrap-server node1:9092 --topic five</span><br><span class="line"><span class="meta prompt_">&gt;</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>启动消费者消费five数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic five --group five-test</span><br></pre></td></tr></table></figure>


</li>
<li><p>查看消费者消费主题__consumer_offsets。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server node1:9092 --consumer.config config/consumer.properties --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; --from-beginning</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="自动提交offset"><a href="#自动提交offset" class="headerlink" title="自动提交offset"></a>自动提交offset</h2><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。<br>自动提交offset的相关参数：</p>
<ul>
<li>enable.auto.commit：是否开启自动提交offset功能，默认是true</li>
<li>auto.commit.interval.ms：自动提交offset的时间间隔，默认是5s</li>
</ul>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130143646759.png" alt="image-20230130143646759"></p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>enable.auto.commit</td>
<td>默认值为 true，消费者会自动周期性地向服务器提交偏移量。</td>
</tr>
<tr>
<td>auto.commit.interval.ms</td>
<td>如果设置了 enable.auto.commit 的值为 true， 则该值定义了消费者偏移量向 Kafka 提交的频率，默认 5s。</td>
</tr>
</tbody></table>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerAutoOffset</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建消费者配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化，必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 是否自动提交offset</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">true</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 提交offset的时间周期1000ms，默认5s</span></span><br><span class="line">        properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注册要消费的主题（可以消费多个主题）</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拉取数据打印</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置1s中消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord: consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="手动提交offset"><a href="#手动提交offset" class="headerlink" title="手动提交offset"></a>手动提交offset</h2><p>虽然自动提交offset十分简单便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。<br>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将本次提交的一批数据最高的偏移量提交；不同点是，同步提交阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而异步提交则没有失败重试机制，故有可能提交失败。</p>
<ul>
<li>commitSync（同步提交）：必须等待offset提交完毕，再去消费下一批数据。</li>
<li>commitAsync（异步提交） ：发送完提交offset请求后，就开始消费下一批数据了。</li>
</ul>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130145741104.png" alt="image-20230130145741104"></p>
<h3 id="同步提交offset"><a href="#同步提交offset" class="headerlink" title="同步提交offset"></a>同步提交offset</h3><p>由于同步提交 offset 有失败重试机制，故更加可靠，但是由于一直等待提交结果，提交的效率比较低。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerByHandSync</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建消费者配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化，必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 是否自动提交offset</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注册要消费的主题（可以消费多个主题）</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拉取数据打印</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置1s中消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord: consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 同步提交offset</span></span><br><span class="line">            kafkaConsumer.commitSync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="异步提交offset"><a href="#异步提交offset" class="headerlink" title="异步提交offset"></a>异步提交offset</h3><p>虽然同步提交 offset 更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会受到很大的影响。因此更多的情况下，会选用异步提交 offset 的方式。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerByHandAsync</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建消费者配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化，必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 是否自动提交offset</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注册要消费的主题（可以消费多个主题）</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拉取数据打印</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置1s中消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord: consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 异步提交offset</span></span><br><span class="line">            kafkaConsumer.commitAsync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="指定-Offset消费"><a href="#指定-Offset消费" class="headerlink" title="指定 Offset消费"></a><strong>指定</strong> Offset消费</h2><p>auto.offset.reset &#x3D; earliest | latest | none 默认是 latest。<br>当 Kafka 中没有初始偏移量（消费者组第一次消费）或服务器上不再存在当前偏移量时（例如该数据已被删除），该怎么办？<br>（1）earliest：自动将偏移量重置为最早的偏移量，–from-beginning。<br>（2）latest（默认值）：自动将偏移量重置为最新偏移量。<br>（3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130151047260.png" alt="image-20230130151047260"></p>
<p>（4）任意指定 offset 位移开始消费</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerSeek</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建消费者配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化，必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test2&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注册要消费的主题（可以消费多个主题）</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        Set&lt;TopicPartition&gt; assignment = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (assignment.size() == <span class="number">0</span>) &#123;</span><br><span class="line">            kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">//  获取消费者分区分配信息（有了分区分配信息才能开始消费）</span></span><br><span class="line">            assignment = kafkaConsumer.assignment();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历所有分区，并指定 offset 从 1000 的位置开始消费</span></span><br><span class="line">        <span class="keyword">for</span>(TopicPartition tp: assignment) &#123;</span><br><span class="line">            kafkaConsumer.seek(tp, <span class="number">1000</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置1s中消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord: consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>注意：每次执行完，需要修改消费者组名；</p>
<h2 id="指定时间消费"><a href="#指定时间消费" class="headerlink" title="指定时间消费"></a>指定时间消费</h2><p>需求：在生产环境中，会遇到最近消费的几个小时数据异常，想重新按照时间消费。例如要求按照时间消费前一天的数据，怎么处理？</p>
<p>实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.internals.Topic;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumerForTime</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建消费者配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化，必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test2&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注册要消费的主题（可以消费多个主题）</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        Set&lt;TopicPartition&gt; assignment = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (assignment.size() == <span class="number">0</span>) &#123;</span><br><span class="line">            kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="comment">//  获取消费者分区分配信息（有了分区分配信息才能开始消费）</span></span><br><span class="line">            assignment = kafkaConsumer.assignment();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;TopicPartition, Long&gt; timestampToSearch = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 封装集合存储，每个分区对应一天前的数据</span></span><br><span class="line">        <span class="keyword">for</span>(TopicPartition tp: assignment) &#123;</span><br><span class="line">            timestampToSearch.put(tp, System.currentTimeMillis() - <span class="number">1</span> * <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取从 1 天前开始消费的每个分区的 offset</span></span><br><span class="line">        Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsets = kafkaConsumer.offsetsForTimes((timestampToSearch));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历每个分区，对每个分区设置消费时间。</span></span><br><span class="line">        <span class="keyword">for</span> (TopicPartition tp: assignment) &#123;</span><br><span class="line">            <span class="type">OffsetAndTimestamp</span> <span class="variable">offsetAndTimestamp</span> <span class="operator">=</span> offsets.get(tp);</span><br><span class="line">            <span class="comment">// 根据时间指定开始消费的位置</span></span><br><span class="line">            <span class="keyword">if</span> (offsetAndTimestamp != <span class="literal">null</span>) &#123;</span><br><span class="line">                kafkaConsumer.seek(tp, offsetAndTimestamp.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord: consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="漏消费和重复消费"><a href="#漏消费和重复消费" class="headerlink" title="漏消费和重复消费"></a>漏消费和重复消费</h2><p><strong>重复消费：</strong>已经消费了数据，但是 offset 没提交。</p>
<p><strong>漏消费：</strong>先提交 offset 后消费，有可能会造成数据的漏消费。</p>
<ul>
<li><p>场景：重复消费。自动提交offset引起。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130154319259.png" alt="image-20230130154319259"></p>
</li>
<li><p>场景：漏消费。设置offset为手动提交，当offset被提交时，数据还在内存中未落盘，此时刚好消费者线程被kill掉，那么offset已经提交，但是数据未处理，导致这部分内存中的数据丢失。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130154429908.png" alt="image-20230130154429908"></p>
</li>
</ul>
<p>怎么能做到既不漏消费也不重复消费呢？<strong>消费者事务</strong>。</p>
<h1 id="经验-1"><a href="#经验-1" class="headerlink" title="经验"></a>经验</h1><h2 id="消费者事务"><a href="#消费者事务" class="headerlink" title="消费者事务"></a>消费者事务</h2><p>如果想完成Consumer端的精准一次性消费，那么需要Kafka消费端将消费过程和提交offset过程做原子绑定。此时我们需要将Kafka的offset保存到支持事务的自定义介质（比如MySQL）。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130154904806.png" alt="image-20230130154904806"></p>
<h2 id="数据积压（消费者如何提高吞吐量）"><a href="#数据积压（消费者如何提高吞吐量）" class="headerlink" title="数据积压（消费者如何提高吞吐量）"></a>数据积压（消费者如何提高吞吐量）</h2><ul>
<li><p>如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数 &#x3D; 分区数。（两者缺一不可）</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130155110392.png" alt="image-20230130155110392"></p>
</li>
<li><p>如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据&#x2F;处理时间 &lt; 生产速度），使处理的数据小于生产的数据，也会造成数据积压。</p>
<p><img src="/2022/03/14/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/image-20230130155212100.png" alt="image-20230130155212100"></p>
</li>
</ul>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>fetch.max.bytes</td>
<td>默认 Default: 52428800（50 m）。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值（50m）仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。一批次的大小受 message.max.bytes （broker config）or max.message.bytes （topic config）影响。</td>
</tr>
<tr>
<td>max.poll.records</td>
<td>一次 poll 拉取数据返回消息的最大条数，默认是 500 条</td>
</tr>
</tbody></table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/03/11/Kafka-Broker/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/11/Kafka-Broker/" class="post-title-link" itemprop="url">Kafka Broker</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-11 10:03:40" itemprop="dateCreated datePublished" datetime="2022-03-11T10:03:40+08:00">2022-03-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-29 16:36:26" itemprop="dateModified" datetime="2023-01-29T16:36:26+08:00">2023-01-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Kafka-Broker-工作流程"><a href="#Kafka-Broker-工作流程" class="headerlink" title="Kafka Broker 工作流程"></a>Kafka Broker 工作流程</h1><h2 id="Zookeeper-存储的-Kafka-信息"><a href="#Zookeeper-存储的-Kafka-信息" class="headerlink" title="Zookeeper 存储的 Kafka 信息"></a>Zookeeper 存储的 Kafka 信息</h2><ol>
<li><p>启动Zookeeper客户端</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 zookeeper-3.7.1]# ./bin/zkCli.sh </span><br></pre></td></tr></table></figure>


</li>
<li><p>通过ls命令查看kafka相关信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /kafka</span><br><span class="line">[admin, brokers, cluster, config, consumers, controller, controller_epoch, feature, isr_change_notification, latest_producer_id_block, log_dir_event_notification]</span><br></pre></td></tr></table></figure></li>
</ol>
<p><img src="/2022/03/11/Kafka-Broker/image-20230128163441415.png" alt="image-20230128163441415"></p>
<h2 id="Kafka-Broker总体工作流程"><a href="#Kafka-Broker总体工作流程" class="headerlink" title="Kafka Broker总体工作流程"></a>Kafka Broker总体工作流程</h2><p><img src="/2022/03/11/Kafka-Broker/image-20230128163849451.png" alt="image-20230128163849451"></p>
<h3 id="模拟-Kafka-上下线，Zookeeper-中数据变化"><a href="#模拟-Kafka-上下线，Zookeeper-中数据变化" class="headerlink" title="模拟 Kafka 上下线，Zookeeper 中数据变化"></a>模拟 Kafka 上下线，Zookeeper 中数据变化</h3><ol>
<li><p>查看&#x2F;kafka&#x2F;brokers&#x2F;ids 路径上的节点。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] ls /kafka/brokers/ids</span><br><span class="line">[0, 1, 2]</span><br></pre></td></tr></table></figure>


</li>
<li><p>查看&#x2F;kafka&#x2F;controller 路径上的数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] get /kafka/controller</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;brokerid&quot;:0,&quot;timestamp&quot;:&quot;1674914573172&quot;&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>查看&#x2F;kafka&#x2F;brokers&#x2F;topics&#x2F;first&#x2F;partitions&#x2F;0&#x2F;state 路径上的数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 6] get /kafka/brokers/topics/first/partitions/0/state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:8,&quot;leader&quot;:2,&quot;version&quot;:1,&quot;leader_epoch&quot;:6,&quot;isr&quot;:[2]&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>停止 node3上的 kafka。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node3 bin]# ./kafka-server-stop.sh </span><br></pre></td></tr></table></figure>


</li>
<li><p>再次查看&#x2F;kafka&#x2F;brokers&#x2F;ids 路径上的节点。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 7] ls /kafka/brokers/ids</span><br><span class="line">[0, 1]</span><br></pre></td></tr></table></figure>


</li>
<li><p>再次查看&#x2F;kafka&#x2F;controller 路径上的数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 8] get /kafka/controller</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;brokerid&quot;:0,&quot;timestamp&quot;:&quot;1674914573172&quot;&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>再次查看&#x2F;kafka&#x2F;brokers&#x2F;topics&#x2F;first&#x2F;partitions&#x2F;0&#x2F;state 路径上的数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 9] get /kafka/brokers/topics/first/partitions/0/state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:8,&quot;leader&quot;:-1,&quot;version&quot;:1,&quot;leader_epoch&quot;:7,&quot;isr&quot;:[2]&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>启动 node3上的 kafka。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node3 kafka]# ./bin/kafka-server-start.sh -daemon ./config/server.properties </span><br></pre></td></tr></table></figure>


</li>
<li><p>再次观察1，2，3步骤中的内容。</p>
</li>
</ol>
<h3 id="Broker重要参数"><a href="#Broker重要参数" class="headerlink" title="Broker重要参数"></a>Broker重要参数</h3><table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>replica.lag.time.max.ms</td>
<td>ISR中，如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值，默认 30s。</td>
</tr>
<tr>
<td>auto.leader.rebalance.enable</td>
<td>默认是 true。 自动 Leader Partition 平衡。</td>
</tr>
<tr>
<td>leader.imbalance.per.broker.percentage</td>
<td>默认是 10%。每个 broker 允许的不平衡的 leader的比率。如果每个 broker 超过了这个值，控制器会触发 leader 的平衡。</td>
</tr>
<tr>
<td>leader.imbalance.check.interval.seconds</td>
<td>默认值 300 秒。检查 leader 负载是否平衡的间隔时间。</td>
</tr>
<tr>
<td>log.segment.bytes</td>
<td>Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分 成块的大小，默认值 1G。</td>
</tr>
<tr>
<td>log.index.interval.bytes</td>
<td>默认 4kb，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引。</td>
</tr>
<tr>
<td>log.retention.hours</td>
<td>Kafka 中数据保存的时间，默认 7 天。</td>
</tr>
<tr>
<td>log.retention.minutes</td>
<td>Kafka 中数据保存的时间，分钟级别，默认关闭。</td>
</tr>
<tr>
<td>log.retention.ms</td>
<td>Kafka 中数据保存的时间，毫秒级别，默认关闭。</td>
</tr>
<tr>
<td>log.retention.check.interval.ms</td>
<td>检查数据是否保存超时的间隔，默认是 5 分钟。</td>
</tr>
<tr>
<td>log.retention.bytes</td>
<td>默认等于-1，表示无穷大。超过设置的所有日志总大小，删除最早的 segment。</td>
</tr>
<tr>
<td>log.cleanup.policy</td>
<td>默认是 delete，表示所有数据启用删除策略；如果设置值为 compact，表示所有数据启用压缩策略。</td>
</tr>
<tr>
<td>num.io.threads</td>
<td>默认是 8。负责写磁盘的线程数。整个参数值要占总核数的 50%。</td>
</tr>
<tr>
<td>num.replica.fetchers</td>
<td>副本拉取线程数，这个参数占总核数的 50%的 1&#x2F;3</td>
</tr>
<tr>
<td>num.network.threads</td>
<td>默认是 3。数据传输线程数，这个参数占总核数的50%的 2&#x2F;3 。</td>
</tr>
<tr>
<td>log.flush.interval.messages</td>
<td>强制页缓存刷写到磁盘的条数，默认是 long 的最大值，9223372036854775807。一般不建议修改，交给系统自己管理。</td>
</tr>
<tr>
<td>log.flush.interval.ms</td>
<td>每隔多久，刷数据到磁盘，默认是 null。一般不建议修改，交给系统自己管理。</td>
</tr>
</tbody></table>
<h1 id="节点服役和退役"><a href="#节点服役和退役" class="headerlink" title="节点服役和退役"></a>节点服役和退役</h1><h2 id="服役新节点"><a href="#服役新节点" class="headerlink" title="服役新节点"></a>服役新节点</h2><h3 id="新节点准备"><a href="#新节点准备" class="headerlink" title="新节点准备"></a>新节点准备</h3><ol>
<li><p>关闭node3节点，右键执行克隆操作，修改克隆好的node4的mac地址。</p>
</li>
<li><p>开启node4，并修改ip。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node4 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=&quot;none&quot;</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6INIT=&quot;yes&quot;</span><br><span class="line">IPV6_AUTOCONF=&quot;yes&quot;</span><br><span class="line">IPV6_DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV6_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;</span><br><span class="line">NAME=&quot;ens33&quot;</span><br><span class="line">UUID=&quot;7815751b-505d-4ae2-b2d4-aa39591dc6aa&quot;</span><br><span class="line">DEVICE=&quot;ens33&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">IPADDR=&quot;192.168.88.154&quot;</span><br><span class="line">PREFIX=&quot;24&quot;</span><br><span class="line">GATEWAY=&quot;192.168.88.2&quot;</span><br><span class="line">DNS1=&quot;192.168.88.2&quot;</span><br><span class="line">DOMAIN=&quot;114.114.114.114&quot;</span><br><span class="line">IPV6_PRIVACY=&quot;no&quot;</span><br></pre></td></tr></table></figure>


</li>
<li><p>在node4上，修改主机名称为node4。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node4 ~]# vim /etc/hostname </span><br><span class="line">node4</span><br></pre></td></tr></table></figure>


</li>
<li><p>重新启动node3、node4。</p>
</li>
<li><p>修改node4中 kafka 的 broker.id 为 3。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node4 ~]# vim /export/server/kafka/config/server.properties </span><br><span class="line">broker.id=3</span><br></pre></td></tr></table></figure>


</li>
<li><p>删除node4中 kafka 下的 datas 和 logs。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node4 kafka]# rm -rf datas/* logs/*</span><br></pre></td></tr></table></figure>


</li>
<li><p>启动 node1、node2、node3上的 kafka 集群。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# ./bin/zk.sh start</span><br><span class="line">(base) [root@node1 ~]# ./bin/kf.sh start</span><br></pre></td></tr></table></figure>


</li>
<li><p>单独启动node4中的 kafka。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node4 kafka]# ./bin/kafka-server-start.sh -daemon ./config/server.properties </span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="执行负载均衡操作"><a href="#执行负载均衡操作" class="headerlink" title="执行负载均衡操作"></a>执行负载均衡操作</h3><ol>
<li><p>创建一个要均衡的主题。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# vim topics-to-move.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;topics&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;first&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>生成一个负载均衡的计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server node1:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;0,1,2,3&quot; --generate</span><br><span class="line"></span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;]&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>创建副本存储计划（所有副本存储在 broker0、broker1、broker2、broker3 中）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# vim increase-replication-factor.json</span><br></pre></td></tr></table></figure>

<p>内容</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;partitions&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;first&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">0</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;log_dirs&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;first&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">1</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;log_dirs&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;first&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">2</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;log_dirs&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


</li>
<li><p>执行副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server node1:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line"></span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started partition reassignments for first-0,first-1,first-2</span><br></pre></td></tr></table></figure>


</li>
<li><p>验证副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server node1:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br><span class="line"></span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition first-0 is still in progress.</span><br><span class="line">Reassignment of partition first-1 is still in progress.</span><br><span class="line">Reassignment of partition first-2 is completed.</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="退役旧节点"><a href="#退役旧节点" class="headerlink" title="退役旧节点"></a>退役旧节点</h2><h3 id="执行负载均衡操作-1"><a href="#执行负载均衡操作-1" class="headerlink" title="执行负载均衡操作"></a>执行负载均衡操作</h3><p>先按照退役一台节点，生成执行计划，然后按照服役时操作流程执行负载均衡。</p>
<ol>
<li><p>创建一个要均衡的主题。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# vim topics-to-move.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;topics&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;first&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>生成一个负载均衡的计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server node1:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;0,1,2&quot; --generate</span><br><span class="line"></span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,3,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[3,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,2,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>创建副本存储计划（所有副本存储在 broker0、broker1、broker2、broker3 中）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# vim increase-replication-factor.json</span><br></pre></td></tr></table></figure>

<p>内容</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;partitions&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;first&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">0</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;log_dirs&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;first&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">1</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;log_dirs&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;first&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">2</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;log_dirs&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;any&quot;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


</li>
<li><p>执行副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server node1:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line"></span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started partition reassignments for first-0,first-1,first-2</span><br></pre></td></tr></table></figure>


</li>
<li><p>验证副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# bin/kafka-reassign-partitions.sh --bootstrap-server node1:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br><span class="line"></span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition first-0 is still in progress.</span><br><span class="line">Reassignment of partition first-1 is still in progress.</span><br><span class="line">Reassignment of partition first-2 is completed.</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="执行停止命令"><a href="#执行停止命令" class="headerlink" title="执行停止命令"></a>执行停止命令</h3><p>在 node4上执行停止命令即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node4 kafka]# ./bin/kafka-server-stop.sh </span><br></pre></td></tr></table></figure>



<h1 id="Kafka副本"><a href="#Kafka副本" class="headerlink" title="Kafka副本"></a>Kafka副本</h1><h2 id="副本基本信息"><a href="#副本基本信息" class="headerlink" title="副本基本信息"></a>副本基本信息</h2><p>（1）Kafka 副本作用：提高数据可靠性。<br>（2）Kafka 默认副本 1 个，生产环境一般配置为 2 个，保证数据可靠性；太多副本会增加磁盘存储空间，增加网络上数据传输，降低效率。<br>（3）Kafka 中副本分为：Leader 和 Follower。Kafka 生产者只会把数据发往 Leader，然后 Follower 找 Leader 进行同步数据。<br>（4）Kafka 分区中的所有副本统称为 AR（Assigned Repllicas）。<br>    AR &#x3D; ISR + OSR<br>    ISR，表示和 Leader 保持同步的 Follower 集合。如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值由 replica.lag.time.max.ms参数设定，默认 30s。Leader 发生故障之后，就会从 ISR 中选举新的 Leader。<br>    OSR，表示 Follower 与 Leader 副本同步时，延迟过多的副本。</p>
<h2 id="Leader选举流程"><a href="#Leader选举流程" class="headerlink" title="Leader选举流程"></a>Leader选举流程</h2><p>Kafka 集群中有一个 broker 的 Controller 会被选举为 Controller Leader，负责管理集群broker 的上下线，所有 topic 的分区副本分配和 Leader 选举等工作。<br>Controller 的信息同步工作是依赖于 Zookeeper 的。</p>
<p><img src="/2022/03/11/Kafka-Broker/image-20230128163849451.png" alt="image-20230128163849451"></p>
<ol>
<li><p>创建一个新的 topic，4 个分区，4 个副本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --create --topic second --partitions 4 --replication-factor 4</span><br><span class="line">Created topic second.</span><br></pre></td></tr></table></figure>


</li>
<li><p>查看 Leader 分布情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --describe --topic second</span><br><span class="line">Topic: second   TopicId: xfv10IihSHeTUtAECrXYZA PartitionCount: 4       ReplicationFactor: 4    Configs: </span><br><span class="line">        Topic: second   Partition: 0    Leader: 3       Replicas: 3,1,0,2       Isr: 3,1,0,2</span><br><span class="line">        Topic: second   Partition: 1    Leader: 1       Replicas: 1,0,2,3       Isr: 1,0,2,3</span><br><span class="line">        Topic: second   Partition: 2    Leader: 0       Replicas: 0,2,3,1       Isr: 0,2,1,3</span><br><span class="line">        Topic: second   Partition: 3    Leader: 2       Replicas: 2,3,1,0       Isr: 2,1,0,3</span><br></pre></td></tr></table></figure>


</li>
<li><p>停止掉node4的 kafka 进程，并查看 Leader 分区情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node4 kafka]# ./bin/kafka-server-stop.sh </span><br><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --describe --topic second</span><br><span class="line">Topic: second   TopicId: xfv10IihSHeTUtAECrXYZA PartitionCount: 4       ReplicationFactor: 4    Configs: </span><br><span class="line">        Topic: second   Partition: 0    Leader: 1       Replicas: 3,1,0,2       Isr: 1,0,2</span><br><span class="line">        Topic: second   Partition: 1    Leader: 1       Replicas: 1,0,2,3       Isr: 1,0,2</span><br><span class="line">        Topic: second   Partition: 2    Leader: 0       Replicas: 0,2,3,1       Isr: 0,2,1</span><br><span class="line">        Topic: second   Partition: 3    Leader: 2       Replicas: 2,3,1,0       Isr: 2,1,0</span><br></pre></td></tr></table></figure>


</li>
<li><p>停止掉node3的 kafka 进程，并查看 Leader 分区情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node3 kafka]# ./bin/kafka-server-stop.sh </span><br><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --describe --topic second</span><br><span class="line">Topic: second   TopicId: xfv10IihSHeTUtAECrXYZA PartitionCount: 4       ReplicationFactor: 4    Configs: </span><br><span class="line">        Topic: second   Partition: 0    Leader: 1       Replicas: 3,1,0,2       Isr: 1,0</span><br><span class="line">        Topic: second   Partition: 1    Leader: 1       Replicas: 1,0,2,3       Isr: 1,0</span><br><span class="line">        Topic: second   Partition: 2    Leader: 0       Replicas: 0,2,3,1       Isr: 0,1</span><br><span class="line">        Topic: second   Partition: 3    Leader: 1       Replicas: 2,3,1,0       Isr: 1,0</span><br></pre></td></tr></table></figure>


</li>
<li><p>启动node4的 kafka 进程，并查看 Leader 分区情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node4 kafka]# ./bin/kafka-server-start.sh -daemon ./config/server.properties </span><br><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --describe --topic second</span><br><span class="line">Topic: second   TopicId: xfv10IihSHeTUtAECrXYZA PartitionCount: 4       ReplicationFactor: 4    Configs: </span><br><span class="line">        Topic: second   Partition: 0    Leader: 1       Replicas: 3,1,0,2       Isr: 1,0,3</span><br><span class="line">        Topic: second   Partition: 1    Leader: 1       Replicas: 1,0,2,3       Isr: 1,0,3</span><br><span class="line">        Topic: second   Partition: 2    Leader: 0       Replicas: 0,2,3,1       Isr: 0,1,3</span><br><span class="line">        Topic: second   Partition: 3    Leader: 1       Replicas: 2,3,1,0       Isr: 1,0,3</span><br></pre></td></tr></table></figure>


</li>
<li><p>启动node3的 kafka 进程，并查看 Leader 分区情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node3 kafka]# ./bin/kafka-server-start.sh -daemon ./config/server.properties </span><br><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --describe --topic second</span><br><span class="line">Topic: second   TopicId: xfv10IihSHeTUtAECrXYZA PartitionCount: 4       ReplicationFactor: 4    Configs: </span><br><span class="line">        Topic: second   Partition: 0    Leader: 3       Replicas: 3,1,0,2       Isr: 1,0,3,2</span><br><span class="line">        Topic: second   Partition: 1    Leader: 1       Replicas: 1,0,2,3       Isr: 1,0,3,2</span><br><span class="line">        Topic: second   Partition: 2    Leader: 0       Replicas: 0,2,3,1       Isr: 0,1,3,2</span><br><span class="line">        Topic: second   Partition: 3    Leader: 1       Replicas: 2,3,1,0       Isr: 1,0,3,2</span><br></pre></td></tr></table></figure>


</li>
<li><p>停止掉node2的 kafka 进程，并查看 Leader 分区情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node2 kafka]# ./bin/kafka-server-stop.sh </span><br><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --describe --topic second</span><br><span class="line">Topic: second   TopicId: xfv10IihSHeTUtAECrXYZA PartitionCount: 4       ReplicationFactor: 4    Configs: </span><br><span class="line">        Topic: second   Partition: 0    Leader: 3       Replicas: 3,1,0,2       Isr: 0,3,2</span><br><span class="line">        Topic: second   Partition: 1    Leader: 0       Replicas: 1,0,2,3       Isr: 0,3,2</span><br><span class="line">        Topic: second   Partition: 2    Leader: 0       Replicas: 0,2,3,1       Isr: 0,3,2</span><br><span class="line">        Topic: second   Partition: 3    Leader: 2       Replicas: 2,3,1,0       Isr: 0,3,2</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Leader-和-Follower-故障处理细节"><a href="#Leader-和-Follower-故障处理细节" class="headerlink" title="Leader 和 Follower 故障处理细节"></a>Leader 和 Follower 故障处理细节</h2><h3 id="Follower故障处理细节"><a href="#Follower故障处理细节" class="headerlink" title="Follower故障处理细节"></a>Follower故障处理细节</h3><ol>
<li>Follower发生故障后会被临时踢出ISR</li>
<li>这个期间Leader和Follower继续接收数据</li>
<li>待该Follower恢复后，Follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向Leader进行同步。</li>
<li>等该Follower的LEO大于等于该Partition的HW，即Follower追上Leader之后，就可以重新加入ISR了。</li>
</ol>
<p><img src="/2022/03/11/Kafka-Broker/image-20230129133820516.png" alt="image-20230129133820516"></p>
<h3 id="Leader故障处理细节"><a href="#Leader故障处理细节" class="headerlink" title="Leader故障处理细节"></a>Leader故障处理细节</h3><ol>
<li>Leader发生故障之后，会从ISR中选出一个新的Leader</li>
<li>为保证多个副本之间的数据一致性，其余的Follower会先将各自的log文件高于HW的部分截掉，然后从新的Leader同步数据。</li>
</ol>
<p>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</p>
<p><img src="/2022/03/11/Kafka-Broker/image-20230129134734692.png" alt="image-20230129134734692"></p>
<h2 id="分区副本分配"><a href="#分区副本分配" class="headerlink" title="分区副本分配"></a>分区副本分配</h2><p>如果 kafka 服务器只有 4 个节点，那么设置 kafka 的分区数大于服务器台数，在 kafka底层如何分配存储副本呢？</p>
<blockquote>
<p>创建 16 分区，3 个副本</p>
</blockquote>
<ol>
<li><p>创建一个新的 topic，名称为 third。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --create --partitions 16 --replication-factor 3 --topic third</span><br></pre></td></tr></table></figure>


</li>
<li><p>查看分区和副本情况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --describe --topic third</span><br><span class="line">Topic: third    TopicId: Nxbo8gxaSKaRSiYVrwChNA PartitionCount: 16      ReplicationFactor: 3    Configs: </span><br><span class="line">        Topic: third    Partition: 0    Leader: 0       Replicas: 0,2,3 Isr: 0,2,3</span><br><span class="line">        Topic: third    Partition: 1    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1</span><br><span class="line">        Topic: third    Partition: 2    Leader: 3       Replicas: 3,1,0 Isr: 3,1,0</span><br><span class="line">        Topic: third    Partition: 3    Leader: 1       Replicas: 1,0,2 Isr: 1,0,2</span><br><span class="line">        Topic: third    Partition: 4    Leader: 0       Replicas: 0,3,1 Isr: 0,3,1</span><br><span class="line">        Topic: third    Partition: 5    Leader: 2       Replicas: 2,1,0 Isr: 2,1,0</span><br><span class="line">        Topic: third    Partition: 6    Leader: 3       Replicas: 3,0,2 Isr: 3,0,2</span><br><span class="line">        Topic: third    Partition: 7    Leader: 1       Replicas: 1,2,3 Isr: 1,2,3</span><br><span class="line">        Topic: third    Partition: 8    Leader: 0       Replicas: 0,1,2 Isr: 0,1,2</span><br><span class="line">        Topic: third    Partition: 9    Leader: 2       Replicas: 2,0,3 Isr: 2,0,3</span><br><span class="line">        Topic: third    Partition: 10   Leader: 3       Replicas: 3,2,1 Isr: 3,2,1</span><br><span class="line">        Topic: third    Partition: 11   Leader: 1       Replicas: 1,3,0 Isr: 1,3,0</span><br><span class="line">        Topic: third    Partition: 12   Leader: 0       Replicas: 0,2,3 Isr: 0,2,3</span><br><span class="line">        Topic: third    Partition: 13   Leader: 2       Replicas: 2,3,1 Isr: 2,3,1</span><br><span class="line">        Topic: third    Partition: 14   Leader: 3       Replicas: 3,1,0 Isr: 3,1,0</span><br><span class="line">        Topic: third    Partition: 15   Leader: 1       Replicas: 1,0,2 Isr: 1,0,2</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="经验"><a href="#经验" class="headerlink" title="经验"></a>经验</h2><h3 id="手动调整分区副本存储"><a href="#手动调整分区副本存储" class="headerlink" title="手动调整分区副本存储"></a>手动调整分区副本存储</h3><p>在生产环境中，每台服务器的配置和性能不一致，但是Kafka只会根据自己的代码规则创建对应的分区副本，就会导致个别服务器存储压力较大。所有需要手动调整分区副本的存储。</p>
<p>需求：创建一个新的topic，4个分区，两个副本，名称为third。将 该topic的所有副本都存储到broker0和broker1两台服务器上。</p>
<p>手动调整分区副本存储的步骤如下：</p>
<ol>
<li><p>创建一个新的 topic，名称为 third。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --create --partitions 4 --replication-factor 2 --topic third</span><br></pre></td></tr></table></figure>


</li>
<li><p>查看分区副本存储情况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --describe --topic third</span><br><span class="line">Topic: third    TopicId: wkYxVGxPTk6tyQ5ypm-yHg PartitionCount: 4       ReplicationFactor: 2    Configs: </span><br><span class="line">        Topic: third    Partition: 0    Leader: 3       Replicas: 3,1   Isr: 3,1</span><br><span class="line">        Topic: third    Partition: 1    Leader: 1       Replicas: 1,0   Isr: 1,0</span><br><span class="line">        Topic: third    Partition: 2    Leader: 0       Replicas: 0,2   Isr: 0,2</span><br><span class="line">        Topic: third    Partition: 3    Leader: 2       Replicas: 2,3   Isr: 2,3</span><br></pre></td></tr></table></figure>


</li>
<li><p>创建副本存储计划（所有副本都指定存储在 broker0、broker1 中）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# vim increase-replication-factor.json</span><br></pre></td></tr></table></figure>

<p>内容：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;partitions&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;third&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">1</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;third&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">1</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;third&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">0</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;third&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">0</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>执行副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-reassign-partitions.sh --bootstrap-server node1:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br></pre></td></tr></table></figure>


</li>
<li><p>验证副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-reassign-partitions.sh --bootstrap-server node1:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br></pre></td></tr></table></figure>


</li>
<li><p>查看分区副本存储情况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --describe --topic third</span><br><span class="line">Topic: third    TopicId: wkYxVGxPTk6tyQ5ypm-yHg PartitionCount: 4       ReplicationFactor: 2    Configs: </span><br><span class="line">        Topic: third    Partition: 0    Leader: 0       Replicas: 0,1   Isr: 1,0</span><br><span class="line">        Topic: third    Partition: 1    Leader: 1       Replicas: 0,1   Isr: 1,0</span><br><span class="line">        Topic: third    Partition: 2    Leader: 0       Replicas: 1,0   Isr: 0,1</span><br><span class="line">        Topic: third    Partition: 3    Leader: 1       Replicas: 1,0   Isr: 0,1</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="Leader-Partition自动平衡"><a href="#Leader-Partition自动平衡" class="headerlink" title="Leader Partition自动平衡"></a>Leader Partition自动平衡</h3><p>正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压力过高，其他宕机的broker重启之后都是follower partition，读写请求很低，造成集群负载不均衡。</p>
<p>下面拿一个主题举例说明，假设集群只有一个主题如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Topic: second   TopicId: xfv10IihSHeTUtAECrXYZA PartitionCount: 4       ReplicationFactor: 4    Configs: </span><br><span class="line">        Topic: second   Partition: 0    Leader: 0       Replicas: 3,1,0,2       Isr: 3,1,0,2</span><br><span class="line">        Topic: second   Partition: 1    Leader: 1       Replicas: 1,0,2,3       Isr: 1,0,2,3</span><br><span class="line">        Topic: second   Partition: 2    Leader: 2       Replicas: 0,2,3,1       Isr: 0,2,1,3</span><br><span class="line">        Topic: second   Partition: 3    Leader: 3       Replicas: 2,3,1,0       Isr: 2,1,0,3</span><br></pre></td></tr></table></figure>

<p>针对broker0节点，分区2的AR优先副本是0节点，但是0节点却不是Leader节点，所以不平衡数加1，AR副本总数是4。所以broker0节点不平衡率为1&#x2F;4&gt;10%，需要再平衡。</p>
<p>broker2和broker3节点和broker0不平衡率一样，需要再平衡。Broker1的不平衡数为0，不需要再平衡。</p>
<p>参数：</p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>auto.leader.rebalance.enable</td>
<td>默认是 true。 自动 Leader Partition 平衡。生产环境中，leader 重选举的代价比较大，可能会带来性能影响，建议设置为 false 关闭。</td>
</tr>
<tr>
<td>leader.imbalance.per.broker.percentage</td>
<td>默认是 10%。每个 broker 允许的不平衡的 leader的比率。如果每个 broker 超过了这个值，控制器会触发 leader 的平衡。</td>
</tr>
<tr>
<td>leader.imbalance.check.interval.seconds</td>
<td>默认值 300 秒。检查 leader 负载是否平衡的间隔时间。</td>
</tr>
</tbody></table>
<h3 id="增加副本因子"><a href="#增加副本因子" class="headerlink" title="增加副本因子"></a>增加副本因子</h3><p>在生产环境当中，由于某个主题的重要等级需要提升，我们考虑增加副本。副本数的增加需要先制定计划，然后根据计划执行。</p>
<ol>
<li><p>创建topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --create --partitions 3 --replication-factor 1 --topic four</span><br></pre></td></tr></table></figure>


</li>
<li><p>手动增加副本存储</p>
<p>（1）创建副本存储计划（所有副本都指定存储在 broker0、broker1、broker2 中）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# vim increase-replication-factor.json</span><br></pre></td></tr></table></figure>

<p>内容：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;partitions&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;four&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">2</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;four&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">2</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span><span class="string">&quot;four&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;partition&quot;</span><span class="punctuation">:</span><span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;replicas&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">                <span class="number">2</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>



<p>（2）执行副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-reassign-partitions.sh --bootstrap-server node1:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h1><h2 id="文件存储机制"><a href="#文件存储机制" class="headerlink" title="文件存储机制"></a>文件存储机制</h2><h3 id="Topic-数据的存储机制"><a href="#Topic-数据的存储机制" class="headerlink" title="Topic 数据的存储机制"></a>Topic 数据的存储机制</h3><p>Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment包括：“.index”文件、“.log”文件和.timeindex等文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号，例如：first-0。</p>
<p><img src="/2022/03/11/Kafka-Broker/image-20230129153950603.png" alt="image-20230129153950603"></p>
<h3 id="Topic数据存储位置"><a href="#Topic数据存储位置" class="headerlink" title="Topic数据存储位置"></a>Topic数据存储位置</h3><ol>
<li><p>启动生产者，并发送消息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-producer.sh --bootstrap-server node1:9092 --topic first</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello world</span></span><br><span class="line"><span class="meta prompt_">&gt;</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>查看 node1（或者 node2、node3）的&#x2F;export&#x2F;server&#x2F;kafka&#x2F;datas&#x2F;first-1（first-0、first-2）路径上的文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 first-1]# ls</span><br><span class="line">00000000000000000000.index  </span><br><span class="line">00000000000000000000.log  </span><br><span class="line">00000000000000000000.timeindex  </span><br><span class="line">00000000000000000051.snapshot  </span><br><span class="line">leader-epoch-checkpoint  </span><br><span class="line">partition.metadata</span><br></pre></td></tr></table></figure>


</li>
<li><p>直接查看 log 日志，发现是乱码。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 first-1]# cat ./00000000000000000000.log </span><br></pre></td></tr></table></figure>


</li>
<li><p>通过工具查看 index 和 log 信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 first-1]# kafka-run-class.sh kafka.tools.DumpLogSegments --files ./00000000000000000000.index </span><br><span class="line">Dumping ./00000000000000000000.index</span><br><span class="line">offset: 0 position: 0</span><br><span class="line">Mismatches in :/export/server/kafka/datas/first-1/./00000000000000000000.index</span><br><span class="line">  Index offset: 0, log offset: 2</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="Log文件和Index文件详解"><a href="#Log文件和Index文件详解" class="headerlink" title="Log文件和Index文件详解"></a>Log文件和Index文件详解</h3><p><img src="/2022/03/11/Kafka-Broker/image-20230129161142188.png" alt="image-20230129161142188"></p>
<p>说明：日志存储参数配置</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>log.segment.bytes</td>
<td>Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分成块的大小，默认值 1G。</td>
</tr>
<tr>
<td>log.index.interval.bytes</td>
<td>默认 4kb，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引。 稀疏索引。</td>
</tr>
</tbody></table>
<h3 id="文件清理策略"><a href="#文件清理策略" class="headerlink" title="文件清理策略"></a>文件清理策略</h3><p>Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间。</p>
<ul>
<li><p>log.retention.hours，最低优先级小时，默认 7 天。</p>
</li>
<li><p>log.retention.minutes，分钟。</p>
</li>
<li><p>log.retention.ms，最高优先级毫秒。</p>
</li>
<li><p>log.retention.check.interval.ms，负责设置检查周期，默认 5 分钟。</p>
</li>
</ul>
<p>那么日志一旦超过了设置的时间，怎么处理呢？<br>Kafka 中提供的日志清理策略有 delete 和 compact 两种。</p>
<p><strong>delete</strong></p>
<p>delete 日志删除：将过期数据删除</p>
<ul>
<li>log.cleanup.policy &#x3D; delete 所有数据启用删除策略</li>
</ul>
<p>（1）基于时间：默认打开。以 segment 中所有记录中的最大时间戳作为该文件时间戳。<br>（2）基于大小：默认关闭。超过设置的所有日志总大小，删除最早的 segment。log.retention.bytes，默认等于-1，表示无穷大。</p>
<p><strong>compact</strong></p>
<p>compact日志压缩：对于相同key的不同value值，只保留最后一个版本。</p>
<ul>
<li>log.cleanup.policy &#x3D; compact 所有数据启用压缩策略</li>
</ul>
<p><img src="/2022/03/11/Kafka-Broker/image-20230129162439945.png" alt="image-20230129162439945"></p>
<p>压缩后的offset可能是不连续的，比如上图中没有6，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费。<br>这种策略只适合特殊场景，比如消息的key是用户ID，value是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。</p>
<h1 id="高效读写数据"><a href="#高效读写数据" class="headerlink" title="高效读写数据"></a>高效读写数据</h1><p>1）Kafka 本身是分布式集群，可以采用分区技术，并行度高<br>2）读数据采用稀疏索引，可以快速定位要消费的数据<br>3）顺序写磁盘<br>Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到 600M&#x2F;s，而随机写只有 100K&#x2F;s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。<br>4）页缓存 + 零拷贝技术<br><strong>零拷贝</strong>：Kafka的数据加工处理操作交由Kafka生产者和Kafka消费者处理。Kafka Broker应用层不关心存储的数据，所以就不用走应用层，传输效率高。<br><strong>PageCache页缓存</strong>：Kafka重度依赖底层操作系统提供的PageCache功 能。当上层有写操作时，操作系统只是将数据写入PageCache。当读操作发生时，先从PageCache中查找，如果找不到，再去磁盘中读取。实际上PageCache是把尽可能多的空闲内存都当做了磁盘缓存来使用。</p>
<p><img src="/2022/03/11/Kafka-Broker/image-20230129163213076.png" alt="image-20230129163213076"></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>log.flush.interval.messages</td>
<td>强制页缓存刷写到磁盘的条数，默认是 long 的最大值，9223372036854775807。一般不建议修改，交给系统自己管理。</td>
</tr>
<tr>
<td>log.flush.interval.ms</td>
<td>每隔多久，刷数据到磁盘，默认是 null。一般不建议修改，交给系统自己管理。</td>
</tr>
</tbody></table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/" class="post-title-link" itemprop="url">Kafka生产者</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-10 20:00:29" itemprop="dateCreated datePublished" datetime="2022-03-10T20:00:29+08:00">2022-03-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-28 17:53:08" itemprop="dateModified" datetime="2023-01-28T17:53:08+08:00">2023-01-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="生产者消息发送流程"><a href="#生产者消息发送流程" class="headerlink" title="生产者消息发送流程"></a>生产者消息发送流程</h1><h2 id="发送原理"><a href="#发送原理" class="headerlink" title="发送原理"></a>发送原理</h2><p>在消息发送的过程中，涉及到了<strong>两个线程——main 线程和 Sender 线程</strong>。在 main 线程中创建了<strong>一个双端队列 RecordAccumulator</strong>。main 线程将消息发送给 RecordAccumulator，Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka Broker。</p>
<p><strong>发送流程</strong></p>
<p><img src="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/image-20230128101123528.png" alt="image-20230128101123528"></p>
<h2 id="生产者重要参数"><a href="#生产者重要参数" class="headerlink" title="生产者重要参数"></a>生产者重要参数</h2><table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>bootstrap.servers</td>
<td>生产者连接集群所需的 broker 地 址 清 单 。 例如node1:9092,node2:9092,node3:9092，可以设置1个或者多个，中间用逗号隔开。注意这里并非需要所有的 broker 地址，因为生产者从给定的broker里查找到其他broker信息。</td>
</tr>
<tr>
<td>key.serializer 和 value.serializer</td>
<td>指定发送消息的 key 和 value 的序列化类型。一定要写全类名。</td>
</tr>
<tr>
<td>buffer.memory</td>
<td>RecordAccumulator 缓冲区总大小，默认 32m。</td>
</tr>
<tr>
<td>batch.size</td>
<td>缓冲区一批数据最大值，默认 16k。适当增加该值，可以提高吞吐量，但是如果该值设置太大，会导致数据传输延迟增加。</td>
</tr>
<tr>
<td>linger.ms</td>
<td>如果数据迟迟未达到 batch.size，sender 等待 linger.time之后就会发送数据。单位 ms，默认值是 0ms，表示没有延迟。生产环境建议该值大小为 5-100ms 之间。</td>
</tr>
<tr>
<td>acks</td>
<td>0：生产者发送过来的数据，不需要等数据落盘应答。1：生产者发送过来的数据，Leader 收到数据后应答。-1（all）：生产者发送过来的数据，Leader+和 isr 队列里面的所有节点收齐数据后应答。默认值是-1，-1和all是等价的。</td>
</tr>
<tr>
<td>max.in.flight.requests.per.connection</td>
<td>允许最多没有返回 ack 的次数，默认为 5，开启幂等性要保证该值是 1-5 的数字。</td>
</tr>
<tr>
<td>retries</td>
<td>当消息发送出现错误的时候，系统会重发消息。retries表示重试次数。默认是 int 最大值，2147483647。如果设置了重试，还想保证消息的有序性，需要设置MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION&#x3D;1否则在重试此失败消息的时候，其他的消息可能发送成功了。</td>
</tr>
<tr>
<td>retry.backoff.ms</td>
<td>两次重试之间的时间间隔，默认是 100ms。</td>
</tr>
<tr>
<td>enable.idempotence</td>
<td>是否开启幂等性，默认 true，开启幂等性。</td>
</tr>
<tr>
<td>compression.type</td>
<td>生产者发送的所有数据的压缩方式。默认是 none，也就是不压缩。支持压缩类型：none、gzip、snappy、lz4 和 zstd。</td>
</tr>
</tbody></table>
<h1 id="异步发送API"><a href="#异步发送API" class="headerlink" title="异步发送API"></a>异步发送API</h1><h2 id="普通异步发送"><a href="#普通异步发送" class="headerlink" title="普通异步发送"></a>普通异步发送</h2><h3 id="异步发送流程"><a href="#异步发送流程" class="headerlink" title="异步发送流程"></a>异步发送流程</h3><p>创建 Kafka 生产者，采用异步的方式发送到 Kafka Broker</p>
<p><img src="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/image-20230128102902905.png" alt="image-20230128102902905"></p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><ol>
<li>创建工程 kafka</li>
<li>导入依赖</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>创建包名：com.st.kafka.producer</p>
</li>
<li><p>编写不带回调函数的 API 代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建kafka生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.配置bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key,value 序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.调用send发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;msg&quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>在node1上开启Kafka消费者</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic first</span><br></pre></td></tr></table></figure>


</li>
<li><p>在idea上执行代码，观察node1控制台是否接收到消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic first</span><br><span class="line">msg0</span><br><span class="line">msg1</span><br><span class="line">msg2</span><br><span class="line">msg3</span><br><span class="line">msg4</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="带回调函数的异步发送"><a href="#带回调函数的异步发送" class="headerlink" title="带回调函数的异步发送"></a>带回调函数的异步发送</h2><p>回调函数会在 producer 收到 ack 时调用，为异步调用，该方法有两个参数，分别是元数据信息（RecordMetadata）和异常信息（Exception），如果 Exception 为 null，说明消息发送成功，如果 Exception 不为 null，说明消息发送失败。</p>
<p><img src="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/image-20230128133814692.png" alt="image-20230128133814692"></p>
<h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><ol>
<li><p>代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerCallback</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1.创建kafka生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.配置bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key,value 序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.调用send发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;msg&quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == <span class="literal">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;主题：&quot;</span> + recordMetadata.topic() + <span class="string">&quot; -&gt; &quot;</span> + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition());</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="comment">// 延迟一会会看到数据发往不同分区</span></span><br><span class="line">            Thread.sleep(<span class="number">10</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


</li>
<li><p>在node1上开启Kafka消费者</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic first</span><br></pre></td></tr></table></figure>


</li>
<li><p>在idea上执行代码，观察node1控制台是否接收到消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic first</span><br><span class="line">msg0</span><br><span class="line">msg1</span><br><span class="line">msg2</span><br><span class="line">msg3</span><br><span class="line">msg4</span><br></pre></td></tr></table></figure>
</li>
<li><p>idea控制台输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">主题：first -&gt; 分区：0</span><br><span class="line">主题：first -&gt; 分区：0</span><br><span class="line">主题：first -&gt; 分区：0</span><br><span class="line">主题：first -&gt; 分区：0</span><br><span class="line">主题：first -&gt; 分区：0</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="同步发送API"><a href="#同步发送API" class="headerlink" title="同步发送API"></a>同步发送API</h1><h2 id="同步发送流程"><a href="#同步发送流程" class="headerlink" title="同步发送流程"></a>同步发送流程</h2><p><img src="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/image-20230128142544078.png" alt="image-20230128142544078"></p>
<h2 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h2><p>只需在异步发送的基础上，再调用一下 get()方法即可。</p>
<p>代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerSync</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1.创建kafka生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.配置bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key,value 序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.调用send发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;sync msg&quot;</span> + i)).get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="生产者分区"><a href="#生产者分区" class="headerlink" title="生产者分区"></a>生产者分区</h1><h2 id="分区好处"><a href="#分区好处" class="headerlink" title="分区好处"></a>分区好处</h2><p>（1）<strong>便于合理使用存储资源</strong>，每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果。</p>
<p>（2）<strong>提高并行度</strong>，生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据。</p>
<p><img src="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/image-20230128143129733.png" alt="image-20230128143129733"></p>
<h2 id="生产者发送消息的分区策略"><a href="#生产者发送消息的分区策略" class="headerlink" title="生产者发送消息的分区策略"></a>生产者发送消息的分区策略</h2><h3 id="默认的分区器-DefaultPartitioner"><a href="#默认的分区器-DefaultPartitioner" class="headerlink" title="默认的分区器 DefaultPartitioner"></a>默认的分区器 DefaultPartitioner</h3><p><img src="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/image-20230128143815346.png" alt="image-20230128143815346"></p>
<h4 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h4><p>将数据发往指定 partition的情况下，例如，将所有数据发往分区 1 中。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerCallbackPartitions</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1.创建kafka生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.配置bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key,value 序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.调用send发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="comment">//指定数据发送到 1 号分区，key 为空</span></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="number">1</span>, <span class="string">&quot;&quot;</span>,<span class="string">&quot;msg&quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == <span class="literal">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;主题：&quot;</span> + recordMetadata.topic() + <span class="string">&quot; -&gt; &quot;</span> + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition());</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>idea控制台输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">主题：first -&gt; 分区：1</span><br><span class="line">主题：first -&gt; 分区：1</span><br><span class="line">主题：first -&gt; 分区：1</span><br><span class="line">主题：first -&gt; 分区：1</span><br><span class="line">主题：first -&gt; 分区：1</span><br></pre></td></tr></table></figure>

<h4 id="案例2"><a href="#案例2" class="headerlink" title="案例2"></a>案例2</h4><p>没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerCallback</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1.创建kafka生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.配置bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key,value 序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.调用send发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="comment">// 依次指定 key 值为 a,b,f ，数据 key 的 hash 值与 3 个分区求余，分别发往 1、2、0</span></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;msg&quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == <span class="literal">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;主题：&quot;</span> + recordMetadata.topic() + <span class="string">&quot; -&gt; &quot;</span> + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition());</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="comment">// 延迟一会会看到数据发往不同分区</span></span><br><span class="line">            Thread.sleep(<span class="number">10</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>idea控制台结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">key为a的输出</span><br><span class="line">主题：first -&gt; 分区：1</span><br><span class="line">主题：first -&gt; 分区：1</span><br><span class="line">主题：first -&gt; 分区：1</span><br><span class="line">主题：first -&gt; 分区：1</span><br><span class="line">主题：first -&gt; 分区：1</span><br><span class="line">key为b的输出</span><br><span class="line">主题：first -&gt; 分区：2</span><br><span class="line">主题：first -&gt; 分区：2</span><br><span class="line">主题：first -&gt; 分区：2</span><br><span class="line">主题：first -&gt; 分区：2</span><br><span class="line">主题：first -&gt; 分区：2</span><br><span class="line">key为f的输出</span><br><span class="line">主题：first -&gt; 分区：0</span><br><span class="line">主题：first -&gt; 分区：0</span><br><span class="line">主题：first -&gt; 分区：0</span><br><span class="line">主题：first -&gt; 分区：0</span><br><span class="line">主题：first -&gt; 分区：0</span><br></pre></td></tr></table></figure>



<h3 id="自定义分区器"><a href="#自定义分区器" class="headerlink" title="自定义分区器"></a>自定义分区器</h3><p>根据自身需求，自己重新实现分区器。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Partitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.Cluster;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 1. 实现接口 Partitioner</span></span><br><span class="line"><span class="comment"> * 2. 实现 3 个方法:partition,close,configure</span></span><br><span class="line"><span class="comment"> * 3. 编写 partition 方法,返回分区号</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyPartitioner</span> <span class="keyword">implements</span> <span class="title class_">Partitioner</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 返回信息对应的分区, 发送的数据包含kafka发0号分区，否则发1号分区</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> s 主题</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> o 消息的key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> bytes 消息的 key 序列化后的字节数组</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> o1 消息的 value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> bytes1 消息的 value 序列化后的字节数组</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cluster 集群元数据可以查看分区信息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(String s, Object o, <span class="type">byte</span>[] bytes, Object o1, <span class="type">byte</span>[] bytes1, Cluster cluster)</span> &#123;</span><br><span class="line">        <span class="comment">// 获取消息</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">msgValue</span> <span class="operator">=</span> o1.toString();</span><br><span class="line">        <span class="comment">// 创建 partition</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line">        <span class="comment">// 判断消息是否包含 kafka</span></span><br><span class="line">        <span class="keyword">if</span> (msgValue.contains(<span class="string">&quot;kafka&quot;</span>))&#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 返回分区号</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Map&lt;String, ?&gt; map)</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用分区</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerCallbackPartitions</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1.创建kafka生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.配置bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key,value 序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 添加自定义分区器</span></span><br><span class="line">        properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MyPartitioner.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        String[] chars = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.调用send发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="comment">//指定数据发送到 1 号分区，key 为空</span></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;kafk&quot;</span> + chars[i % chars.length] + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == <span class="literal">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;主题：&quot;</span> + recordMetadata.topic() + <span class="string">&quot; -&gt; &quot;</span> + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition());</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>idea控制台输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">主题：first -&gt; 分区：1</span><br><span class="line">主题：first -&gt; 分区：1</span><br><span class="line">主题：first -&gt; 分区：1</span><br><span class="line">主题：first -&gt; 分区：0</span><br><span class="line">主题：first -&gt; 分区：0</span><br></pre></td></tr></table></figure>



<h1 id="经验"><a href="#经验" class="headerlink" title="经验"></a>经验</h1><h2 id="生产者如何提高吞吐量"><a href="#生产者如何提高吞吐量" class="headerlink" title="生产者如何提高吞吐量"></a>生产者如何提高吞吐量</h2><ul>
<li>batch.size：批次大小，默认16k </li>
<li>linger.ms：等待时间，修改为5-100ms</li>
<li>compression.type：压缩snappy</li>
<li>RecordAccumulator：缓冲区大小，修改为64m</li>
</ul>
<p>代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerParameters</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建kafka生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.配置bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key,value 序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// batch.size：批次大小，默认 16K</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line">        <span class="comment">// linger.ms：等待时间，默认 0</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// RecordAccumulator：缓冲区大小，默认 32M：buffer.memory</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line">        <span class="comment">// compression.type：压缩，默认 none，可配置值 gzip、snappy、lz4 和 zstd</span></span><br><span class="line">        properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, <span class="string">&quot;snappy&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.调用send发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;msg&quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="数据可靠性"><a href="#数据可靠性" class="headerlink" title="数据可靠性"></a>数据可靠性</h2><h3 id="ack-应答原理"><a href="#ack-应答原理" class="headerlink" title="ack 应答原理"></a>ack 应答原理</h3><p>ACK应答级别</p>
<p><strong>0</strong></p>
<p><img src="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/image-20230128154952708.png" alt="image-20230128154952708"></p>
<p><strong>1</strong></p>
<p><img src="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/image-20230128155018634.png" alt="image-20230128155018634"></p>
<p><strong>-1（all）</strong></p>
<p><img src="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/image-20230128155120815.png" alt="image-20230128155120815"></p>
<p><strong>思考</strong>：Leader收到数据，所有Follower都开始同步数据，但有一个Follower，因为某种故障，迟迟不能与Leader进行同步，那这个问题怎么解决呢？<br>Leader维护了一个动态的in-sync replica set（ISR），意为和Leader保持同步的Follower+Leader集合(leader：0，isr:0,1,2)。如果Follower长时间未向Leader发送通信请求或同步数据，则该Follower将被踢出ISR。该时间阈值由replica.lag.time.max.ms参数设定，默认30s。例如2超时，(leader:0, isr:0,1)。这样就不用等长期联系不上或者已经故障的节点。</p>
<p>如果分区副本设置为1个，或 者ISR里应答的最小副本数量（ min.insync.replicas 默认为1）设置为1，和ack&#x3D;1的效果是一样的，仍然有丢数的风险（leader：0，isr:0）。</p>
<h3 id="数据完全可靠条件"><a href="#数据完全可靠条件" class="headerlink" title="数据完全可靠条件"></a>数据完全可靠条件</h3><p>数据完全可靠条件 &#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2</p>
<h3 id="可靠性总结"><a href="#可靠性总结" class="headerlink" title="可靠性总结"></a>可靠性总结</h3><p>acks&#x3D;0，生产者发送过来数据就不管了，可靠性差，效率高；<br>acks&#x3D;1，生产者发送过来数据Leader应答，可靠性中等，效率中等；<br>acks&#x3D;-1，生产者发送过来数据Leader和ISR队列里面所有Follwer应答，可靠性高，效率低；<br>在生产环境中，acks&#x3D;0很少使用；acks&#x3D;1，一般用于传输普通日志，允许丢个别数据；acks&#x3D;-1，一般用于传输和钱相关的数据，对可靠性要求比较高的场景。</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerAck</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建kafka生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.配置bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key,value 序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 acks</span></span><br><span class="line">        properties.put(ProducerConfig.ACKS_CONFIG, <span class="string">&quot;all&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 重试次数 retries，默认是 int 最大值，2147483647</span></span><br><span class="line">        properties.put(ProducerConfig.RETRIES_CONFIG, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.调用send发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;msg&quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="数据去重"><a href="#数据去重" class="headerlink" title="数据去重"></a>数据去重</h2><h3 id="数据传递语义"><a href="#数据传递语义" class="headerlink" title="数据传递语义"></a>数据传递语义</h3><ul>
<li>至少一次（At Least Once）&#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2</li>
<li>最多一次（At Most Once）&#x3D; ACK级别设置为0</li>
<li>总结：<br>  At Least Once可以保证数据不丢失，但是不能保证数据不重复；<br>  At Most Once可以保证数据不重复，但是不能保证数据不丢失。</li>
<li>精确一次（Exactly Once）：对于一些非常重要的信息，比如和钱相关的数据，要求数据既不能重复也不丢失。Kafka 0.11版本以后，引入了一项重大特性：幂等性和事务。</li>
</ul>
<h3 id="幂等性"><a href="#幂等性" class="headerlink" title="幂等性"></a>幂等性</h3><h4 id="幂等性原理"><a href="#幂等性原理" class="headerlink" title="幂等性原理"></a>幂等性原理</h4><p>幂等性就是指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。<br>精确一次（Exactly Once） &#x3D; 幂等性 + 至少一次（ ack&#x3D;-1 + 分区副本数&gt;&#x3D;2 + ISR最小副本数量&gt;&#x3D;2） 。<br>重复数据的判断标准：具有&lt;PID, Partition, SeqNumber&gt;相同主键的消息提交时，Broker只会持久化一条。其中PID是Kafka每次重启都会分配一个新的；Partition 表示分区号；Sequence Number是单调自增的。<br>所以幂等性只能保证的是在单分区单会话内不重复。</p>
<h4 id="如何使用幂等性"><a href="#如何使用幂等性" class="headerlink" title="如何使用幂等性"></a>如何使用幂等性</h4><p>开启参数 <strong>enable.idempotence</strong> 默认为 true，false 关闭。</p>
<h3 id="生产者事务"><a href="#生产者事务" class="headerlink" title="生产者事务"></a>生产者事务</h3><h4 id="Kafka-事务原理"><a href="#Kafka-事务原理" class="headerlink" title="Kafka 事务原理"></a>Kafka 事务原理</h4><p><strong>说明：开启事务，必须开启幂等性。</strong></p>
<p><img src="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/image-20230128160946392.png" alt="image-20230128160946392"></p>
<h4 id="Kafka事务API"><a href="#Kafka事务API" class="headerlink" title="Kafka事务API"></a>Kafka事务API</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 初始化事务</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">initTransactions</span><span class="params">()</span>;</span><br><span class="line"><span class="comment">// 2 开启事务</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException;</span><br><span class="line"><span class="comment">// 3 在事务内提交已经消费的偏移量（主要用于消费者）</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, String consumerGroupId)</span> <span class="keyword">throws</span> ProducerFencedException;</span><br><span class="line"><span class="comment">// 4 提交事务</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException;</span><br><span class="line"><span class="comment">// 5 放弃事务（类似于回滚事务的操作）</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException;</span><br></pre></td></tr></table></figure>

<h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><p>单个 Producer，使用事务保证消息的仅一次发送。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerTransactions</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建kafka生产者的配置对象</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.配置bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;node1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key,value 序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置事务 id（必须），事务 id 任意起名</span></span><br><span class="line">        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, <span class="string">&quot;transaction_id_0&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化事务</span></span><br><span class="line">        kafkaProducer.initTransactions();</span><br><span class="line">        <span class="comment">// 开启事务</span></span><br><span class="line">        kafkaProducer.beginTransaction();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 4.调用send发送消息</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">                kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;msg&quot;</span> + i));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 提交事务</span></span><br><span class="line">            kafkaProducer.commitTransaction();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="comment">// 终止事务</span></span><br><span class="line">            kafkaProducer.abortTransaction();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 5.关闭资源</span></span><br><span class="line">            kafkaProducer.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="数据有序"><a href="#数据有序" class="headerlink" title="数据有序"></a>数据有序</h2><p>单分区内，有序（有条件）；</p>
<p>多分区，分区与分区间无序；</p>
<p><img src="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/image-20230128162051709.png" alt="image-20230128162051709"></p>
<h2 id="数据乱序"><a href="#数据乱序" class="headerlink" title="数据乱序"></a>数据乱序</h2><ul>
<li>kafka在1.x版本之前保证数据单分区有序，条件如下：<br>max.in.flight.requests.per.connection&#x3D;1（不需要考虑是否开启幂等性）。</li>
<li>kafka在1.x及以后版本保证数据单分区有序，条件如下：<br>（1）未开启幂等性<br>max.in.flight.requests.per.connection需要设置为1。<br>（2）开启幂等性<br>max.in.flight.requests.per.connection需要设置小于等于5。<br>原因说明：因为在kafka1.x以后，启用幂等后，kafka服务端会缓存producer发来的最近5个request的元数据，故无论如何，都可以保证最近5个request的数据都是有序的。</li>
</ul>
<p><img src="/2022/03/10/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/image-20230128162447847.png" alt="image-20230128162447847"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/03/09/Kafka%E9%83%A8%E7%BD%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/09/Kafka%E9%83%A8%E7%BD%B2/" class="post-title-link" itemprop="url">Kafka部署</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-09 20:10:41" itemprop="dateCreated datePublished" datetime="2022-03-09T20:10:41+08:00">2022-03-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-29 23:10:31" itemprop="dateModified" datetime="2023-01-29T23:10:31+08:00">2023-01-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><h2 id="规划"><a href="#规划" class="headerlink" title="规划"></a>规划</h2><table>
<thead>
<tr>
<th>node1</th>
<th>node2</th>
<th>node3</th>
</tr>
</thead>
<tbody><tr>
<td>zk</td>
<td>zk</td>
<td>zk</td>
</tr>
<tr>
<td>kafka</td>
<td>kafka</td>
<td>kafka</td>
</tr>
</tbody></table>
<h2 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h2><ol>
<li><p>官方下载地址</p>
<p><a target="_blank" rel="noopener" href="https://kafka.apache.org/downloads.html">https://kafka.apache.org/downloads.html</a></p>
</li>
<li><p>解压安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 software]# tar -zxvf kafka_2.12-3.0.0.tgz</span><br><span class="line">(base) [root@node1 software]# mv kafka_2.12-3.0.0 ../server/kafka</span><br></pre></td></tr></table></figure>


</li>
<li><p>进入到&#x2F;export&#x2F;server&#x2F;kafka目录，修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# cd config/</span><br><span class="line">(base) [root@node1 config]# vim server.properties </span><br></pre></td></tr></table></figure>

<p>编辑以下内容</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#broker 的全局唯一编号，不能重复，只能是数字。</span></span><br><span class="line"><span class="attr">broker.id</span>=<span class="string">0</span></span><br><span class="line"><span class="comment">#处理网络请求的线程数量</span></span><br><span class="line"><span class="attr">num.network.threads</span>=<span class="string">3</span></span><br><span class="line"><span class="comment">#用来处理磁盘 IO 的线程数量</span></span><br><span class="line"><span class="attr">num.io.threads</span>=<span class="string">8</span></span><br><span class="line"><span class="comment">#发送套接字的缓冲区大小</span></span><br><span class="line"><span class="attr">socket.send.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#接收套接字的缓冲区大小</span></span><br><span class="line"><span class="attr">socket.receive.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#请求套接字的缓冲区大小</span></span><br><span class="line"><span class="attr">socket.request.max.bytes</span>=<span class="string">104857600</span></span><br><span class="line"><span class="comment">#kafka 运行日志(数据)存放的路径，路径不需要提前创建，kafka 自动帮你创建，可以</span></span><br><span class="line"><span class="attr">配置多个磁盘路径，路径与路径之间可以用&quot;，&quot;分隔</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/export/server/kafka/datas</span></span><br><span class="line"><span class="comment">#topic 在当前 broker 上的分区个数</span></span><br><span class="line"><span class="attr">num.partitions</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#用来恢复和清理 data 下数据的线程数量</span></span><br><span class="line"><span class="attr">num.recovery.threads.per.data.dir</span>=<span class="string">1</span></span><br><span class="line"><span class="comment"># 每个 topic 创建时的副本数，默认时 1 个副本</span></span><br><span class="line"><span class="attr">offsets.topic.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#segment 文件保留的最长时间，超时将被删除</span></span><br><span class="line"><span class="attr">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment">#每个 segment 文件的大小，默认最大 1G</span></span><br><span class="line"><span class="attr">log.segment.bytes</span>=<span class="string">1073741824</span></span><br><span class="line"><span class="comment"># 检查过期数据的时间，默认 5 分钟检查一次是否数据过期</span></span><br><span class="line"><span class="attr">log.retention.check.interval.ms</span>=<span class="string">300000</span></span><br><span class="line"><span class="comment">#配置连接 Zookeeper 集群地址（在 zk 根目录下创建/kafka，方便管理）</span></span><br><span class="line"><span class="attr">zookeeper.connect</span>=<span class="string">node1:2181,node2:2181,node3:2181/kafka</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>同步到node2和node3节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 server]# scp -r kafka/ root@node2:/export/server/</span><br><span class="line">(base) [root@node1 server]# scp -r kafka/ root@node3:/export/server/</span><br></pre></td></tr></table></figure>


</li>
<li><p>分别在node2和node3节点上修改配置文件&#x2F;export&#x2F;server&#x2F;kafka&#x2F;config&#x2F;server.properties中的broker.id&#x3D;1和broker.id&#x3D;2</p>
</li>
<li><p>配置环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# vim /etc/profile</span><br></pre></td></tr></table></figure>

<p>添加如下内容</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#KAFKA_HOME</span></span><br><span class="line"><span class="attr">export</span> <span class="string">KAFKA_HOME=/export/server/kafka</span></span><br><span class="line"><span class="attr">export</span> <span class="string">PATH=$PATH:$KAFKA_HOME/bin</span></span><br></pre></td></tr></table></figure>

<p>刷新环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# source /etc/profile</span><br></pre></td></tr></table></figure>

<p>分别在node2和node3节点上配置kafka环境变量。</p>
</li>
<li><p>启动集群</p>
<p>启动zookeeper集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# ./bin/zk.sh start</span><br></pre></td></tr></table></figure>

<p>依次在node1、node2和node3节点上启动kafka。注意：配置文件的路径要能够到 server.properties。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# $KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br><span class="line">(base) [root@node2 ~]# $KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br><span class="line">(base) [root@node3 ~]# $KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="集群启停脚本"><a href="#集群启停脚本" class="headerlink" title="集群启停脚本"></a>集群启停脚本</h2><ol>
<li><p>在~&#x2F;bin目录下创建kf.sh脚本文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 bin]# vim kf.sh</span><br></pre></td></tr></table></figure>

<p>脚本内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">&quot;start&quot;</span>)&#123;</span><br><span class="line"> <span class="keyword">for</span> i <span class="keyword">in</span> node1 node2 node3</span><br><span class="line"> <span class="keyword">do</span></span><br><span class="line"> <span class="built_in">echo</span> <span class="string">&quot; --------启动 <span class="variable">$i</span> Kafka-------&quot;</span></span><br><span class="line"> ssh <span class="variable">$i</span> <span class="string">&quot;/export/server/kafka/bin/kafka-server-start.sh -daemon /export/server/kafka/config/server.properties&quot;</span></span><br><span class="line"> <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">&quot;stop&quot;</span>)&#123;</span><br><span class="line"> <span class="keyword">for</span> i <span class="keyword">in</span> node1 node2 node3</span><br><span class="line"> <span class="keyword">do</span></span><br><span class="line"> <span class="built_in">echo</span> <span class="string">&quot; --------停止 <span class="variable">$i</span> Kafka-------&quot;</span></span><br><span class="line"> ssh <span class="variable">$i</span> <span class="string">&quot;/export/server/kafka/bin/kafka-server-stop.sh &quot;</span></span><br><span class="line"> <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>添加执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 bin]# chmod u+x kf.sh </span><br></pre></td></tr></table></figure>


</li>
<li><p>启动集群命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 bin]# ./kf.sh start</span><br></pre></td></tr></table></figure>


</li>
<li><p>停止集群命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 bin]# ./kf.sh stop</span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong>停止 Kafka 集群时，一定要等 Kafka 所有节点进程全部停止后再停止 Zookeeper集群。因为 Zookeeper 集群当中记录着 Kafka 集群相关信息，Zookeeper 集群一旦先停止，Kafka 集群就没有办法再获取停止进程的信息，只能手动杀死 Kafka 进程了。</p>
</li>
</ol>
<h1 id="Kafka命令"><a href="#Kafka命令" class="headerlink" title="Kafka命令"></a>Kafka命令</h1><h2 id="主题命令行操作"><a href="#主题命令行操作" class="headerlink" title="主题命令行操作"></a>主题命令行操作</h2><h3 id="查看操作主题命令参数"><a href="#查看操作主题命令参数" class="headerlink" title="查看操作主题命令参数"></a>查看操作主题命令参数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh </span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的 Kafka Broker 主机名称和端口号。</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的 topic 名称。</td>
</tr>
<tr>
<td>–create</td>
<td>创建主题。</td>
</tr>
<tr>
<td>–delete</td>
<td>删除主题。</td>
</tr>
<tr>
<td>–alter</td>
<td>修改主题。</td>
</tr>
<tr>
<td>–list</td>
<td>查看所有主题。</td>
</tr>
<tr>
<td>–describe</td>
<td>查看主题详细描述。</td>
</tr>
<tr>
<td>–partitions &lt;Integer: # of partitions&gt;</td>
<td>设置分区数。</td>
</tr>
<tr>
<td>–replication-factor&lt;Integer: replication factor&gt;</td>
<td>设置分区副本。</td>
</tr>
<tr>
<td>–config &lt;String: name&#x3D;value&gt;</td>
<td>更新系统默认的配置。</td>
</tr>
</tbody></table>
<h3 id="查看当前服务器中的所有-topic"><a href="#查看当前服务器中的所有-topic" class="headerlink" title="查看当前服务器中的所有 topic"></a>查看当前服务器中的所有 topic</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --list</span><br></pre></td></tr></table></figure>



<h3 id="创建-topic"><a href="#创建-topic" class="headerlink" title="创建 topic"></a>创建 topic</h3><p>创建 first topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --create --partitions 1 --replication-factor 3 --topic first</span><br></pre></td></tr></table></figure>

<p>选项说明：</p>
<p>–topic 定义 topic 名</p>
<p>–replication-factor 定义副本数</p>
<p>–partitions 定义分区数</p>
<h3 id="查看-topic-的详情"><a href="#查看-topic-的详情" class="headerlink" title="查看 topic 的详情"></a>查看 topic 的详情</h3><p>查看 first 主题的详情</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --describe --topic first</span><br></pre></td></tr></table></figure>



<h3 id="修改分区数"><a href="#修改分区数" class="headerlink" title="修改分区数"></a>修改分区数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --alter --topic first --partitions 3</span><br></pre></td></tr></table></figure>



<h3 id="删除-topic"><a href="#删除-topic" class="headerlink" title="删除 topic"></a>删除 topic</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-topics.sh --bootstrap-server node1:9092 --delete --topic first</span><br></pre></td></tr></table></figure>

<p>如果删除topic时，显示marked for deletion。可通过Zookeeper下的zkCli.sh中的deleteall命令来删除</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /kafka/brokers/topics</span><br><span class="line">[zk: localhost:2181(CONNECTED) 8] deleteall /kafka/brokers/topics/first</span><br></pre></td></tr></table></figure>





<h2 id="生产者命令行操作"><a href="#生产者命令行操作" class="headerlink" title="生产者命令行操作"></a>生产者命令行操作</h2><h3 id="查看操作生产者命令参数"><a href="#查看操作生产者命令参数" class="headerlink" title="查看操作生产者命令参数"></a>查看操作生产者命令参数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-producer.sh </span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的 Kafka Broker 主机名称和端口号。</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的 topic 名称。</td>
</tr>
</tbody></table>
<h3 id="发送消息"><a href="#发送消息" class="headerlink" title="发送消息"></a>发送消息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-producer.sh --bootstrap-server node1:9092 --topic first</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello world</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello Hadoop</span></span><br></pre></td></tr></table></figure>



<h2 id="消费者命令行操作"><a href="#消费者命令行操作" class="headerlink" title="消费者命令行操作"></a>消费者命令行操作</h2><h3 id="查看操作消费者命令参数"><a href="#查看操作消费者命令参数" class="headerlink" title="查看操作消费者命令参数"></a>查看操作消费者命令参数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-consumer.sh </span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的 Kafka Broker 主机名称和端口号。</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的 topic 名称。</td>
</tr>
<tr>
<td>–from-beginning</td>
<td>从头开始消费。</td>
</tr>
<tr>
<td>–group &lt;String: consumer group id&gt;</td>
<td>指定消费者组名称。</td>
</tr>
</tbody></table>
<h3 id="消费消息"><a href="#消费消息" class="headerlink" title="消费消息"></a>消费消息</h3><p>消费first主题中的消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic first</span><br></pre></td></tr></table></figure>



<p>把主题中所有的数据都读取出来（包括历史数据）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 kafka]# ./bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/03/08/Kafka%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/08/Kafka%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">Kafka基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-08 21:12:41" itemprop="dateCreated datePublished" datetime="2022-03-08T21:12:41+08:00">2022-03-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-28 21:33:09" itemprop="dateModified" datetime="2023-01-28T21:33:09+08:00">2023-01-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p><strong>Kafka传统定义：</strong>Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列（MessageQueue），主要应用于大数据实时处理领域。</p>
<p><strong>Kafka最新定义 ：</strong> Kafka是 一个开源的分布式事件流平台 （Event StreamingPlatform），被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用。</p>
<p><strong>发布&#x2F;订阅</strong>：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订阅者只接收感兴趣的消息。</p>
<h2 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h2><p>目前企业中比较常见的消息队列产品主要有 Kafka、ActiveMQ 、RabbitMQ 、RocketMQ 等。</p>
<p>在大数据场景主要采用 Kafka 作为消息队列。在 Java 开发中主要采用 ActiveMQ、RabbitMQ、RocketMQ。</p>
<h3 id="传统消息队列的应用场景"><a href="#传统消息队列的应用场景" class="headerlink" title="传统消息队列的应用场景"></a>传统消息队列的应用场景</h3><p>主要应用场景包括：缓存&#x2F;消峰、解耦和异步通信</p>
<h4 id="缓存-x2F-消峰"><a href="#缓存-x2F-消峰" class="headerlink" title="缓存&#x2F;消峰"></a>缓存&#x2F;消峰</h4><p>有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</p>
<p><img src="/2022/03/08/Kafka%E5%9F%BA%E7%A1%80/image-20230126154744548.png" alt="image-20230126154744548"></p>
<p>在双十一活动中，大量用户同时购买同一商品，参与用户数超过了系统处理能力，如不进行流量削峰，极易造成系统崩溃的场景。同时使用消息队列缓存，可以避免卖超的场景。</p>
<h4 id="解耦"><a href="#解耦" class="headerlink" title="解耦"></a>解耦</h4><p>允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p>
<p><img src="/2022/03/08/Kafka%E5%9F%BA%E7%A1%80/image-20230126154744549.png" alt="image-20230126154744548"></p>
<h4 id="异步通信"><a href="#异步通信" class="headerlink" title="异步通信"></a>异步通信</h4><p>允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们。</p>
<p><strong>同步处理</strong></p>
<p><img src="/2022/03/08/Kafka%E5%9F%BA%E7%A1%80/image-20230126154744550.png" alt="image-20230126154744550"></p>
<p><strong>异步处理</strong></p>
<p><img src="/2022/03/08/Kafka%E5%9F%BA%E7%A1%80/image-20230126154744551.png" alt="image-20230126154744551"></p>
<h3 id="消息队列的两种模式"><a href="#消息队列的两种模式" class="headerlink" title="消息队列的两种模式"></a>消息队列的两种模式</h3><h4 id="点对点"><a href="#点对点" class="headerlink" title="点对点"></a>点对点</h4><p>消费者主动拉取数据，消息收到后清除消息</p>
<p><img src="/2022/03/08/Kafka%E5%9F%BA%E7%A1%80/image-20230126154744552.png" alt="image-20230126154744552"></p>
<h4 id="发布订阅模式"><a href="#发布订阅模式" class="headerlink" title="发布订阅模式"></a>发布订阅模式</h4><ul>
<li><p>可以有多个topic主题（浏览、点赞、收藏、评论等）</p>
</li>
<li><p>消费者消费数据之后，不删除数据</p>
</li>
<li><p>每个消费者相互独立，都可以消费到数据</p>
</li>
</ul>
<p><img src="/2022/03/08/Kafka%E5%9F%BA%E7%A1%80/image-20230126154744553.png" alt="image-20230126154744553"></p>
<h2 id="Kafka基础架构"><a href="#Kafka基础架构" class="headerlink" title="Kafka基础架构"></a>Kafka基础架构</h2><ol>
<li>为方便扩展，并提高吞吐量，一个topic分为多个partition。</li>
<li>配合分区的设计，提出消费者组的概念，组内每个消费者并行消费。</li>
<li>为提高可用性，为每个partition增加若干副本，类似NameNode HA。</li>
<li>ZK中记录谁是leader，Kafka2.8.0以后也可以配置不采用ZK。</li>
</ol>
<p><img src="/2022/03/08/Kafka%E5%9F%BA%E7%A1%80/image-20230126154744554.png" alt="image-20230126154744554"></p>
<p>（1）Producer：消息生产者，就是向 Kafka broker 发消息的客户端。<br>（2）Consumer：消息消费者，向 Kafka broker 取消息的客户端。<br>（3）Consumer Group（CG）：消费者组，由多个 consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。<br>（4）Broker：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个broker 可以容纳多个 topic。<br>（5）Topic：可以理解为一个队列，生产者和消费者面向的都是一个 topic。<br>（6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。<br>（7）Replica：副本。一个 topic 的每个分区都有若干个副本，一个 Leader 和若干个Follower。<br>（8）Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 Leader。<br>（9）Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和Leader 数据的同步。Leader 发生故障时，某个 Follower 会成为新的 Leader。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/03/01/HBase%E9%9B%86%E6%88%90Hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/01/HBase%E9%9B%86%E6%88%90Hive/" class="post-title-link" itemprop="url">HBase集成Hive</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-01 20:26:45" itemprop="dateCreated datePublished" datetime="2022-03-01T20:26:45+08:00">2022-03-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-12-27 22:53:07" itemprop="dateModified" datetime="2022-12-27T22:53:07+08:00">2022-12-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h1><p>如果大量的数据已经存放在 HBase 上面，需要对已经存在的数据进行数据分析处理，那么 Phoenix 并不适合做特别复杂的 SQL 处理，此时可以使用 hive 映射 HBase 的表格，之后写 HQL 进行分析处理。</p>
<h1 id="HBase-与-Hive-集成"><a href="#HBase-与-Hive-集成" class="headerlink" title="HBase 与 Hive 集成"></a>HBase 与 Hive 集成</h1><p>在 hive-site.xml 中添加 zookeeper 的属性，如下：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&lt;property&gt;</span></span><br><span class="line"> 	<span class="attr">&lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;</span></span><br><span class="line"> 	<span class="attr">&lt;value&gt;node1,node2,node3&lt;/value&gt;</span></span><br><span class="line"><span class="attr">&lt;/property&gt;</span></span><br><span class="line"><span class="attr">&lt;property&gt;</span></span><br><span class="line"> 	<span class="attr">&lt;name&gt;hive.zookeeper.client.port&lt;/name&gt;</span></span><br><span class="line"> 	<span class="attr">&lt;value&gt;2181&lt;/value&gt;</span></span><br><span class="line"><span class="attr">&lt;/property&gt;</span></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/02/28/HBase%E6%95%B4%E5%90%88Phoenix/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/28/HBase%E6%95%B4%E5%90%88Phoenix/" class="post-title-link" itemprop="url">HBase整合Phoenix</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-28 21:01:09" itemprop="dateCreated datePublished" datetime="2022-02-28T21:01:09+08:00">2022-02-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-12-27 22:38:22" itemprop="dateModified" datetime="2022-12-27T22:38:22+08:00">2022-12-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Phoenix"><a href="#Phoenix" class="headerlink" title="Phoenix"></a>Phoenix</h1><h2 id="Phoenix定义"><a href="#Phoenix定义" class="headerlink" title="Phoenix定义"></a>Phoenix定义</h2><p>Phoenix 是 HBase 的开源 SQL 皮肤。可以使用标准 JDBC API 代替 HBase 客户端 API来创建表，插入数据和查询 HBase 数据。</p>
<p>在 Client 和 HBase 之间放一个 Phoenix 中间层不会减慢速度，因为用户编写的数据处理代码和 Phoenix 编写的没有区别，不仅如此，Phoenix 对于用户输入的 SQL 同样会有大量的优化手段（就像 hive 自带 sql 优化器一样）。</p>
<h2 id="部署Phoenix"><a href="#部署Phoenix" class="headerlink" title="部署Phoenix"></a>部署Phoenix</h2><ol>
<li><p>解压 tar 包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 software]# tar -zxvf phoenix-hbase-2.4.0-5.1.2-bin.tar.gz ../server/phoenix</span><br></pre></td></tr></table></figure>


</li>
<li><p>复制 server 包并拷贝到各个节点的 hbase&#x2F;lib</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 phoenix]# cp phoenix-server-hbase-2.4.0-5.1.2.jar $HBASE_HOME/lib/</span><br><span class="line">(base) [root@node1 phoenix]# scp phoenix-server-hbase-2.4.0-5.1.2.jar root@node2:/export/server/hbase-2.5.2/lib/</span><br><span class="line">(base) [root@node1 phoenix]# scp phoenix-server-hbase-2.4.0-5.1.2.jar root@node3:/export/server/hbase-2.5.2/lib/</span><br></pre></td></tr></table></figure>


</li>
<li><p>配置环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 phoenix]# vim /etc/profile</span><br></pre></td></tr></table></figure>

<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#phoenix</span></span><br><span class="line"><span class="attr">export</span> <span class="string">PHOENIX_HOME=/export/server/phoenix</span></span><br><span class="line"><span class="attr">export</span> <span class="string">PHOENIX_CLASSPATH=$PHOENIX_HOME</span></span><br><span class="line"><span class="attr">export</span> <span class="string">PATH=$PATH:$PHOENIX_HOME/bin</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 phoenix]# source /etc/profile</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 conf]# cd $HBASE_HOME/conf</span><br><span class="line">(base) [root@node1 conf]# vim hbase-site.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 将以下配置添加到 hbase-site.xml 后边</span></span><br><span class="line"><span class="attr">&lt;!--</span> <span class="string">支持HBase命名空间映射 --&gt;</span></span><br><span class="line"><span class="attr">&lt;property&gt;</span></span><br><span class="line">    <span class="attr">&lt;name&gt;phoenix.schema.isNamespaceMappingEnabled&lt;/name&gt;</span></span><br><span class="line">    <span class="attr">&lt;value&gt;true&lt;/value&gt;</span></span><br><span class="line"><span class="attr">&lt;/property&gt;</span></span><br><span class="line"><span class="attr">&lt;!--</span> <span class="string">支持索引预写日志编码 --&gt;</span></span><br><span class="line"><span class="attr">&lt;property&gt;</span></span><br><span class="line">  <span class="attr">&lt;name&gt;hbase.regionserver.wal.codec&lt;/name&gt;</span></span><br><span class="line">  <span class="attr">&lt;value&gt;org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec&lt;/value&gt;</span></span><br><span class="line"><span class="attr">&lt;/property&gt;</span></span><br></pre></td></tr></table></figure>

<p>分发到其他服务器上</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 conf]# scp hbase-site.xml root@node2:$PWD</span><br><span class="line">(base) [root@node1 conf]# scp hbase-site.xml root@node3:$PWD</span><br></pre></td></tr></table></figure>
</li>
<li><p>将配置后的hbase-site.xml拷贝到phoenix的bin目录</p>
</li>
</ol>
   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 conf]# cp $HBASE_HOME/conf/hbase-site.xml $PHOENIX_HOME/bin/</span><br></pre></td></tr></table></figure>

<ol start="6">
<li><p>重启 HBase</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 phoenix]# stop-hbase.sh</span><br><span class="line">(base) [root@node1 phoenix]# start-hbase.sh</span><br></pre></td></tr></table></figure>


</li>
<li><p>连接 Phoenix，第一次启动Phoenix连接HBase会稍微慢一点。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 bin]# $PHOENIX_HOME/bin/sqlline.py node1:2181</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="fhclk"
      src="/images/avatar1.png">
  <p class="site-author-name" itemprop="name">fhclk</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">117</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/fhclk" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fhclk" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">fhclk</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/src/clicklove.js"></script>
</body>
</html>
