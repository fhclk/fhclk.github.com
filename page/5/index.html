<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"fhclk.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="拾荒者">
<meta property="og:url" content="http://fhclk.github.io/page/5/index.html">
<meta property="og:site_name" content="拾荒者">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="fhclk">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://fhclk.github.io/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>拾荒者</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">拾荒者</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">昨夜西风凋碧树，独上高楼，望尽天涯路</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/02/19/%E7%AC%AC%E4%B8%80%E6%80%A7%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/19/%E7%AC%AC%E4%B8%80%E6%80%A7%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">第一性原理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-19 12:42:26" itemprop="dateCreated datePublished" datetime="2022-02-19T12:42:26+08:00">2022-02-19</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><h2 id="哲学概念"><a href="#哲学概念" class="headerlink" title="哲学概念"></a>哲学概念</h2><blockquote>
<p>第一性原理是基本的命题和假设，它不能被省略，不能被删除，也不能被违反。【亚里士多德】</p>
</blockquote>
<h2 id="量子力学概念"><a href="#量子力学概念" class="headerlink" title="量子力学概念"></a>量子力学概念</h2><p>第一性原理物理学概念</p>
<p>广义：一切基于量子力学的原理的计算。</p>
<p>侠义：不使用经验参数，只用电子质量，光速，质子中子质量等少数实验数据去做量子计算。</p>
<p>第一性原理和经验是两个极端，第一性原理是根据某些硬性规定，纯理论的一种推演，而经验参数是通过大量的实例做出的一个规律性的数据。</p>
<p>第一性原理倡导的是推演，就是找到一个最基础的东西，不断的往上去推演。</p>
<p>第一性原理是一种演绎性思维。</p>
<p><strong>常见的思维方式</strong></p>
<ul>
<li>演绎</li>
</ul>
<p>根据一个最底层的东西，往上推导。演绎法比较厉害的就是三段论</p>
<p><img src="/2022/02/19/%E7%AC%AC%E4%B8%80%E6%80%A7%E5%8E%9F%E7%90%86/image-20230219121638706.png" alt="image-20230219121638706"></p>
<p>演绎性的好处，前提正确，后面的也正确。</p>
<ul>
<li>归纳</li>
</ul>
<p>有N个已知的数据和现象推出一个结论</p>
<ul>
<li><p>类比</p>
</li>
<li><p>溯因</p>
</li>
</ul>
<blockquote>
<p>理论家的工作分成两步，第一步是发现公理，第二步是从公理出发推出结论。【爱因斯坦】</p>
</blockquote>
<blockquote>
<p>商界有个古老的法则，第一步是找到一个简单的基本的道理；第二步非常严格地按照这个道理行事。【查理芒格《穷查理宝典》】</p>
</blockquote>
<p>第一性原理地哲学根基</p>
<p>本体论：探究世界本原或基质。</p>
<p>简一律：根本地那个律。</p>
<p>第一因：最早地那个动力因。</p>
<h1 id="如何应用第一性原理"><a href="#如何应用第一性原理" class="headerlink" title="如何应用第一性原理"></a>如何应用第一性原理</h1><p><img src="/2022/02/19/%E7%AC%AC%E4%B8%80%E6%80%A7%E5%8E%9F%E7%90%86/image-20230219123005437.png" alt="image-20230219123005437"></p>
<ol>
<li><p>归零</p>
<p>从头算回归最基本地概念，寻找最基本地基石</p>
</li>
<li><p>解构</p>
<p>用物理学的思维分解现象，层层拨开找到改变的突破点。</p>
</li>
<li><p>重构</p>
<p>重新建造一个新的赛道、新的模式、新的方法，把原来的颠覆掉、替代掉。</p>
</li>
</ol>
<h1 id="如何找到第一性原理？如何培养第一性思维？"><a href="#如何找到第一性原理？如何培养第一性思维？" class="headerlink" title="如何找到第一性原理？如何培养第一性思维？"></a>如何找到第一性原理？如何培养第一性思维？</h1><p>跨学科学习，学习各学科的基础，掌握重要学科的重要理论，建立全面的知识体系。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/02/18/SparkSQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/18/SparkSQL/" class="post-title-link" itemprop="url">SparkSQL</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-18 21:12:45" itemprop="dateCreated datePublished" datetime="2022-02-18T21:12:45+08:00">2022-02-18</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="SparkSQL概述"><a href="#SparkSQL概述" class="headerlink" title="SparkSQL概述"></a>SparkSQL概述</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>SparkSQL是Spark的一个模块，用于处理海量的结构化数据。它支持SQL语言、性能强、可以自动优化、API简单、兼容HIVE等</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul>
<li>融合性：SQL可以无缝集成在代码中，随时用SQL处理数据</li>
<li>统一的数据访问：一套标准的API可读写不同的数据源</li>
<li>Hive兼容：可以使用SparkSQL直接计算并生成Hive数据表</li>
<li>标准化连接：支持标准化JDBC&#x2F;ODBC连接，方便和各种数据库进行数据交互</li>
</ul>
<h2 id="SparkSQL和Hive的异同"><a href="#SparkSQL和Hive的异同" class="headerlink" title="SparkSQL和Hive的异同"></a>SparkSQL和Hive的异同</h2><p><img src="/2022/02/18/SparkSQL/image-20221208112145644.png" alt="image-20221208112145644"></p>
<h2 id="SparkSQL的数据抽象"><a href="#SparkSQL的数据抽象" class="headerlink" title="SparkSQL的数据抽象"></a>SparkSQL的数据抽象</h2><p>各计算框架的数据抽象</p>
<ul>
<li>Pandas - DataFrame - 二维表数据结构 &#x2F; 单机（本地）集合</li>
<li>SparkCore - RDD - 无标准数据结构，存储什么数据都可以 &#x2F; 分布式集合（分区）</li>
<li>SparkSQL - DataFrame - 二维表数据结构 &#x2F; 分布式集合（分区）</li>
<li>SparkSQL for JVM - DataSet&#x2F;DataFrame</li>
<li>SparkSQL for Python&#x2F;R - DataFrame</li>
</ul>
<h2 id="SparkSession对象"><a href="#SparkSession对象" class="headerlink" title="SparkSession对象"></a>SparkSession对象</h2><p>在RDD阶段，程序的执行入口是：SparkContext。</p>
<p>在Spark2.0后，SparkSession作为Spark编码的统一入口对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment">#构建SparkSession对象，构建器模式，通过builder方法来构建</span></span><br><span class="line">    <span class="comment">#通过getOrCreate()方法创建</span></span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&#x27;study01&#x27;</span>).master(<span class="string">&#x27;local[*&#x27;</span>).config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).getOrCreate()</span><br></pre></td></tr></table></figure>



<h1 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h1><h2 id="DataFrame组成"><a href="#DataFrame组成" class="headerlink" title="DataFrame组成"></a>DataFrame组成</h2><p>结构层面：</p>
<ul>
<li>StructType对象描述整个DataFrame的表结构</li>
<li>StructField对象描述一个列的信息</li>
</ul>
<p>数据层面</p>
<ul>
<li>Row对象记录一行数据</li>
<li>Column对象记录一列数据并包含列的信息</li>
</ul>
<p><img src="/2022/02/18/SparkSQL/image-20221208115422089.png" alt="image-20221208115422089"></p>
<p>StructType描述，如下图</p>
<p><img src="/2022/02/18/SparkSQL/image-20221208115655961.png" alt="image-20221208115655961"></p>
<p>一个StructField记录：列名、列类型、列是否运行为空。</p>
<p>多个StructField组成一个StructType对象。</p>
<p>一个StructType对象可以描述一个DataFrame。</p>
<h2 id="DataFrame的代码构建"><a href="#DataFrame的代码构建" class="headerlink" title="DataFrame的代码构建"></a>DataFrame的代码构建</h2><h3 id="基于RDD"><a href="#基于RDD" class="headerlink" title="基于RDD"></a>基于RDD</h3><h4 id="方式1"><a href="#方式1" class="headerlink" title="方式1"></a>方式1</h4><p>通过SparkSession对象的createDataFrame方法将RDD转换为DataFrame。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment">#构建SparkSession对象，构建器模式，通过builder方法来构建</span></span><br><span class="line">    <span class="comment">#通过getOrCreate()方法创建</span></span><br><span class="line">    spark = SparkSession.builder.\</span><br><span class="line">        appName(<span class="string">&#x27;study01&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">    rdd = sc.textFile(<span class="string">&quot;hdfs://node1:8020/input/sql/people.txt&quot;</span>).\</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;,&#x27;</span>)).\</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: [x[<span class="number">0</span>], <span class="built_in">int</span>(x[<span class="number">1</span>])])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#构建dataFrame，这里只传入列名称，类型从RDD中进行推断，是否允许为空（默认允许true）</span></span><br><span class="line">    df = spark.createDataFrame(rdd, schema=[<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>])</span><br><span class="line">    <span class="comment">#打印表结构</span></span><br><span class="line">    df.printSchema()</span><br><span class="line">    <span class="comment">#打印数据</span></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    df.createTempView(<span class="string">&#x27;ttt&#x27;</span>)</span><br><span class="line">    spark.sql(<span class="string">&quot;select * from ttt where age &lt; 30&quot;</span>).show()</span><br></pre></td></tr></table></figure>



<h4 id="方式2"><a href="#方式2" class="headerlink" title="方式2"></a>方式2</h4><p>通过StructType对象来定义DataFrame的“表结构”转换RDD</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.\</span><br><span class="line">        appName(<span class="string">&#x27;study01&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">    rdd = sc.textFile(<span class="string">&quot;hdfs://node1:8020/input/sql/stu_score.txt&quot;</span>).\</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;,&#x27;</span>)).\</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: [<span class="built_in">int</span>(x[<span class="number">0</span>]), x[<span class="number">1</span>], <span class="built_in">int</span>(x[<span class="number">2</span>])])</span><br><span class="line"></span><br><span class="line">    schema = StructType().\</span><br><span class="line">        add(<span class="string">&#x27;id&#x27;</span>, IntegerType(), nullable=<span class="literal">False</span>).\</span><br><span class="line">        add(<span class="string">&#x27;name&#x27;</span>, StringType(), nullable=<span class="literal">True</span>).\</span><br><span class="line">        add(<span class="string">&#x27;score&#x27;</span>, IntegerType(), nullable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#构建dataFrame，</span></span><br><span class="line">    df = spark.createDataFrame(rdd, schema)</span><br><span class="line">    <span class="comment">#打印表结构</span></span><br><span class="line">    df.printSchema()</span><br><span class="line">    <span class="comment">#打印数据</span></span><br><span class="line">    df.show()</span><br></pre></td></tr></table></figure>



<h4 id="方式3"><a href="#方式3" class="headerlink" title="方式3"></a>方式3</h4><p>使用RDD的toDF方法转换RDD</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.\</span><br><span class="line">        appName(<span class="string">&#x27;study01&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">    rdd = sc.textFile(<span class="string">&quot;hdfs://node1:8020/input/sql/stu_score.txt&quot;</span>).\</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;,&#x27;</span>)).\</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: [<span class="built_in">int</span>(x[<span class="number">0</span>]), x[<span class="number">1</span>], <span class="built_in">int</span>(x[<span class="number">2</span>])])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#方式1，只传列名，类型靠推断，是否允许为空是true</span></span><br><span class="line">    df1 = rdd.toDF([<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;score&#x27;</span>])</span><br><span class="line">    df1.printSchema()</span><br><span class="line">    df1.show()</span><br><span class="line"></span><br><span class="line">    schema = StructType().\</span><br><span class="line">        add(<span class="string">&#x27;id&#x27;</span>, IntegerType(), nullable=<span class="literal">False</span>).\</span><br><span class="line">        add(<span class="string">&#x27;name&#x27;</span>, StringType(), nullable=<span class="literal">True</span>).\</span><br><span class="line">        add(<span class="string">&#x27;score&#x27;</span>, IntegerType(), nullable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#方式2，传入完整的schema描述对象StructType</span></span><br><span class="line">    df2 = rdd.toDF(schema=schema)</span><br><span class="line">    df2.printSchema()</span><br><span class="line">    df2.show()</span><br></pre></td></tr></table></figure>



<h3 id="基于Pandas的DataFrame"><a href="#基于Pandas的DataFrame" class="headerlink" title="基于Pandas的DataFrame"></a>基于Pandas的DataFrame</h3><p>将Pandas的DataFrame对象，转变为分布式的SparkSQL DataFrame对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.\</span><br><span class="line">        appName(<span class="string">&#x27;study01&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    pdf = pd.DataFrame(&#123;</span><br><span class="line">        <span class="string">&#x27;id&#x27;</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: [<span class="string">&#x27;张三&#x27;</span>, <span class="string">&#x27;李四&#x27;</span>, <span class="string">&#x27;王五&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;age&#x27;</span>:[<span class="number">11</span>,<span class="number">23</span>,<span class="number">24</span>]</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    df = spark.createDataFrame(pdf)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br></pre></td></tr></table></figure>



<h3 id="读取外部数据"><a href="#读取外部数据" class="headerlink" title="读取外部数据"></a>读取外部数据</h3><p>通过SparkSQL的统一的API进行数据的读取构建DataFrame</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sparksession.read.<span class="built_in">format</span>(<span class="string">&quot;text|csv|parquet|orc|avro|jdbc|...&quot;</span>)</span><br><span class="line">	.option(<span class="string">&quot;K&quot;</span>, <span class="string">&quot;V&quot;</span>) <span class="comment">#option可选</span></span><br><span class="line">    .schema(StructType | String) <span class="comment">#STRING的语法如.schema(&quot;name STRING&quot;, &quot;age INT&quot;)</span></span><br><span class="line">    .load(<span class="string">&quot;被读取的文件的路径， 支持本地文件系统和HDFS&quot;</span>)</span><br></pre></td></tr></table></figure>



<h4 id="读取text数据源"><a href="#读取text数据源" class="headerlink" title="读取text数据源"></a>读取text数据源</h4><p>使用format(“text”)读取文本数据，读取到的DataFrame只会有一个列，列名默认称之为：value</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.\</span><br><span class="line">        appName(<span class="string">&#x27;study01&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    schema = StructType().add(<span class="string">&quot;data&quot;</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;text&quot;</span>)\</span><br><span class="line">        .schema(schema)\</span><br><span class="line">        .load(<span class="string">&quot;hdfs://node1:8020/input/sql/people.txt&quot;</span>)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="comment">#root</span></span><br><span class="line"><span class="comment"># |-- data: string (nullable = true)</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#+-----------+</span></span><br><span class="line"><span class="comment">#|       data|</span></span><br><span class="line"><span class="comment">#+-----------+</span></span><br><span class="line"><span class="comment">#|Michael, 29|</span></span><br><span class="line"><span class="comment">#|   Andy, 30|</span></span><br><span class="line"><span class="comment">#| Justin, 19|</span></span><br><span class="line"><span class="comment">#+-----------+</span></span><br><span class="line"><span class="comment">#</span></span><br></pre></td></tr></table></figure>



<h4 id="读取json数据源"><a href="#读取json数据源" class="headerlink" title="读取json数据源"></a>读取json数据源</h4><p>使用format(“json”)读取json数据。</p>
<p>JSON类型一般不用写.schema，json自带，json带有列名和列类型（字符串和数字）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.\</span><br><span class="line">        appName(<span class="string">&#x27;study01&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;json&quot;</span>)\</span><br><span class="line">        .load(<span class="string">&quot;hdfs://node1:8020/input/sql/people.json&quot;</span>)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="comment">#root</span></span><br><span class="line"><span class="comment"># |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment"># |-- name: string (nullable = true)</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#+----+-------+</span></span><br><span class="line"><span class="comment">#| age|   name|</span></span><br><span class="line"><span class="comment">#+----+-------+</span></span><br><span class="line"><span class="comment">#|null|Michael|</span></span><br><span class="line"><span class="comment">#|  30|   Andy|</span></span><br><span class="line"><span class="comment">#|  19| Justin|</span></span><br><span class="line"><span class="comment">#+----+-------+</span></span><br><span class="line"><span class="comment"># </span></span><br></pre></td></tr></table></figure>



<h4 id="读取csv数据"><a href="#读取csv数据" class="headerlink" title="读取csv数据"></a>读取csv数据</h4><p>使用format(“csv”)读取csv数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.\</span><br><span class="line">        appName(<span class="string">&#x27;study01&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>)\</span><br><span class="line">        .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&#x27;;&#x27;</span>)\</span><br><span class="line">        .option(<span class="string">&quot;header&quot;</span>, <span class="literal">False</span>)\</span><br><span class="line">        .option(<span class="string">&quot;encoding&quot;</span>, <span class="string">&quot;utf-8&quot;</span>)\</span><br><span class="line">        .schema(<span class="string">&quot;name STRING, age INT, job STRING&quot;</span>)\</span><br><span class="line">        .load(<span class="string">&quot;hdfs://node1:8020/input/sql/people.csv&quot;</span>)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br></pre></td></tr></table></figure>



<h4 id="读取parquet数据"><a href="#读取parquet数据" class="headerlink" title="读取parquet数据"></a>读取parquet数据</h4><p>使用format(“parquet”)读取parquet数据。</p>
<p>parquet是Spark中常用的一种列式存储文件格式，和Hive中的ORC差不多，它俩都是列存储格式。</p>
<p>parquet与普通文件的区别</p>
<ul>
<li><p>parquet内置了schema（列名\列类型\是否为空）</p>
</li>
<li><p>存储是以列作为存储格式</p>
</li>
<li><p>存储是序列化存储在文件中的（有压缩属性体积）</p>
</li>
</ul>
<p>Parquet文件不能直接打开，在pycharm中可以安卓插件【Avro and Parquet Viewer】来查看。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.\</span><br><span class="line">        appName(<span class="string">&#x27;study01&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;parquet&#x27;</span>)\</span><br><span class="line">        .load(<span class="string">&quot;hdfs://node1:8020/input/sql/users.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="comment">#root</span></span><br><span class="line"><span class="comment"># |-- name: string (nullable = true)</span></span><br><span class="line"><span class="comment"># |-- favorite_color: string (nullable = true)</span></span><br><span class="line"><span class="comment"># |-- favorite_numbers: array (nullable = true)</span></span><br><span class="line"><span class="comment"># |    |-- element: integer (containsNull = true)</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#+------+--------------+----------------+</span></span><br><span class="line"><span class="comment">#|  name|favorite_color|favorite_numbers|</span></span><br><span class="line"><span class="comment">#+------+--------------+----------------+</span></span><br><span class="line"><span class="comment">#|Alyssa|          null|  [3, 9, 15, 20]|</span></span><br><span class="line"><span class="comment">#|   Ben|           red|              []|</span></span><br><span class="line"><span class="comment">#+------+--------------+----------------+</span></span><br><span class="line"><span class="comment">#  </span></span><br></pre></td></tr></table></figure>



<h2 id="DataFrame入门操作"><a href="#DataFrame入门操作" class="headerlink" title="DataFrame入门操作"></a>DataFrame入门操作</h2><p>DataFrame支持两种风格进行编程，分别是：</p>
<ul>
<li>DSL风格</li>
<li>SQL风格</li>
</ul>
<p><strong>DSL语法风格</strong></p>
<p>DSL称之为：领域特定语言。</p>
<p>其实就是指DataFrame的特有API。</p>
<p>DSL风格就是以调用API的方式来处理Data，比如：df.where().limit()</p>
<p><strong>SQL语法风格</strong></p>
<p>使用SQL语句处理DataFrame的数据。</p>
<p>比如：spark.sql(“select * from xxx”)</p>
<h3 id="DSL"><a href="#DSL" class="headerlink" title="DSL"></a>DSL</h3><h4 id="show方法"><a href="#show方法" class="headerlink" title="show方法"></a>show方法</h4><p>功能：展示DataFrame中的数据，默认20条</p>
<p>语法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.show(参数<span class="number">1</span>, 参数<span class="number">2</span>)</span><br><span class="line">- 参数<span class="number">1</span>：默认<span class="number">20</span>条，控制展示多少条</span><br><span class="line">- 参数<span class="number">2</span>：是否阶段列，默认只输出<span class="number">20</span>各字符的长度，过长不显示，要显示的话truncate=<span class="literal">True</span></span><br></pre></td></tr></table></figure>



<h4 id="printSchema方法"><a href="#printSchema方法" class="headerlink" title="printSchema方法"></a>printSchema方法</h4><p>功能：打印输出df的schema信息</p>
<p>语法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure>



<h4 id="select"><a href="#select" class="headerlink" title="select"></a>select</h4><p>功能：选择DataFrame中指定的列（通过参数进行控制）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.\</span><br><span class="line">        appName(<span class="string">&#x27;study01&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;parquet&#x27;</span>)\</span><br><span class="line">        .load(<span class="string">&quot;hdfs://node1:8020/input/sql/users.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取对象</span></span><br><span class="line">    names = df[<span class="string">&#x27;name&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(names)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#支持字符串方式传入</span></span><br><span class="line">    df.select(<span class="string">&#x27;name&#x27;</span>).show()</span><br><span class="line">    df.select([<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;favorite_color&#x27;</span>]).show()</span><br><span class="line">    df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;favorite_numbers&quot;</span>).show()</span><br><span class="line">    <span class="comment">#支持column对象方式传入</span></span><br><span class="line">    df.select(df[<span class="string">&#x27;name&#x27;</span>], df[<span class="string">&#x27;favorite_color&#x27;</span>]).show()</span><br></pre></td></tr></table></figure>



<h4 id="filter和where"><a href="#filter和where" class="headerlink" title="filter和where"></a>filter和where</h4><p>功能：过滤DataFrame内的数据，返回一个过滤后的DataFrame</p>
<p>where和filter功能上是等价的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.\</span><br><span class="line">        appName(<span class="string">&#x27;study01&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;json&#x27;</span>)\</span><br><span class="line">        .load(<span class="string">&quot;hdfs://node1:8020/input/sql/people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    df.where(<span class="string">&quot;age &lt; 30&quot;</span>).show()</span><br><span class="line">    df.where(df[<span class="string">&#x27;age&#x27;</span>] &lt; <span class="number">30</span>).show()</span><br><span class="line"></span><br><span class="line">    df.<span class="built_in">filter</span>(<span class="string">&quot;age &lt; 30&quot;</span>).show()</span><br><span class="line">    df.<span class="built_in">filter</span>(df[<span class="string">&#x27;age&#x27;</span>] &lt; <span class="number">30</span>).show()</span><br></pre></td></tr></table></figure>



<h3 id="groupBy-分组"><a href="#groupBy-分组" class="headerlink" title="groupBy 分组"></a>groupBy 分组</h3><p>功能：按照指定的列进行分组，返回值是GroupedData对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.\</span><br><span class="line">        appName(<span class="string">&#x27;study01&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;csv&#x27;</span>)\</span><br><span class="line">        .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>)\</span><br><span class="line">        .option(<span class="string">&quot;header&quot;</span>, <span class="literal">False</span>)\</span><br><span class="line">        .option(<span class="string">&quot;encoding&quot;</span>, <span class="string">&quot;utf-8&quot;</span>)\</span><br><span class="line">        .schema(<span class="string">&quot;name STRING, age INT, job STRING&quot;</span>)\</span><br><span class="line">        .load(<span class="string">&quot;hdfs://node1:8020/input/sql/people.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    df.groupBy(<span class="string">&#x27;job&#x27;</span>).count().show()</span><br><span class="line">    df.groupBy(<span class="string">&#x27;job&#x27;</span>, <span class="string">&#x27;age&#x27;</span>).count().show()</span><br></pre></td></tr></table></figure>

<p><strong>GroupedData对象</strong></p>
<p>GroupedData对象是一个特殊的DataFrame数据集。这个对象是经过groupBy后得到的返回值，内部记录了以分组形式存储的数据。</p>
<p>GroupedData对象的API：min、max、avg、sum和count等</p>
<h3 id="SQL风格"><a href="#SQL风格" class="headerlink" title="SQL风格"></a>SQL风格</h3><h4 id="注册DataFrame称为表"><a href="#注册DataFrame称为表" class="headerlink" title="注册DataFrame称为表"></a>注册DataFrame称为表</h4><p>要使用SQL风格的语法，需要将DataFrame注册成表，采用方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.createTempView(<span class="string">&#x27;score&#x27;</span>) <span class="comment">#注册一个临时表</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&#x27;score&#x27;</span>) <span class="comment">#注册一个临时表,如果存在进行替换</span></span><br><span class="line">df.createGlobalTempView(<span class="string">&#x27;score&#x27;</span>) <span class="comment">#注册一个全局表</span></span><br></pre></td></tr></table></figure>

<p><strong>全局表</strong></p>
<p>跨SparkSession对象使用，在一个程序内的多个SparkSession中均可调用，查询前带上前缀 <code>global_temp</code></p>
<p><strong>临时表</strong></p>
<p>只在当前SparkSession中可用</p>
<h4 id="SQL查询"><a href="#SQL查询" class="headerlink" title="SQL查询"></a>SQL查询</h4><p>注册好表后，可以通过</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparksession.sql(sql语句)</span><br></pre></td></tr></table></figure>

<p>来执行sql查询，返回一个新的DataFrame</p>
<h4 id="pyspark-sql-functions包"><a href="#pyspark-sql-functions包" class="headerlink" title="pyspark.sql.functions包"></a>pyspark.sql.functions包</h4><p>pyspark.sql.functions包提供了一系列计算函数供SparkSQL使用。包中的函数返回值多数都是Column对象。</p>
<h4 id="数据清洗API"><a href="#数据清洗API" class="headerlink" title="数据清洗API"></a>数据清洗API</h4><h5 id="dropDuplicates"><a href="#dropDuplicates" class="headerlink" title="dropDuplicates"></a>dropDuplicates</h5><p>功能：对DF数据进行去重，如果重复数据有多条，取第一条。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 无参数是对数据进行整体去重</span></span><br><span class="line">df.dropDuplicates().show()</span><br><span class="line"><span class="comment"># 可以针对字段进行去重</span></span><br><span class="line">df.dropDuplicates([<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;job&#x27;</span>]).show()</span><br></pre></td></tr></table></figure>



<h5 id="dropna"><a href="#dropna" class="headerlink" title="dropna"></a>dropna</h5><p>功能：如果数据中包含null，通过dropna来进行判断，符合条件就删除这一行数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果有缺失，进行数据删除</span></span><br><span class="line"><span class="comment"># 无参数，为how=&#x27;any&#x27;,只要有一个列是null，数据整行删除。如果how=&#x27;all&#x27; 表示全部列为空，才会删除</span></span><br><span class="line">df.dropna().show()</span><br><span class="line"><span class="comment"># 指定阈值进行删除，thresh=3表示，有效的列最少有3个，这行数据才保留，设定thresh后，how参数无效</span></span><br><span class="line">df.dropna(thresh=<span class="number">3</span>).show()</span><br><span class="line"><span class="comment"># 可以指定阈值 配合指定列进行工作</span></span><br><span class="line"><span class="comment"># thresh=2，subset=[&#x27;name&#x27;, &#x27;age&#x27;]表示 针对这2列，有效列最少为2个才保留数据</span></span><br><span class="line">df.dropna(thresh=<span class="number">2</span>, subset=[<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>]).show()</span><br></pre></td></tr></table></figure>



<h5 id="fillna"><a href="#fillna" class="headerlink" title="fillna"></a>fillna</h5><p>功能：根据参数的规则，来进行null的替换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将所有为空，按照指定的值进行填充，任何空都会被填充</span></span><br><span class="line">df.fillna(<span class="string">&#x27;loss&#x27;</span>).show()</span><br><span class="line"><span class="comment"># 指定列进行填充</span></span><br><span class="line">df.fillna(<span class="string">&#x27;loss&#x27;</span>, subset=[<span class="string">&#x27;job&#x27;</span>]).show()</span><br><span class="line"><span class="comment"># 给定字典，设定各个列的填充规则</span></span><br><span class="line">df.fillna(&#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;未知姓名&#x27;</span>, <span class="string">&#x27;age&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;job&#x27;</span>: <span class="string">&#x27;worker&#x27;</span>&#125;).show()</span><br></pre></td></tr></table></figure>



<h3 id="SparkSQL统一API写出DataFrame数据"><a href="#SparkSQL统一API写出DataFrame数据" class="headerlink" title="SparkSQL统一API写出DataFrame数据"></a>SparkSQL统一API写出DataFrame数据</h3><p>统一API语法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df.write.mode().<span class="built_in">format</span>().option(K,V).save(PATH)</span><br><span class="line"><span class="comment"># mode 传入模式字符串，可选，append 追加，overwrite 覆盖， ignore 忽略，error 重复就报异常（默认）</span></span><br><span class="line"><span class="comment"># format 传入字符串格式，可选，text，csv，json，parquet，orc，avro，jdbc</span></span><br><span class="line"><span class="comment"># 注意text源只支持单列df写出</span></span><br><span class="line"><span class="comment"># option 设置属性</span></span><br><span class="line"><span class="comment"># save 写出路径，支持本地文件和hdfs</span></span><br></pre></td></tr></table></figure>



<h3 id="通过JDBC读写数据库（MYSQL）"><a href="#通过JDBC读写数据库（MYSQL）" class="headerlink" title="通过JDBC读写数据库（MYSQL）"></a>通过JDBC读写数据库（MYSQL）</h3><h4 id="安装驱动"><a href="#安装驱动" class="headerlink" title="安装驱动"></a>安装驱动</h4><p>驱动文件的版本要与mysql的版本对应</p>
<p>安装路径(linux)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/server/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/jars</span><br></pre></td></tr></table></figure>

<h4 id="写出"><a href="#写出" class="headerlink" title="写出"></a>写出</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>).\</span><br><span class="line">       	<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).\</span><br><span class="line">        option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://node1:3306/test?useSSL=false&amp;useUnicode=true&quot;</span>).\</span><br><span class="line">        option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;u_data&quot;</span>).\</span><br><span class="line">        option(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;root&quot;</span>).\</span><br><span class="line">        option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;hadoop&quot;</span>).\</span><br><span class="line">        save()</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<ul>
<li>jdbc连接字符串中，建议使用useSSL&#x3D;false确保连接可以正常连接（不使用SSL安全协议进行连接）</li>
<li>jdbc连接字符串中，建议使用useUnicode&#x3D;true来确保传输中不出现乱码</li>
<li>save()不要填参数</li>
<li>datable属性，指定写出的表名</li>
</ul>
<h4 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> df = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>). \</span><br><span class="line">        option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://node1:3306/test?useSSL=false&amp;useUnicode=true&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;u_data&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;hadoop&quot;</span>). \</span><br><span class="line">        load()</span><br><span class="line"></span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<ul>
<li>读出来是自带schema，不需要设置schema，因为数据库就有schema</li>
<li>load() 不需要加参数</li>
<li>dbtable：指定读取的表名</li>
</ul>
<h1 id="SparkSQL函数"><a href="#SparkSQL函数" class="headerlink" title="SparkSQL函数"></a>SparkSQL函数</h1><h2 id="定义UDF函数"><a href="#定义UDF函数" class="headerlink" title="定义UDF函数"></a>定义UDF函数</h2><p>SqarkSQL和Hive一样支持定义函数。</p>
<p>Hive自定义函数类型：</p>
<p><strong>1. UDF （User - Defined - Function）函数</strong></p>
<p>一对一的关系，输入一个值经过函数以后输出一个值；</p>
<p>在Hive中继承UDF类，方法名称为evaluate，返回值不能是void，其实就是实现一个方法。</p>
<p><strong>2. UDAF（User - Defined  Aggregation  Function）聚合函数</strong></p>
<p>多对一的关系，输入多个值输出一个值，通常与groupBy联合使用。</p>
<p><strong>3. UDTF（User  Defined  Table-Generating Functions）函数</strong></p>
<p>一对多的关系，输入一个值输出多个值（一行变多行）；</p>
<p>用户自定义生成函数，有点像flatMap。</p>
<blockquote>
<p>在SparkSQL中，目前仅仅支持UDF和UDAF，目前python仅支持UDF</p>
</blockquote>
<h4 id="SparkSQL定义UDF函数"><a href="#SparkSQL定义UDF函数" class="headerlink" title="SparkSQL定义UDF函数"></a>SparkSQL定义UDF函数</h4><h5 id="方式1-1"><a href="#方式1-1" class="headerlink" title="方式1"></a>方式1</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sparksession.udf.register()</span><br><span class="line"><span class="comment"># 注册的UDF可以用于DSL和SQL，返回值用于SQL风格，传参内给的名字用于SQL风格</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># udf对象 = sarksession.udf.register(参数1, 参数2, 参数3)</span></span><br><span class="line"><span class="comment"># 参数1：UDF名称，可用于SQL风格</span></span><br><span class="line"><span class="comment"># 参数2：被注册成UDF的方法名</span></span><br><span class="line"><span class="comment"># 参数3：声明UDF的返回值类型</span></span><br><span class="line"><span class="comment"># udf对象：返回值对象，是一个UDF对象，可用于DSL风格</span></span><br></pre></td></tr></table></figure>

<p><strong>方式2</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pyspark.sql.functions.udf</span><br><span class="line"><span class="comment"># 仅能用于DSL风格</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># from pyspark.sql import functions as F</span></span><br><span class="line"><span class="comment"># udf对象 = F.udf(参数1, 参数2)</span></span><br><span class="line"><span class="comment"># 参数1：被注册成UDF的方法名, 指具体的计算方法。如：def add(x,y): x + y  add就是将要被注册成UDF的方法名</span></span><br><span class="line"><span class="comment"># 参数2：声明UDF的返回值类型</span></span><br><span class="line"><span class="comment"># udf对象：返回值对象，是一个UDF对象，可用于DSL风格</span></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.builder.\</span><br><span class="line">        appName(<span class="string">&#x27;study01&#x27;</span>).\</span><br><span class="line">        master(<span class="string">&#x27;local[*]&#x27;</span>).\</span><br><span class="line">        config(<span class="string">&#x27;spark.sql.shuffle.partitions&#x27;</span>, <span class="string">&#x27;4&#x27;</span>).\</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    rdd = sc.textFile(<span class="string">&quot;hdfs://node1:8020/input/sql/stu_score.txt&quot;</span>). \</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;,&#x27;</span>)). \</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: [<span class="built_in">int</span>(x[<span class="number">0</span>]), x[<span class="number">1</span>], <span class="built_in">int</span>(x[<span class="number">2</span>])])</span><br><span class="line"></span><br><span class="line">    df = rdd.toDF([<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;score&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 方式1 注册UDF，功能：将数字乘于10</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_ride_10</span>(<span class="params">num</span>):</span><br><span class="line">        <span class="keyword">return</span> num * <span class="number">10</span></span><br><span class="line">    <span class="comment"># 返回值用于DSL风格，内部注册的名称用于SQL（字符串表达式）风格</span></span><br><span class="line">    <span class="comment"># 参数1：UDF名称（可用于SQL风格），参数2：UDF的本体方法（处理逻辑），参数3：声明返回值类型</span></span><br><span class="line">    <span class="comment"># 返回值可用于DSL</span></span><br><span class="line">    udf2 = spark.udf.register(<span class="string">&quot;udf1&quot;</span>, num_ride_10, IntegerType())</span><br><span class="line"></span><br><span class="line">    df.select(udf2(df[<span class="string">&#x27;score&#x27;</span>])).show()</span><br><span class="line">    df.selectExpr(<span class="string">&#x27;udf1(score)&#x27;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#房市注册，仅能用于DSL风格</span></span><br><span class="line">    udf3 = F.udf(num_ride_10, IntegerType())</span><br><span class="line">    df.select(udf3(df[<span class="string">&#x27;score&#x27;</span>])).show()</span><br></pre></td></tr></table></figure>



<p>注意：</p>
<p>返回int，可以用IntegerType</p>
<p>返回小数，可以用FloatType或者DoubleType</p>
<p>返回数组list，可用ArrayType</p>
<p>返回字典，可用StructType</p>
<p>这些Spark内置的数据类型均存储在 <code>pyspark.sql.types</code> 包中</p>
<h2 id="使用窗口函数"><a href="#使用窗口函数" class="headerlink" title="使用窗口函数"></a>使用窗口函数</h2><h3 id="开窗函数"><a href="#开窗函数" class="headerlink" title="开窗函数"></a>开窗函数</h3><p>开窗函数的引入是为了既显示聚集前的数据，又显示聚集后的数据。即在每一行的最后一列添加聚合函数的结果。</p>
<p>开窗用于为行定义一个窗口（窗口指运算将要操作的行的集合），它对一组值进行操作，不需要使用GROUP BY子句对数据进行分组，能够在同一行中同时返回基础行的列和聚合列。</p>
<h3 id="聚合函数和开窗函数"><a href="#聚合函数和开窗函数" class="headerlink" title="聚合函数和开窗函数"></a>聚合函数和开窗函数</h3><p>聚合函数是将多行变成一行， count， avg，…</p>
<p>开窗函数是将一行变成多行；</p>
<p>聚合函数如果要显示其他的列必须将列加入到group by中</p>
<p>开窗函数可以不用group by，直接将所有信息显示出来</p>
<h3 id="开窗函数分类"><a href="#开窗函数分类" class="headerlink" title="开窗函数分类"></a>开窗函数分类</h3><ol>
<li><p>聚合开窗函数 </p>
<p>聚合函数（列） OVER（选项），这里的选项可以是PARTITION BY子句，但不可以是ORDER BY 子句。</p>
</li>
<li><p>排序开窗函数 </p>
<p>排序函数（列） OVER（选项），这里的选项可以是ORDER BY子句，也可以是OVER（PARTITION BY 子句 ORDER BY 子句），但不可以是PARTITION BY 子句。</p>
</li>
<li><p>分区类型NTILE的窗口函数</p>
</li>
</ol>
<h3 id="窗口函数的语法"><a href="#窗口函数的语法" class="headerlink" title="窗口函数的语法"></a>窗口函数的语法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 聚合类型 SUM\MIN\MAX\AVG\COUNT</span></span><br><span class="line"><span class="built_in">sum</span>() OVER([PARTITION BY XXX][ORDER BY XXX [DESC]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 排序类型： ROW_NUMBER|RANK|DENSE_RANK</span></span><br><span class="line">ROW_NUMBER() OVER([PARTITION BY XXX][ORDER BY XXX [DESC]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分区类型： NTILE</span></span><br><span class="line">NTILE(number) OVER([PARTITION BY XXX][ORDER BY XXX [DESC]])</span><br></pre></td></tr></table></figure>





<h1 id="SparkSQL运行流程"><a href="#SparkSQL运行流程" class="headerlink" title="SparkSQL运行流程"></a>SparkSQL运行流程</h1><h2 id="RDD执行流程"><a href="#RDD执行流程" class="headerlink" title="RDD执行流程"></a>RDD执行流程</h2><blockquote>
<p>代码 -&gt; DAG调度器逻辑任务 -&gt; Task调度器任务分配和管理监控 -&gt; Worker干活</p>
</blockquote>
<h2 id="SparkSQL的自动优化"><a href="#SparkSQL的自动优化" class="headerlink" title="SparkSQL的自动优化"></a>SparkSQL的自动优化</h2><p>RDD的运行完全按照开发者的代码执行，如果开发者水平有限，RDD的执行效率也会受到影响。</p>
<p>而SparkSQL会对写完的代码，执行“自动优化”，以提升代码的运行效率，避免开发者水平影响到代码执行效率。</p>
<p>其原因是RDD中的数据类型不限格式和结构，DataFrame是二维表结构，可以被针对处理。</p>
<p>SparkSQL的自动优化，依赖于Catalyst优化器</p>
<h2 id="Catalyst优化器"><a href="#Catalyst优化器" class="headerlink" title="Catalyst优化器"></a>Catalyst优化器</h2><p>Catalyst优化器是为了解决过多依赖Hive的问题，用它替代Hive中的优化器。</p>
<p>SparkSQL架构如下：</p>
<p><img src="/2022/02/18/SparkSQL/image-20221211112825489.png" alt="image-20221211112825489"></p>
<ol>
<li>API层简单的说就是Spark会通过一些API接受SQL语句。</li>
<li>收到SQL语句后，将其交给Catalyst，Catalyst负责解析SQL，生成执行计划等。</li>
<li>Catalyst的输出应该是RDD的执行计划。</li>
<li>最终交由集群运行。</li>
</ol>
<p>具体流程：</p>
<p><img src="/2022/02/18/SparkSQL/image-20221211114333532.png" alt="image-20221211114333532"></p>
<p><strong>step1：解析SQL，并且生成AST（抽象语法树）</strong></p>
<p><img src="/2022/02/18/SparkSQL/image-20221211114438677.png" alt="image-20221211114438677"></p>
<p><strong>step2：在AST中加入元数据信息，这一步是为了优化，例如col &#x3D; col这样的条件</strong></p>
<p><img src="/2022/02/18/SparkSQL/image-20221211114704326.png" alt="image-20221211114704326"></p>
<p><strong>step3：对已经加入元数据的AST，输入优化器，进行优化</strong></p>
<p>两种常见的优化：</p>
<p><img src="/2022/02/18/SparkSQL/image-20221211115003319.png" alt="image-20221211115003319"></p>
<p><img src="/2022/02/18/SparkSQL/image-20221211115052191.png" alt="image-20221211115052191"></p>
<p><strong>step4：上面的过程生成的AST其实最终还无法运行，这个AST叫做逻辑计划，结束后需要生成物理计划，从而生成RDD来运行</strong></p>
<p>在生成“物理计划”的时候，会经过“成本模型”对整棵树再次执行优化，选择一个更好的计划。</p>
<p>在生成“物理计划”以后，因为考虑到性能，所以会使用代码生成，在机器中运行。</p>
<p><strong>总结</strong></p>
<p>catalyst优化细节很多，大方面的优化有2点：</p>
<ul>
<li><p>谓词下推（Predicate Pushdown）\ 断言下推：将逻辑判断提前到前面，以减少shuffle阶段的数据量， 即<strong>行过滤，提前执行where</strong></p>
</li>
<li><p>列值裁剪（Column Pruning）：将加载的列进行裁剪，尽量减少被处理的数据的宽度，即<strong>列过滤，提前规划select的字段数量</strong></p>
</li>
</ul>
<h2 id="SparkSQL执行流程"><a href="#SparkSQL执行流程" class="headerlink" title="SparkSQL执行流程"></a>SparkSQL执行流程</h2><p><img src="/2022/02/18/SparkSQL/image-20221211120120259.png" alt="image-20221211120120259"></p>
<ol>
<li>提交SparkSQL代码</li>
<li>catalyst优化<ol>
<li>生成原始AST语法树</li>
<li>标记AST元数据</li>
<li>进行断言下推和列值裁剪，以及其它方面的优化作用在AST上</li>
<li>将最终AST得到，生成执行计划</li>
<li>将执行计划翻译为RDD代码</li>
</ol>
</li>
<li>Driver执行环境入口构建（SparkSession）</li>
<li>DAG调度器规划逻辑任务</li>
<li>TASK调度器分配逻辑任务到具体Executor上工作并监控管理任务</li>
<li>Worker干活</li>
</ol>
<h1 id="SparkSQL整合Hive"><a href="#SparkSQL整合Hive" class="headerlink" title="SparkSQL整合Hive"></a>SparkSQL整合Hive</h1><h2 id="Hive组件"><a href="#Hive组件" class="headerlink" title="Hive组件"></a>Hive组件</h2><ol>
<li>SQL优化翻译器（执行引擎），翻译SQL到MapReduce并提交到YARN执行</li>
<li>MetaStore元数据管理中心</li>
</ol>
<h2 id="Spark-On-Hive原理"><a href="#Spark-On-Hive原理" class="headerlink" title="Spark On Hive原理"></a>Spark On Hive原理</h2><p>对于Spark来说，它自身是一个执行引擎，没有元数据管理功能。</p>
<p>而Spark和Hive结合，<strong>Spark提供执行引擎能力</strong>，<strong>Hive的MetaStore提供元数据管理能力</strong>，就产生Spark On Hive。</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>根据原理，就是Spark能够连接上Hive的MetaStore即可。</p>
<p>步骤1：</p>
<p>在Spark的conf目录中，创建hive-site.xml，内容如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&#x27;text/xsl&#x27; href=&#x27;configuration.xsl&#x27;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- spark创建表存到哪里 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- hive的metastore在哪 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://node1:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>步骤2：</p>
<p>将mysql的驱动jar包放到spark的jars目录。</p>
<blockquote>
<p>因为要连接元数据，会有部分功能连接到mysql库，需要mysql驱动包</p>
</blockquote>
<p>步骤3：</p>
<p>确保hive配置了MetaStore相关服务，检查hive配置文件目录内的：hive-site.xml，确保有如下配置。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://node1:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>步骤4：</p>
<p>启动hive的MetaStore服务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup /export/server/apache-hive-3.1.2-bin/bin/hive --service metastore 2&gt;&amp;1 &gt;&gt; /var/log/metastore.log &amp;</span><br></pre></td></tr></table></figure>

<p>nohup：后台启动程序的命令，使用</p>
<ul>
<li><code>nohup xxx命令 &amp;</code>  将命令后台执行，日志输出到当前目录的nohup.out中</li>
<li><code>nohup xxx命令 2&gt;&amp;1 &gt;&gt; 某路径下的日志文件 &amp;</code>  将命令后台执行，将日志输出到你指定的路径中</li>
</ul>
<p>测试：</p>
<p>bin&#x2F;pyspark：在里面直接写spark.sql(“sql语句”).show()即可</p>
<h2 id="在代码中集成Spark-On-Hive"><a href="#在代码中集成Spark-On-Hive" class="headerlink" title="在代码中集成Spark On Hive"></a>在代码中集成Spark On Hive</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession.\</span><br><span class="line">        builder.appName(<span class="string">&quot;create df&quot;</span>).\</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>).\</span><br><span class="line">        config(<span class="string">&quot;spark.sql.shuffle.paratitions&quot;</span>, <span class="string">&quot;4&quot;</span>).\</span><br><span class="line">        config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://node1:8020/user/hive/warehouse&quot;</span>).\</span><br><span class="line">        config(<span class="string">&quot;hive.metastore.uris&quot;</span>, <span class="string">&quot;thrift://node1:9083&quot;</span>).\</span><br><span class="line">        enableHiveSupport().\ </span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&quot;select * from itheima.t_2&quot;</span>).show()</span><br></pre></td></tr></table></figure>



<h1 id="分布式SQL引擎配置"><a href="#分布式SQL引擎配置" class="headerlink" title="分布式SQL引擎配置"></a>分布式SQL引擎配置</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>Spark中有个服务叫做：ThriftServer服务，可以启动并监听在10000端口。这个服务对外提供功能，我们可以用数据库工具或者代码连接，直接写SQL操作Spark。</p>
<p><img src="/2022/02/18/SparkSQL/image-20221211223801811.png" alt="image-20221211223801811"></p>
<p>当使用ThriftServer后，相当于是一个持续性的Spark On Hive集成模式。它提供10000端口，持续对外提供服务，外部可以通过这个端口连接上来，写SQL，让Spark运行。</p>
<h2 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h2><p>启动ThriftServer</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">SPARK_HOME/sbin/start-thriftserver.sh --hiveconf hive.server2.thrift.port=10000 --hiveconf hive.server2.thrift.bind.host=node1 --master <span class="built_in">local</span>[2]</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">master选择<span class="built_in">local</span>，每一条sql都是<span class="built_in">local</span>进程执行</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">master选择yarn，每一条sql都是在YARN集群中执行</span></span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/" class="post-title-link" itemprop="url">Spark内核调度</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-09 14:07:07" itemprop="dateCreated datePublished" datetime="2022-02-09T14:07:07+08:00">2022-02-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h1><p>Spark的核心是根据RDD来实现的，Spark Scheduler则为Spark核心实现的重要一环，其作用就是任务调度。Spark的任务调度就是如何组织任务去处理RDD中每个分区的数据，根据RDD的依赖关系构建DAG，基于DAG划分Stage，将每个Stage中的任务发到指定的节点运行。基于Spark的任务调度原理，可以合理规划资源利用，做到尽可能最少的资源高效地完成任务计算。</p>
<p>WordCount的DAG图：</p>
<p><img src="/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/image-20221207141854497.png" alt="image-20221207141854497"></p>
<h2 id="DAG-1"><a href="#DAG-1" class="headerlink" title="DAG"></a>DAG</h2><p>DAG：有向无环图（有方向没有形成闭环的一个执行流程图），标识代码的逻辑执行流程。</p>
<p><img src="/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/image-20221207142235571.png" alt="image-20221207142235571"></p>
<p><strong>JOB和Action</strong></p>
<p>Action：返回值不是RDD的算子。它的作用是一个出发开关，会将Action算子之前的一串rdd依赖链条执行起来。一个Action会产生一个DAG图。</p>
<p><img src="/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/image-20221207142737106.png" alt="image-20221207142737106"></p>
<blockquote>
<p>1个Action会产生一个DAG，会在程序运行中产生一个JOB。 <code>1个Action = 1个DAG = 1个JOB</code></p>
<p>一个代码运行起来，在Spark中称之为Application。1个Application中，可以有多个JOB，每个JOB内含有一个DAG，同时每个JOB都是由一个Action产生的。</p>
</blockquote>
<p><strong>DAG和分区</strong></p>
<p>Spark是分布式（多分区）的，那么DAG和分区之间也是有关联的。</p>
<p><img src="/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/image-20221207143426357.png" alt="image-20221207143426357"></p>
<h1 id="DAG的宽窄依赖和阶段划分"><a href="#DAG的宽窄依赖和阶段划分" class="headerlink" title="DAG的宽窄依赖和阶段划分"></a>DAG的宽窄依赖和阶段划分</h1><p>SparkRDD前后之间的关系分为<code>宽依赖</code>和<code>窄依赖</code>。</p>
<p>窄依赖：父RDD的一个分区，全部将数据发给子RDD的一个分区。</p>
<p>宽依赖：父RDD的一个分区，将数据发给RDD的多个分区。</p>
<p>宽依赖还有一个别名：shuffle</p>
<h2 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h2><p><img src="/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/image-20221207143755864.png" alt="image-20221207143755864"></p>
<h2 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h2><p><img src="/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/image-20221207143830716.png" alt="image-20221207143830716"></p>
<h2 id="阶段划分"><a href="#阶段划分" class="headerlink" title="阶段划分"></a>阶段划分</h2><p>对于Spark来说，会根据DAG，按照宽依赖，划分不同的DAG阶段。</p>
<p>划分依据：从后向前，遇到宽依赖，就划分出一个阶段，称之为stage。</p>
<p><img src="/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/image-20221208092610241.png" alt="image-20221208092610241"></p>
<p>在stage内部，一定都是窄依赖。</p>
<h1 id="内存迭代计算"><a href="#内存迭代计算" class="headerlink" title="内存迭代计算"></a>内存迭代计算</h1><p><img src="/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/image-20221208094448308.png" alt="image-20221208094448308"></p>
<p>如图，基于带有分区的DAG，从图中可以得到逻辑上的最优task分配。一个task是一个线程来具体执行。图中task1中rdd1、rdd2和rdd3的迭代计算，都是由一个task（线程）完成，这一阶段的这条线，是纯内存计算。task1、task2和task3形成了3个并行的内存计算管道。</p>
<p>Spark默认受全局并行度的限制，除了个别算子有特殊分区情况，大部分算子，都会遵循全局并行的要求，来规划自己的分区数。如果全局并行度是3，其实大部分算子分区都是3。</p>
<blockquote>
<p>在Spark中，推荐只设置全局并行度，不要在算子上设置并行度</p>
</blockquote>
<h1 id="Spark并行度"><a href="#Spark并行度" class="headerlink" title="Spark并行度"></a>Spark并行度</h1><p>Spark的并行：在同一时间内，有多少个task在同时运行。</p>
<p>并行度：并行能力的设置。</p>
<p>比如：设置并行度是6，就有6个task并行执行，rdd的分区就被规划成6个分区。</p>
<h2 id="如何设置并行度"><a href="#如何设置并行度" class="headerlink" title="如何设置并行度"></a>如何设置并行度</h2><p>可以在代码中和配置文件中以及提交程序的客户端参数中设置。</p>
<p>优先级从高到低：</p>
<ol>
<li>代码中</li>
<li>客户端提交参数中</li>
<li>配置文件中</li>
<li>默认（1，但是不会全部以1来执行，多数情况下基于读取文件的分片数量来作为默认并行度）</li>
</ol>
<p>全局并行度配置参数</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spark.default.parallelism</span></span><br></pre></td></tr></table></figure>

<p><strong>推荐全局设置并行度</strong></p>
<p>配置文件中：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">conf/spark-defaults.conf中设置</span></span><br><span class="line"><span class="attr">spark.default.parallelism</span> <span class="string">100</span></span><br></pre></td></tr></table></figure>

<p>在客户端提交参数中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --conf &quot;spark.default.parallelism=100&quot;</span><br></pre></td></tr></table></figure>

<p>在代码中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf()</span><br><span class="line">conf.<span class="built_in">set</span>(<span class="string">&quot;spark.default.parallelism&quot;</span>, <span class="string">&quot;100&quot;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>全局并行度是推荐设置，不要针对RDD改分区，可能会影响内存迭代管道的构建，或者会产生额外的shuffle</p>
</blockquote>
<h2 id="集群中如何规划并行度"><a href="#集群中如何规划并行度" class="headerlink" title="集群中如何规划并行度"></a>集群中如何规划并行度</h2><p>设置为cpu总核的2<del>10倍。比如集群可用cpu核心是100个，建议并行度是200</del>1000。</p>
<p>设置成cpu核心的倍数，目的是让cpu尽量不空闲，执行完一个task后有其他task可以继续执行，提高cpu利用率。</p>
<h1 id="Spark任务调度"><a href="#Spark任务调度" class="headerlink" title="Spark任务调度"></a>Spark任务调度</h1><p>Spark的任务，由Driver进行调度，这个工作包括：</p>
<ol>
<li>逻辑DAG产生</li>
<li>分区DAG产生</li>
<li>Task划分</li>
<li>将Task分配给Executor并监控其工作</li>
</ol>
<p><img src="/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/image-20221208100811888.png" alt="image-20221208100811888"></p>
<p>如图：Spark程序的调度流程：</p>
<ol>
<li>构建Driver</li>
<li>构建SparkContext（执行环境入口对象）</li>
<li>基于DAG Scheduler（DAG调度器）构建逻辑Task分配</li>
<li>基于TaskScheduler（Task调度器）将逻辑Task分配到各个Executor上干活，并监控它们。</li>
<li>Worker（Executor），被TaskScheduler管理监控，听从它们的指令干活，并定期汇报进度。</li>
</ol>
<p>Driver内的两个组件：</p>
<ol>
<li>DAG调度器：将逻辑DAG图进行处理，最终得到逻辑上的Task划分。</li>
<li>TASK调度器：基于DAG Scheduler的产出，类规划这些逻辑TASK应该在哪些物理的Executor上运行，以及监控管理它们的运行。</li>
</ol>
<h1 id="Spark概念名词大全"><a href="#Spark概念名词大全" class="headerlink" title="Spark概念名词大全"></a>Spark概念名词大全</h1><ul>
<li>ClusterManager：在Standalone模式中即为Master（主节点），控制整个集群，监控Worker。在YARN模式中为资源管理器。</li>
<li>Worker：从节点，负责控制计算节点，启动Executor。在YARN模式中为NodeManager，负责计算节点的控制。</li>
<li>Driver：运行Application的main()函数并创建SparkContext。</li>
<li>Executor：执行器，在worker node上执行任务的组件、用于启动线程池运行任务。每个Application拥有独立的一组Executors。</li>
<li>SparkContext：整个应用的上下文，控制应用的生命周期。</li>
<li>RDD：Spark的基本计算单元，一组RDD可形成执行的有向无环图RDD Graph。</li>
<li>DAG Scheduler：实现将Spark作业分解成一到多个Stage，每个Stage根据RDD的Partition个数决定Task的个数，然后生成相应的Task set放到TaskScheduler中。</li>
<li>TaskScheduler：将任务（Task）分发给Executor执行。（所以Executor执行的就是我们的代码）</li>
<li>Stage：一个Spark作业一般包含一到多个Stage。</li>
<li>Task：一个Stage包含一到多个Task，通过多个Task实现并行运行的功能。</li>
<li>Transformations：转换(Transformations) (如：map, filter, groupBy, join等)，Transformations操作是Lazy的，也就是说从一个RDD转换生成另一个RDD的操作不是马上执行，Spark在遇到Transformations操作时只会记录需要这样的操作，并不会去执行，需要等到有Actions操作的时候才会真正启动计算过程进行计算。（后面的wc例子就会有很好的说明）</li>
<li>Actions：操作(Actions) (如：count, collect, save等)，Actions操作会返回结果或把RDD数据写到存储系统中。Actions是触发Spark启动计算的动因。</li>
<li>SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。SparkEnv内创建并包含如下一些重要组件的引用。</li>
<li>MapOutPutTracker：负责Shuffle元信息的存储。</li>
<li>BroadcastManager：负责广播变量的控制与元信息的存储。</li>
<li>BlockManager：负责存储管理、创建和查找块。</li>
<li>MetricsSystem：监控运行时性能指标信息。</li>
<li>SparkConf：负责存储配置信息。</li>
</ul>
<h2 id="Spark层级梳理"><a href="#Spark层级梳理" class="headerlink" title="Spark层级梳理"></a>Spark层级梳理</h2><ol>
<li>一个Spark环境可以运行多个Application。</li>
<li>一个代码运行起来，会称为一个Application。</li>
<li>Application内部可以有多个Job</li>
<li>每个Job由一个Action产生，并且每个Job有自己的DAG执行图</li>
<li>一个Job的DAG图，会基于窄依赖和宽依赖划分成不同的阶段</li>
<li>不同阶段内基于分区数量，形成多个并行的内存迭代管道</li>
<li>每个内存迭代管道形成一个Task（DAG调度器划分将JOB内划分出具体的Task任务，一个JOB被划分出来的task在逻辑上称之为这个job的taskset）</li>
</ol>
<h2 id="Spark-Shuffle"><a href="#Spark-Shuffle" class="headerlink" title="Spark Shuffle"></a>Spark Shuffle</h2><h3 id="MapReduce-Shuffle"><a href="#MapReduce-Shuffle" class="headerlink" title="MapReduce Shuffle"></a>MapReduce Shuffle</h3><p><img src="/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/image-20221208103531463.png" alt="image-20221208103531463"></p>
<h3 id="Spark-Shuffle-1"><a href="#Spark-Shuffle-1" class="headerlink" title="Spark Shuffle"></a>Spark Shuffle</h3><p>Spark在DAG调度阶段会将一个Job划分成多个Stage，上游Stage做map工作，下游Stage做Reduce工作，其本质上还是MapReduce计算框架。</p>
<p>Shuffle是连接map和reduce之间的桥梁，它将map的输出对应到reduce输入中，涉及到序列化、跨节点网络IO以及磁盘读写IO等。</p>
<p><img src="/2022/02/09/Spark-%E5%86%85%E6%A0%B8%E8%B0%83%E5%BA%A6/image-20221208103955136.png" alt="image-20221208103955136"></p>
<p>Spark的Shuffle分为Write和Read两个阶段，分属于两个不同的Stage，前者是Parent Stage的最后一步，后者是Child Stage的第一步。</p>
<p>执行Shuffle的主体是Stage中的并发任务，这些任务份ShuffleMapTask和ResultTask两种，ShuffleMapTask要进行Shuffle，ResultTask负责返回计算结果，一个Job中只有最后的Stage采用ResultTask，其他的均为ShuffleMapTask。如果要按照map段和reduce端来分析的话，ShuffleMapTask可以即是map端任务，又是reduce端任务，因为Spark中Shuffle是可以串行的；ResultTask则只能充当reduce端任务的角色。</p>
<h3 id="ShuffleManager（Shuffle管理器）"><a href="#ShuffleManager（Shuffle管理器）" class="headerlink" title="ShuffleManager（Shuffle管理器）"></a>ShuffleManager（Shuffle管理器）</h3><p>ShuffleManager：负责Shuffle过程的执行、计算和处理。</p>
<p>实现方式有两种：HashShuffleManager和SortShuffleManager</p>
<h4 id="HashShuffleManager"><a href="#HashShuffleManager" class="headerlink" title="HashShuffleManager"></a>HashShuffleManager</h4><p>Spark1.2以前，默认的shuffle计算引擎，其缺点是会产生大量的中间磁盘文件，进而有大量的磁盘IO操作影响了性能。</p>
<h4 id="SortShuffleManager"><a href="#SortShuffleManager" class="headerlink" title="SortShuffleManager"></a>SortShuffleManager</h4><p>Spark1.2后，默认的ShuffleManager。对比HashShuffleManager的改进，每个task在进行shuffle操作时，虽然也会产生大量的中间磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个task就只有一个磁盘文件。在下一次读取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/" class="post-title-link" itemprop="url">Spark核心编程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-05 11:11:10" itemprop="dateCreated datePublished" datetime="2022-02-05T11:11:10+08:00">2022-02-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h1><h2 id="RDD-定义"><a href="#RDD-定义" class="headerlink" title="RDD 定义"></a>RDD 定义</h2><blockquote>
<p>RDD(Resilient Distributed Dataset) 弹性分布式数据集，是Spark中最基本的数据抽象，代表一个不可变、可分区、里面的元素可以并行计算的集合</p>
</blockquote>
<ul>
<li>Dataset：一个数据集合，用于存放数据的</li>
<li>Distributed：分布式存储，可用于并行计算</li>
<li>Resilient：可以存储在内存中或磁盘中</li>
</ul>
<h2 id="RDD五大特性"><a href="#RDD五大特性" class="headerlink" title="RDD五大特性"></a>RDD五大特性</h2><h3 id="RDD是有分区的"><a href="#RDD是有分区的" class="headerlink" title="RDD是有分区的"></a>RDD是有分区的</h3><p>RDD的分区是RDD数据存储的最小单位</p>
<p>一份RDD的数据，本质上是分割成了多个分区</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]).glom().collect()</span><br><span class="line">[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]]                                         </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>], <span class="number">3</span>).glom().collect()</span><br><span class="line">[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure>

<h3 id="RDD的方法会作用在其所有分区上"><a href="#RDD的方法会作用在其所有分区上" class="headerlink" title="RDD的方法会作用在其所有分区上"></a>RDD的方法会作用在其所有分区上</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>], <span class="number">3</span>).glom().collect()</span><br><span class="line">[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>], <span class="number">3</span>).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x * <span class="number">10</span>).glom().collect()</span><br><span class="line">[[<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>], [<span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>], [<span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>, <span class="number">100</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">#RDD 3个分区，在执行了map操作，将数据都乘以10后，3个分区的数据都乘以10了</span></span><br></pre></td></tr></table></figure>

<h3 id="RDD之间是有依赖关系（RDD有血缘关系）"><a href="#RDD之间是有依赖关系（RDD有血缘关系）" class="headerlink" title="RDD之间是有依赖关系（RDD有血缘关系）"></a>RDD之间是有依赖关系（RDD有血缘关系）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd1 = sc.textFile(<span class="string">&quot;hdfs://node1:8020/input/words.txt&quot;</span>)</span><br><span class="line">    rdd2 = rdd1.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    rdd3 = rdd2.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">    rdd4 = rdd3.reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(rdd4.collect())</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#rdd2依赖rdd1，rdd3依赖rdd2，rdd4依赖rdd3</span></span><br></pre></td></tr></table></figure>



<h3 id="Key-Value型的RDD可以有分区器（可能的）"><a href="#Key-Value型的RDD可以有分区器（可能的）" class="headerlink" title="Key-Value型的RDD可以有分区器（可能的）"></a>Key-Value型的RDD可以有分区器（可能的）</h3><p>默认分区器：Hash分区规则，可以手动设置一个分区器（rdd.partitionBy的方法来设置）</p>
<p>这个特性是可能的，因为并不是所有的RDD都是Key-Value型</p>
<p>Key-Value RDD：RDD中存储的是二元元组，这就是Key-Value型RDD</p>
<p>二元元组：只有2个元素的元组，比如（’hadoop’, 1)</p>
<h3 id="RDD的分区规划，会尽量靠近数据所在的服务器"><a href="#RDD的分区规划，会尽量靠近数据所在的服务器" class="headerlink" title="RDD的分区规划，会尽量靠近数据所在的服务器"></a>RDD的分区规划，会尽量靠近数据所在的服务器</h3><p>在初始RDD（读取数据的时候）规划的时候，分区会尽量规划到存储数据所在的服务器上。这样可以走本地读取，避免网络读取。</p>
<p>本地读取：Executor所在的服务器，同样是一个DataNode，同时这个DataNode上有它要读取的数据，所以可以直接取机器硬盘上的数据，无需走网络传输。</p>
<p>Spark会在确保并行计算能力的前提下，尽量确保本地读取，从而提升性能</p>
<h1 id="RDD编程"><a href="#RDD编程" class="headerlink" title="RDD编程"></a>RDD编程</h1><h2 id="程序入口SparkContext对象"><a href="#程序入口SparkContext对象" class="headerlink" title="程序入口SparkContext对象"></a>程序入口SparkContext对象</h2><p>Spark RDD编程的程序入口对象是SparkContext对象（不论何种语言）</p>
<p>只有构建出SparkContext，基于它才能执行后续的API调用和计算</p>
<p>本质上，SparkContext对编程来说，主要功能就是创建第一个RDD出来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="RDD的创建"><a href="#RDD的创建" class="headerlink" title="RDD的创建"></a>RDD的创建</h2><p>RDD的创建主要有2种方式：</p>
<ul>
<li>通过并行化集合创建（本地对象转分布式RDD）</li>
<li>读取外部数据源（读取文件）</li>
</ul>
<p><strong>并行化创建</strong></p>
<p>并行化创建是指将本地集合 -&gt; 转向分布式RDD</p>
<p><strong>获取RDD分区数</strong></p>
<p><code>getNumPartitions</code>  API     </p>
<p>获取RDD分区数量，返回值是Int数字</p>
<p><strong>读取文件创建</strong></p>
<p><code>textFile</code> API   </p>
<p>可以读取本地文件，也可以读取HDFS数据</p>
<p><code>wholeTextFile</code> API   </p>
<p>小文件读取专用，适合读取一堆小文件</p>
<h2 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h2><p>分布式集合对象上的API称之为算子</p>
<p><strong>算子分类</strong></p>
<ul>
<li>Transformation：转换算子</li>
<li>Action：动作（行动）算子</li>
</ul>
<p><strong>Transformation算子</strong></p>
<p>定义：RDD的算子，返回值仍旧是一个RDD的，称为转换算子</p>
<p>特性：这类算子是<code>lazy  懒加载</code>的。如果没有action算子，Tranformation算子是不工作的。</p>
<p><strong>Action算子</strong></p>
<p>定义：返回值不是RDD的就是Action算子</p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206134922918.png" alt="image-20221206134922918"></p>
<h2 id="常用的Transformation算子"><a href="#常用的Transformation算子" class="headerlink" title="常用的Transformation算子"></a>常用的Transformation算子</h2><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p>功能：map算子，是将RDD的数据一条条处理（处理的逻辑基于map算子中接收的处理函数），返回新的RDD</p>
<p>语法：</p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206135501887.png" alt="image-20221206135501887"></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;SPARK_HOME&#x27;</span>] = <span class="string">&#x27;/export/server/spark-3.2.0-bin-hadoop3.2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">map_func</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="keyword">return</span> data * <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># f:(T) -&gt; U</span></span><br><span class="line">    <span class="built_in">print</span>(rdd.<span class="built_in">map</span>(map_func).collect())</span><br><span class="line"></span><br><span class="line">    <span class="comment">#匿名lambda的方式</span></span><br><span class="line">    <span class="built_in">print</span>(rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x * <span class="number">10</span>).collect())</span><br><span class="line">    </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[10, 20, 30, 40, 50, 60, 70, 80]</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[10, 20, 30, 40, 50, 60, 70, 80]</span></span><br></pre></td></tr></table></figure>



<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><p>功能：对RDD执行map操作，然后进行<strong>解除嵌套</strong>操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#嵌套的list</span></span><br><span class="line"><span class="built_in">list</span> = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]]</span><br><span class="line"><span class="comment">#解除嵌套的list</span></span><br><span class="line"><span class="built_in">list</span> = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]</span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;SPARK_HOME&#x27;</span>] = <span class="string">&#x27;/export/server/spark-3.2.0-bin-hadoop3.2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="string">&#x27;a b c&#x27;</span>, <span class="string">&#x27;a c e&#x27;</span>, <span class="string">&#x27;e c a&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(rdd.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27; &#x27;</span>)).collect())</span><br><span class="line">    </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;a&#x27;, &#x27;c&#x27;, &#x27;e&#x27;, &#x27;e&#x27;, &#x27;c&#x27;, &#x27;a&#x27;]      </span></span><br></pre></td></tr></table></figure>



<h3 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h3><p>功能：针对KV型的RDD，自动按照key分组，然后根据你提供的聚合逻辑，完成组内数据（value）的聚合操作</p>
<p>用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd.reduceByKey(func)</span><br><span class="line"><span class="comment"># func: (V, V) -&gt; V</span></span><br><span class="line"><span class="comment"># 接受2个传入参数（类型要一致），返回一个返回值，类型和传入要求一致</span></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;SPARK_HOME&#x27;</span>] = <span class="string">&#x27;/export/server/spark-3.2.0-bin-hadoop3.2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([(<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;c&#x27;</span>,<span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>,<span class="number">1</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>)])</span><br><span class="line">    <span class="built_in">print</span>(rdd.reduceByKey(<span class="keyword">lambda</span> a, b: a + b).collect())</span><br><span class="line">    </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[(&#x27;b&#x27;, 2), (&#x27;c&#x27;, 1), (&#x27;a&#x27;, 3)]      </span></span><br></pre></td></tr></table></figure>



<h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><p>功能：将rdd的数据进行分组</p>
<p>语法：</p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206141147532.png" alt="image-20221206141147532"></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">    rdd2 = rdd.groupBy(<span class="keyword">lambda</span> num: <span class="string">&#x27;even&#x27;</span> <span class="keyword">if</span> num % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;odd&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(rdd2.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], <span class="built_in">list</span>(x[<span class="number">1</span>]))).collect())</span><br><span class="line">    </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[(&#x27;even&#x27;, [2, 4, 6, 8]), (&#x27;odd&#x27;, [1, 3, 5, 7])]  </span></span><br></pre></td></tr></table></figure>



<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>功能：过滤想要的数据进行保留</p>
<p>语法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd.<span class="built_in">filter</span>(func)</span><br><span class="line"><span class="comment"># func: (T) -&gt; bool 传入1个参数（随意类型），返回值必须是True或False</span></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(rdd.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">1</span>).collect())</span><br><span class="line">    </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[1, 3, 5, 7]  </span></span><br></pre></td></tr></table></figure>



<h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><p>功能：对rdd数据进行去重，返回新的rdd</p>
<p>语法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd.distinct(参数)</span><br><span class="line"><span class="comment"># 参数，去重分区数量，一般不用传</span></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(rdd.distinct().collect())</span><br><span class="line">    </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[4, 8, 1, 5, 2, 6, 3, 7]</span></span><br></pre></td></tr></table></figure>



<h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><p>功能：2个rdd合并成一个rdd返回，只是合并，不会去重，不同类型的rdd均可以合并</p>
<p>用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.union(other_rdd)</span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd1 = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">    rdd2 = sc.parallelize([<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(rdd1.union(rdd2).collect())</span><br><span class="line">    </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[1, 2, 3, 4, 5, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]</span></span><br></pre></td></tr></table></figure>



<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p>功能：对rdd进行join操作（可实现SQL的内&#x2F;外连接）</p>
<p>注意：join算子只能用于二元元组</p>
<p>用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd.join(other_add) <span class="comment">#内连接</span></span><br><span class="line">rdd.leftOuterJoin(other_rdd) <span class="comment">#左外</span></span><br><span class="line">rdd.rightOuterJoin(other_rdd) <span class="comment">#右外</span></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd1 = sc.parallelize([(<span class="number">101</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="number">102</span>, <span class="string">&#x27;b&#x27;</span>), (<span class="number">103</span>, <span class="string">&#x27;c&#x27;</span>), (<span class="number">104</span>, <span class="string">&#x27;d&#x27;</span>)])</span><br><span class="line">    rdd2 = sc.parallelize([(<span class="number">101</span>, <span class="string">&#x27;x&#x27;</span>), (<span class="number">103</span>, <span class="string">&#x27;y&#x27;</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(rdd1.join(rdd2).collect())</span><br><span class="line">    <span class="built_in">print</span>(rdd1.leftOuterJoin(rdd2).collect())</span><br><span class="line">    <span class="built_in">print</span>(rdd1.rightOuterJoin(rdd2).collect())</span><br><span class="line">    </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[(101, (&#x27;a&#x27;, &#x27;x&#x27;)), (103, (&#x27;c&#x27;, &#x27;y&#x27;))]</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[(104, (&#x27;d&#x27;, None)), (101, (&#x27;a&#x27;, &#x27;x&#x27;)), (102, (&#x27;b&#x27;, None)), (103, (&#x27;c&#x27;, &#x27;y&#x27;))]</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[(101, (&#x27;a&#x27;, &#x27;x&#x27;)), (103, (&#x27;c&#x27;, &#x27;y&#x27;))]</span></span><br></pre></td></tr></table></figure>



<h3 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h3><p>功能：求2个rdd的交集，返回一个新的rdd</p>
<p>用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.intersection(other_rdd)</span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd1 = sc.parallelize([(<span class="number">101</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="number">102</span>, <span class="string">&#x27;b&#x27;</span>), (<span class="number">103</span>, <span class="string">&#x27;c&#x27;</span>), (<span class="number">104</span>, <span class="string">&#x27;d&#x27;</span>)])</span><br><span class="line">    rdd2 = sc.parallelize([(<span class="number">101</span>, <span class="string">&#x27;x&#x27;</span>), (<span class="number">103</span>, <span class="string">&#x27;c&#x27;</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(rdd1.intersection(rdd2).collect())</span><br><span class="line">    </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[(103, &#x27;c&#x27;)]</span></span><br></pre></td></tr></table></figure>



<h3 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h3><p>功能：将rdd加上嵌套，这个嵌套按照分区来进行。比如rdd数据[1,2,3,4,5]有2个分区，那么，glom后，数据变成[[1,2,3], [4,5]]</p>
<p>用法：<code>rdd.glom()</code></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>], <span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(rdd.glom().collect())</span><br><span class="line">    </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[[1, 2], [3, 4], [5, 6, 7, 8]]</span></span><br></pre></td></tr></table></figure>



<h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><p>功能：针对KV型rdd，自动根据key分组</p>
<p>用法：<code>rdd.groupByKey()</code></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([(<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">5</span>)])</span><br><span class="line">    <span class="built_in">print</span>(rdd.groupByKey().<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], <span class="built_in">list</span>(x[<span class="number">1</span>]))).collect())</span><br><span class="line">  </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[(&#x27;b&#x27;, [2, 5]), (&#x27;c&#x27;, [3]), (&#x27;a&#x27;, [1, 4])]</span></span><br></pre></td></tr></table></figure>



<h3 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h3><p>功能：对rdd数据进行排序，基于你指定的排序依据</p>
<p>语法：</p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206152059608.png" alt="image-20221206152059608"></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([(<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">5</span>)])</span><br><span class="line">    <span class="built_in">print</span>(rdd.sortBy(<span class="keyword">lambda</span> x: x[<span class="number">1</span>], ascending=<span class="literal">True</span>, numPartitions=<span class="number">2</span>).collect())</span><br><span class="line">    <span class="built_in">print</span>(rdd.sortBy(<span class="keyword">lambda</span> x: x[<span class="number">1</span>], ascending=<span class="literal">False</span>, numPartitions=<span class="number">1</span>).collect())</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;c&#x27;, 3), (&#x27;a&#x27;, 4), (&#x27;b&#x27;, 5)]</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[(&#x27;b&#x27;, 5), (&#x27;a&#x27;, 4), (&#x27;c&#x27;, 3), (&#x27;b&#x27;, 2), (&#x27;a&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>



<h3 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h3><p>功能：针对KV型rdd，按照key进行排序</p>
<p>语法：</p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206152535926.png" alt="image-20221206152535926"></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;SPARK_HOME&#x27;</span>] = <span class="string">&#x27;/export/server/spark-3.2.0-bin-hadoop3.2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([(<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;e&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;D&#x27;</span>, <span class="number">8</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(rdd.sortByKey().collect()) <span class="comment">#默认升序排序</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果要确保全局有序，排序分区数要是1，如果不是1的话，只能确保各个分区类排好序，整体上不保证</span></span><br><span class="line">    <span class="built_in">print</span>(rdd.sortByKey(<span class="literal">True</span>, numPartitions=<span class="number">2</span>).collect())</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#对排序的key进行整理</span></span><br><span class="line">    <span class="built_in">print</span>(rdd.sortByKey(<span class="literal">True</span>, numPartitions=<span class="number">2</span>, keyfunc=<span class="keyword">lambda</span> key: <span class="built_in">str</span>(key).lower()).collect())</span><br><span class="line">    </span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[(&#x27;B&#x27;, 2), (&#x27;D&#x27;, 8), (&#x27;a&#x27;, 1), (&#x27;a&#x27;, 4), (&#x27;c&#x27;, 3), (&#x27;e&#x27;, 5)]</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[(&#x27;a&#x27;, 1), (&#x27;a&#x27;, 4), (&#x27;B&#x27;, 2), (&#x27;c&#x27;, 3), (&#x27;D&#x27;, 8), (&#x27;e&#x27;, 5)]</span></span><br></pre></td></tr></table></figure>



<h2 id="常用Action算子"><a href="#常用Action算子" class="headerlink" title="常用Action算子"></a>常用Action算子</h2><h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h3><p>功能：统计key出现的次数（一般适用于KV型的RDD）</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;SPARK_HOME&#x27;</span>] = <span class="string">&#x27;/export/server/spark-3.2.0-bin-hadoop3.2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd1 = sc.textFile(<span class="string">&quot;hdfs://node1:8020/input/words.txt&quot;</span>)</span><br><span class="line">    rdd2 = rdd1.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    rdd3 = rdd2.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(rdd3.countByKey())</span><br><span class="line">    </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;defaultdict(&lt;class &#x27;int&#x27;&gt;, &#123;&#x27;hello&#x27;: 3, &#x27;spark&#x27;: 1, &#x27;hadoop&#x27;: 1, &#x27;flink&#x27;: 1&#125;)</span></span><br></pre></td></tr></table></figure>



<h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><p>功能：将rdd各个分区内的数据，统一收集到driver中，形成一个List对象</p>
<p>注意：rdd是分布式对象，其数据量可以很大，所以在用这个算子之前，要了解结果数据集不会太大，不然会把Driver内存撑爆</p>
<p>用法：<code>rdd.collect()</code></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">    <span class="built_in">print</span>(rdd.collect())</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[1, 2, 3, 4, 5]</span></span><br></pre></td></tr></table></figure>



<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><p>功能：对rdd数据集按照你传入的逻辑进行聚合</p>
<p>语法：</p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206154632113.png" alt="image-20221206154632113"></p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206154653501.png" alt="image-20221206154653501"></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>))</span><br><span class="line">    <span class="built_in">print</span>(rdd.reduce(<span class="keyword">lambda</span> a, b: a + b))</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;45</span></span><br></pre></td></tr></table></figure>



<h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><p>功能：和reduce一样，接受传入逻辑进行聚合，聚合是带有初始值的</p>
<p>注意：这个初始值会作用在：分区内聚合、分区间聚合</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>), <span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(rdd.fold(<span class="number">10</span>, <span class="keyword">lambda</span> a, b: a + b))</span><br><span class="line"> </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;85</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#数据分布在3个分区 如[[1,2,3],[4,5,6],[7,8,9]]</span></span><br><span class="line"><span class="comment">#分区1，1,2,3聚合的时候带上10作为初始值得到16</span></span><br><span class="line"><span class="comment">#分区2，4,5,6聚合的时候带上10作为初始值得到25</span></span><br><span class="line"><span class="comment">#分区3，7,8,9聚合的时候带上10作为初始值得到34</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#3个分区结果聚合带上10作为初始值，结果：10 + 16 + 25 + 34 = 85</span></span><br></pre></td></tr></table></figure>



<h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><p>功能：取出rdd的第一个元素</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sc.parallelize(<span class="built_in">range</span>(<span class="number">3</span>, <span class="number">10</span>)).first())</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;3</span></span><br></pre></td></tr></table></figure>



<h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><p>功能：取出rdd的前N个元素，组合成List返回</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sc.parallelize(<span class="built_in">range</span>(<span class="number">3</span>, <span class="number">10</span>)).take(<span class="number">4</span>))</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[3, 4, 5, 6]</span></span><br></pre></td></tr></table></figure>



<h3 id="top"><a href="#top" class="headerlink" title="top"></a>top</h3><p>功能：对rdd数据集进行降序排序，取前N个</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sc.parallelize(<span class="built_in">range</span>(<span class="number">3</span>, <span class="number">10</span>)).top(<span class="number">4</span>))</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[9, 8, 7, 6]</span></span><br></pre></td></tr></table></figure>



<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>功能：计算rdd有多少条数据，返回值是一个数字</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sc.parallelize(<span class="built_in">range</span>(<span class="number">3</span>, <span class="number">10</span>)).count())</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;7</span></span><br></pre></td></tr></table></figure>



<h3 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample"></a>takeSample</h3><p>功能：随机抽样rdd的数据</p>
<p>用法：</p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206160645256.png" alt="image-20221206160645256"></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(rdd.takeSample(<span class="literal">True</span>, <span class="number">20</span>))</span><br><span class="line">    <span class="built_in">print</span>(rdd.takeSample(<span class="literal">False</span>, <span class="number">20</span>))</span><br><span class="line">    <span class="built_in">print</span>(rdd.takeSample(<span class="literal">False</span>, <span class="number">5</span>))</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[1, 4, 2, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1, 4]</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[1, 2, 1, 1, 1, 3, 5, 1, 4, 1, 4, 6]</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[1, 1, 1, 1, 2]</span></span><br></pre></td></tr></table></figure>



<h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h3><p>功能：对rdd进行排序取前N个</p>
<p>用法：</p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206161237241.png" alt="image-20221206161237241"></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">6</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(rdd.takeOrdered(<span class="number">3</span>))</span><br><span class="line">    <span class="built_in">print</span>(rdd.takeOrdered(<span class="number">3</span>, <span class="keyword">lambda</span> x: -x))</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[1, 2, 3]</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[9, 7, 6]</span></span><br></pre></td></tr></table></figure>



<h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><p>功能：对rdd的每一个元素，执行提供的逻辑操作，它没有返回值</p>
<p>用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd.foreach(func)</span><br><span class="line"><span class="comment"># func: (T) -&gt; None</span></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;SPARK_HOME&#x27;</span>] = <span class="string">&#x27;/export/server/spark-3.2.0-bin-hadoop3.2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>], <span class="number">1</span>)</span><br><span class="line">    rdd.foreach(<span class="keyword">lambda</span> x: <span class="built_in">print</span>(x * <span class="number">10</span>))</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;10</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;30</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;20</span></span><br></pre></td></tr></table></figure>



<h3 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h3><p>功能：将rdd的数据写入到文本文件中，支持本地文件、hdfs文件系统</p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;SPARK_HOME&#x27;</span>] = <span class="string">&#x27;/export/server/spark-3.2.0-bin-hadoop3.2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">100</span>))</span><br><span class="line">    rdd.saveAsTextFile(<span class="string">&quot;hdfs://node1:8020/output/range100&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206163035163.png" alt="image-20221206163035163"></p>
<p>注意：</p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206163152460.png" alt="image-20221206163152460"></p>
<p><strong>注意点</strong></p>
<p>foreach和saveAsTextFile算子是分区（Executor）直接执行的，跳过Driver，由分区所在的Executor直接执行，其余的Action算子都会将结果发送至Driver</p>
<h2 id="分区操作算子（Transformation-amp-Action）"><a href="#分区操作算子（Transformation-amp-Action）" class="headerlink" title="分区操作算子（Transformation &amp; Action）"></a>分区操作算子（Transformation &amp; Action）</h2><h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206165850734.png" alt="image-20221206165850734"></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;SPARK_HOME&#x27;</span>] = <span class="string">&#x27;/export/server/spark-3.2.0-bin-hadoop3.2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">6</span>], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">func_map</span>(<span class="params">data</span>):</span><br><span class="line">        num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">            num += i</span><br><span class="line">        <span class="keyword">return</span> [num]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(rdd.mapPartitions(func_map).collect())</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[4, 6, 22]</span></span><br></pre></td></tr></table></figure>



<h3 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h3><p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206171225266.png" alt="image-20221206171225266"></p>
<h3 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h3><p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206171333040.png" alt="image-20221206171333040"></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;SPARK_HOME&#x27;</span>] = <span class="string">&#x27;/export/server/spark-3.2.0-bin-hadoop3.2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([(<span class="string">&#x27;hadoop&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;spark&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;flink&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;hadoop&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;spark&#x27;</span>, <span class="number">1</span>)])</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">partition_self</span>(<span class="params">key</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;hadoop&#x27;</span> == key: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="string">&#x27;spark&#x27;</span> == key <span class="keyword">or</span> <span class="string">&#x27;flink&#x27;</span> == key):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">#分区号不要超标，设置3个分区，分区号只能是0，1，2</span></span><br><span class="line">    <span class="built_in">print</span>(rdd.partitionBy(<span class="number">3</span>, partition_self).glom().collect())</span><br><span class="line">  </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[[(&#x27;hadoop&#x27;, 1), (&#x27;hadoop&#x27;, 1)], [(&#x27;spark&#x27;, 1), (&#x27;flink&#x27;, 1), (&#x27;spark&#x27;, 1)], []]</span></span><br></pre></td></tr></table></figure>



<h3 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h3><p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206171957688.png" alt="image-20221206171957688"></p>
<h3 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h3><p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206172041561.png" alt="image-20221206172041561"></p>
<h3 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h3><p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206172122526.png" alt="image-20221206172122526"></p>
<h1 id="RDD的持久化"><a href="#RDD的持久化" class="headerlink" title="RDD的持久化"></a>RDD的持久化</h1><h2 id="RDD的数据是过程数据"><a href="#RDD的数据是过程数据" class="headerlink" title="RDD的数据是过程数据"></a>RDD的数据是过程数据</h2><p>RDD之间进行相互迭代计算（Transformation的转换），当执行开启后，新RDD的生成，代表老的RDD的消失。</p>
<p>RDD的数据是过程数据，只在处理的过程中存在，一旦处理完成，就不见了。</p>
<p>这个特性可以最大化的利用资源，老旧RDD没用了就从内存中清理，给后续的计算腾出内存空间。</p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206173712249.png" alt="image-20221206173712249"></p>
<p>如上图，rdd3被2次使用，第一次使用后，其实rdd3就不存在了。第二次用的时候，只能基于rdd的血缘关系，从rdd1重新执行，构建出rdd3，供rdd5使用。</p>
<h2 id="RDD缓存"><a href="#RDD缓存" class="headerlink" title="RDD缓存"></a>RDD缓存</h2><p>将指定的rdd数据保留在内存或硬盘上。</p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206174001016.png" alt="image-20221206174001016"></p>
<p><strong>缓存特点</strong></p>
<ul>
<li>缓存技术可以将过程RDD数据，持久化保存到内存或者硬盘上。</li>
<li>但是，这个保存在设定上认为是不安全的（存在丢失的风险）</li>
<li>缓存可以保留RDD之间的血缘（依赖）关系，一旦缓存丢失，可以基于血缘关系的记录，重新计算这个RDD的数据</li>
</ul>
<p>每个分区的缓存都会保存到其对应的Executor中的内存中，或者是对应的Executor所在服务器的硬盘上。</p>
<p><strong>缓存是如何保存</strong></p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206174850915.png" alt="image-20221206174850915"></p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206174914952.png" alt="image-20221206174914952"></p>
<h2 id="RDD-CheckPoint"><a href="#RDD-CheckPoint" class="headerlink" title="RDD CheckPoint"></a>RDD CheckPoint</h2><p>将RDD数据保存起来，<strong>它仅支持硬盘存储</strong>，它被设计认为是安全的。它不保留血缘关系。</p>
<p><strong>CheckPoint是如何保存数据的</strong></p>
<p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206174751409.png" alt="image-20221206174751409"></p>
<h3 id="缓存和CheckPoint的对比"><a href="#缓存和CheckPoint的对比" class="headerlink" title="缓存和CheckPoint的对比"></a>缓存和CheckPoint的对比</h3><p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206175054890.png" alt="image-20221206175054890"></p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第一步，选择CP的保存路径</span></span><br><span class="line"><span class="comment">#如果是Local模式，可以支持本地文件系统，如果在集群运行，千万要用HDFS</span></span><br><span class="line">sc.setCheckpointDir(<span class="string">&quot;hdfs://node1:8020/output/bj52ckp&quot;</span>)</span><br><span class="line"><span class="comment">#用的时候，直接调用checkoutpoint算子即可</span></span><br><span class="line">rdd.checkpoint()</span><br></pre></td></tr></table></figure>



<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>CheckPoint是一种重量级的使用，也就是RDD的计算成本很高的时候，采用CheckPoint比较合适。</p>
<p>或者数据量很大，用CheckPoint比较合适。</p>
<p>如果数据量小，或者RDD重新计算是非常块的，用CheckPoint没啥必要，直接缓存即可。</p>
<ul>
<li>Cache和CheckPoint两个API都不是Action类型</li>
<li>所以，要它两工作，必须在后面接上Action</li>
<li>接上Action的目的，是让RDD有数据，而不是为了让CheckPoint和Cache工作</li>
</ul>
<h1 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h1><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p>代码中存在本地对象时，会被发送到每个分区的处理线程上使用，也就是一个Executor内，其实存放了2份一样的数据。Executor是进程，进程内的资源共享，进程内保存了两份一样的数据，操作内存资源浪费。</p>
<p>如果将本地资源标记为广播变量对象，Spark只会给每个Executor发送一份数据，Executor内部的各个线程（分区）共享这份数据，而不是每个分区的处理线程都发送一份，从而节省了内存。</p>
<p>使用方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.将本地list标记成广播变量即可</span></span><br><span class="line">broadcast = sc.broadcast(<span class="built_in">list</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.使用广播变量，从broadcast对象中取出本地list对象即可</span></span><br><span class="line">value = broadcast.value</span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    stu_info_list = [(<span class="number">1</span>, <span class="string">&#x27;张三&#x27;</span>, <span class="number">11</span>), (<span class="number">2</span>, <span class="string">&#x27;李四&#x27;</span>, <span class="number">13</span>), (<span class="number">3</span>, <span class="string">&#x27;王五&#x27;</span>, <span class="number">11</span>), (<span class="number">4</span>, <span class="string">&#x27;朱六&#x27;</span>, <span class="number">11</span>)]</span><br><span class="line"></span><br><span class="line">    broadcast = sc.broadcast(stu_info_list)</span><br><span class="line"></span><br><span class="line">    score_info_rdd = sc.parallelize([</span><br><span class="line">        (<span class="number">1</span>, <span class="string">&#x27;语文&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="string">&#x27;数学&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="string">&#x27;外语&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="string">&#x27;自然&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">1</span>, <span class="string">&#x27;化学&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="string">&#x27;物理&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="string">&#x27;政治&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="string">&#x27;俄语&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">1</span>, <span class="string">&#x27;日语&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="string">&#x27;编程&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="string">&#x27;生物&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="string">&#x27;历史&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">map_func</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="built_in">id</span> = data[<span class="number">0</span>]</span><br><span class="line">        name = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        value = broadcast.value</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> value:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">id</span> == i[<span class="number">0</span>]:</span><br><span class="line">                name = i[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> (name, data[<span class="number">1</span>], data[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(score_info_rdd.<span class="built_in">map</span>(map_func).collect())</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;[(&#x27;张三&#x27;, &#x27;语文&#x27;, 99), (&#x27;李四&#x27;, &#x27;数学&#x27;, 99), (&#x27;王五&#x27;, &#x27;外语&#x27;, 99), (&#x27;朱六&#x27;, &#x27;自然&#x27;, 99), (&#x27;张三&#x27;, &#x27;化学&#x27;, 99), (&#x27;李四&#x27;, &#x27;物理&#x27;, 99), (&#x27;王五&#x27;, &#x27;政治&#x27;, 99), (&#x27;朱六&#x27;, &#x27;俄语&#x27;, 99), (&#x27;张三&#x27;, &#x27;日语&#x27;, 99), (&#x27;李四&#x27;, &#x27;编程&#x27;, 99), (&#x27;王五&#x27;, &#x27;生物&#x27;, 99), (&#x27;朱六&#x27;, &#x27;历史&#x27;, 99)]</span></span><br></pre></td></tr></table></figure>



<h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p>对map算子计算中的数据，进行计数累加得到全部数据计算后的累加结果。</p>
<p><strong>没有累加器的示例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">map_func</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="keyword">global</span> count</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(count)</span><br><span class="line"></span><br><span class="line">    rdd.<span class="built_in">map</span>(map_func).collect()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;最终结果&#x27;</span>,count)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;1</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;2</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;3</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;1</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;2</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;3</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;1</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;2</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;3</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;最终结果 0</span></span><br></pre></td></tr></table></figure>

<p>上述代码中存在的问题：</p>
<blockquote>
<p>count来自Driver对象，当在分布式的map算子中需要count对象的时候</p>
<p>Driver会将count发送给每一个Executor一份（复制发送）</p>
<p>最后打印count时，被打印的count是Driver中的那个，所以，不管Executor中累加到多少，都和Driver中的count无关</p>
</blockquote>
<p><strong>累加器</strong></p>
<p>累加器对象可以从各个Executor中收集它们的执行结果，作用会自己身上。</p>
<p>用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.accumulator(初始值)</span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">&quot;study_00&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    count = sc.accumulator(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">map_func</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="keyword">global</span> count</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(count)</span><br><span class="line"></span><br><span class="line">    rdd.<span class="built_in">map</span>(map_func).collect()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;最终结果&#x27;</span>,count)</span><br><span class="line">   </span><br><span class="line"><span class="comment">#&gt;&gt;&gt;输出</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;1</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;2</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;3</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;1</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;2</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;3</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;1</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;2</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;3</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt;最终结果 9</span></span><br></pre></td></tr></table></figure>

<p><strong>注意事项</strong></p>
<p>如果累加器计数的代码，存在重新构建的步骤中，累加器计数的代码可能被多次执行。</p>
<h1 id="提交到YARN集群中运行"><a href="#提交到YARN集群中运行" class="headerlink" title="提交到YARN集群中运行"></a>提交到YARN集群中运行</h1><p><img src="/2022/02/05/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/image-20221206153551743.png" alt="image-20221206153551743"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/02/01/Spark%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">Spark基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-01 21:07:40" itemprop="dateCreated datePublished" datetime="2022-02-01T21:07:40+08:00">2022-02-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Spark框架概述"><a href="#Spark框架概述" class="headerlink" title="Spark框架概述"></a>Spark框架概述</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>Apache Spark是用于大规模数据处理的统一分析引擎。</p>
<p>RDD：弹性分布式数据集。RDD是一种分布式内存抽象，其使得程序员能够在大规模集群中做内存运算，并且有一定的容错方式。而这也是整个Spark的核心数据结构，Spark整个平台都围绕着RDD进行。</p>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221208221426265.png" alt="image-20221208221426265"></p>
<p>Spark的特点是对任意类型的数据进行自定义计算。它可以计算结构化、半结构化、非结构化等各种类型的数据结构。同时也支持使用Python、Java、Scala、R及SQL语言去开发应用程序计算数据。</p>
<h2 id="Spark-VS-Hadoop-MapReduce"><a href="#Spark-VS-Hadoop-MapReduce" class="headerlink" title="Spark VS Hadoop(MapReduce)"></a>Spark VS Hadoop(MapReduce)</h2><p>Spark和Hadoop技术栈的区别：</p>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221208221847274.png" alt="image-20221208221847274"></p>
<p>尽管Spark相对Hadoop具有较大优势，但Spark并不能完全替代Hadoop。</p>
<ul>
<li>在计算层面，Spark相比较MapReduce有巨大的性能优势，但至今仍有许多计算工具基于MapReduce架构，比如非常成熟的Hive</li>
<li>Spark仅做计算，而Hadoop生态圈不仅有计算（MapReduce），也有存储（HDFS）和资源管理调度（YARN），HDFS和YARN仍是许多大数据体系的核心架构。</li>
</ul>
<h2 id="Spark四大特点"><a href="#Spark四大特点" class="headerlink" title="Spark四大特点"></a>Spark四大特点</h2><h3 id="速度快"><a href="#速度快" class="headerlink" title="速度快"></a>速度快</h3><p>Spark支持内存计算，并且通过DAG（有向无环图）执行引擎支持无环数据流，其在内存中运行速度比MapReduce快（官方宣称比MapReduce快100倍，在硬盘中快10倍）。</p>
<p>Spark处理数据与MapReduce处理数据相比，有两个不同点：</p>
<ul>
<li>Spark处理数据时，可以将中间处理结果存储到内存中。</li>
<li>Spark提供了非常丰富的算子（API），可以做到复杂任务在一个Spark程序中完成。</li>
</ul>
<h3 id="易于使用"><a href="#易于使用" class="headerlink" title="易于使用"></a>易于使用</h3><p>Spark支持使用Python、Java、Scala、R及SQL语言去开发应用程序</p>
<h3 id="通用性强"><a href="#通用性强" class="headerlink" title="通用性强"></a>通用性强</h3><p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221208223515333.png" alt="image-20221208223515333"></p>
<p>可以在一个应用中无缝地使用这些工具库。</p>
<h3 id="运行方式"><a href="#运行方式" class="headerlink" title="运行方式"></a>运行方式</h3><p>Spark支持多种运行方式。包括Hadoop和Mesos，也支持Standalone的独立运行模式，也可以运行在云Kubernetes上。</p>
<p>对于数据源，Spark支持HDFS、HBase、Cassandra及Kafka等。</p>
<h2 id="Spark框架模块"><a href="#Spark框架模块" class="headerlink" title="Spark框架模块"></a>Spark框架模块</h2><p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221208223922127.png" alt="image-20221208223922127"></p>
<p><strong>Spark Core</strong></p>
<p>Spark的核心，提供Spark核心功能，是Spark运行的基础。Spark Core以RDD为数据抽象，提供Python、Java、Scala、R语言的API，可以编程进行海量离线数据批处理计算。</p>
<p><strong>SparkSQL</strong></p>
<p>基于SparkCore之上，提供结构化数据的处理模块，SparkSQL支持以SQL语言对数据进行处理，SparkSQL本身针对离线计算场景。同时基于SparkSQL，Spark提供了StructedStreaming模块，进行数据流式计算。</p>
<p><strong>SparkStreaming</strong></p>
<p>以SparkCore为基础，提供数据流式计算的功能。</p>
<p><strong>MLlib</strong></p>
<p>以SparkCore为基础，进行机器学习，内置大量机器学习卡和API算法。</p>
<p><strong>GraphX</strong></p>
<p>以SparkCore为基础，进行图计算</p>
<h2 id="Spark运行模式"><a href="#Spark运行模式" class="headerlink" title="Spark运行模式"></a>Spark运行模式</h2><p><strong>本地模式（单机）</strong></p>
<p>以一个独立的进程，通过其内部的多个线程来模拟整个Spark运行环境</p>
<p><strong>Standalone模式（集群）</strong></p>
<p>Spark中各个角色以独立进程的形式存在，并组成Spark集群环境</p>
<p><strong>Hadoop YARN模式（集群）</strong></p>
<p>Spark中的各个角色运行在YARN的容器内部，并组成Spark集群环境</p>
<p><strong>Kubernetes模式（容器集群）</strong></p>
<p>Spark中的各个角色运行在Kubernetes的容器内部，并组成Spark集群环境</p>
<h3 id="Spark的架构角色"><a href="#Spark的架构角色" class="headerlink" title="Spark的架构角色"></a>Spark的架构角色</h3><h4 id="YARN角色"><a href="#YARN角色" class="headerlink" title="YARN角色"></a>YARN角色</h4><p>资源管理层面：</p>
<ul>
<li>集群资源管理者（Master）：ResourceManager</li>
<li>单机资源管理者（Worker）：NodeManager</li>
</ul>
<p>任务计算层面：</p>
<ul>
<li>单任务管理者（Master）：ApplicationMaster</li>
<li>单任务执行者（Worder）：Task（容器内计算框架的工作角色）</li>
</ul>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221208225238293.png" alt="image-20221208225238293"></p>
<h4 id="Spark运行角色"><a href="#Spark运行角色" class="headerlink" title="Spark运行角色"></a>Spark运行角色</h4><p>资源管理层面：</p>
<ul>
<li>管理者：Master角色（管理整个集群的资源），YARN是ResourceManager</li>
<li>工作者：Worker角色（管理单个服务器的资源），YARN是NodeManager</li>
</ul>
<p>任务执行层面</p>
<ul>
<li>某任务管理者：Driver角色（管理单个任务在运行的时候的工作），YARN是ApplicationMaster</li>
<li>某任务执行者：Executor角色（单个任务运行的时候的一堆工作者，干活的），YARN是容器中运行的具体工作进程（TASK）</li>
</ul>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221208225924296.png" alt="image-20221208225924296"></p>
<h1 id="Spark部署方式"><a href="#Spark部署方式" class="headerlink" title="Spark部署方式"></a>Spark部署方式</h1><h2 id="Local"><a href="#Local" class="headerlink" title="Local"></a>Local</h2><p>Local模式是以一个独立进程配合其内部线程来提供完成Spark运行时环境。Local模式可以通过spark-shell&#x2F;pyspark&#x2F;spark-submit等来开启。</p>
<h2 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h2><h3 id="Standalone架构"><a href="#Standalone架构" class="headerlink" title="Standalone架构"></a>Standalone架构</h3><p>Standalone模式是Sark自带的一种集群模式，不同于本地模式启动多个进程来模拟集群环境，Standalone模式是真实的在多个机器之间搭建Spark集群的环境，完全可以利用该模式搭建多机器机器，用于实际的大数据处理。</p>
<p>Standalone是完整的Spark运行环境，其中Master角色以Master进程存在，Worker角色以Worker进程存在。Driver和Executor运行于Worker进程内，由Worker提供资源供给它们运行。</p>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210151603188.png" alt="image-20221210151603188"></p>
<p>Standalone集群在进程上主要有3类进程：</p>
<ol>
<li><p>主节点Master进程</p>
<p>Master角色，管理整个集群资源，并托管运行各个任务的Driver。</p>
</li>
<li><p>从节点Workers</p>
<p>Worker角色，管理每个集群的资源，分配对应的资源来运行Executor（TASK）；每个从节点分配资源信息给Worker管理，资源信息包含Memory和CPU Cores核数</p>
</li>
<li><p>历史服务器HistoryServer（可选）</p>
<p>Spark Application运行完成后，保存事件日志数据至HDFS，启动HistoryServer可以查看应用运行相关信息</p>
</li>
</ol>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210152323860.png" alt="image-20221210152323860"></p>
<h3 id="Spark应用架构"><a href="#Spark应用架构" class="headerlink" title="Spark应用架构"></a>Spark应用架构</h3><p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210155620073.png" alt="image-20221210155620073"></p>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210155700797.png" alt="image-20221210155700797"></p>
<p>从上图可以看出Spark Application运行到集群上时，由两部分组成：Driver Program和Excutors。</p>
<p><strong>Driver Program</strong></p>
<p>相当于AppMaster，整个应用的管理者，负责应用中所有Job的调度执行。</p>
<p>运行JVM Process，运行程序的main函数，必须建立SparkContext上下文对象。</p>
<p>一个SparkApplication仅有一个。</p>
<p><strong>Executors</strong></p>
<p>相当于一个线程池，运行JVM Process，其中有很多线程，每个线程运行一个Task任务，一个Task任务运行需要1 core CPU，所以可以认为Executor中线程数就等于CPU core核数。</p>
<p>一个Spark Application中可以有多个Executors，可以设置个数和资源信息。</p>
<p><strong>执行流程</strong></p>
<p>用户程序从最开始的提交到最终的计算执行，需要经历一下几个阶段：</p>
<ol>
<li>用户程序创建SparkContext时，新创建的SparkContext实例会连接到ClusterManager。 ClusterManager会根据用户提交时设置的CPU和内存等信息为本次提交分配计算资源，启动Executor。</li>
<li>Driver会将用户程序划分为不同的执行阶段Stage，每个执行阶段Stage由一组完全相同的Task组成，这些Task分别作用于待处理数据的不同分区。在接到划分完成和Task创建后，Driver会向Executor发送Task。</li>
<li>Executor在接收到Task后，会下载Task的运行时依赖，在准备好Task的执行环境后，会开始执行Task，并且将Task的运行状态汇报给Driver。</li>
<li>Driver会根据收到的Task的运行状态来处理不同的状态更新。Task分两种：一种时Shuffle Map Task，它实现数据的重新洗牌，洗牌的结果保存到Executor所在节点的文件系统中；另外一种是Result Task，它负责生成结果数据。</li>
<li>Driver会不断地调用Task，将Task发送到Executor执行，在所有地Task都正确执行或者超过执行次数地限制仍然没有执行成功时停止。</li>
</ol>
<p><strong>Spark程序运行层次结构</strong></p>
<p>Spark Application程序运行时三个核心概念：Job、Stage、Task。</p>
<ul>
<li><strong>Job</strong> ：由多个Task地并行计算部分，一般Spark中的action操作（如save、collect）会生成一个Job</li>
<li><strong>Stage</strong> ：Job的组成单位，一个Job会切分成多个Stage，Stage彼此之间互相依赖顺序执行，而每个Stage是多个Task的集合，类似map和reduce stage</li>
<li><strong>Task</strong> ：被分配到Executor的单位工作内容，它是Spark中的最小执行单位，一般来说有多少个paritition，就会有多少个Task，就会有多少个Task，每个Task只会处理单一分支上的数据。</li>
</ul>
<blockquote>
<p>一个Spark Application中，包含多个Job，每个Job有多个Stage组成，每个Job的执行安装DAG图进行的。其中每个Satge中包含多个Task任务，每个Task以线程方式执行。</p>
</blockquote>
<p><strong>端口</strong></p>
<ul>
<li>4040：是一个运行的Application在运行的过程中临时绑定的端口，用以查看当前的任务状态。4040被占用会顺延到4041、4042等。4040是一个临时端口，当程序运行完成后，4040就会被注销。</li>
<li>8080：默认时Standalone下，Master角色的web端口，用以查看当前Master（集群）的状态</li>
<li>18080：默认时历史服务器的端口，由于每个程序运行完成后，4040端口就被注销了，可以通过历史服务费回看某个程序的运行状态。</li>
</ul>
<h2 id="Standalone-HA"><a href="#Standalone-HA" class="headerlink" title="Standalone HA"></a>Standalone HA</h2><p>基于Zookeeper实现HA</p>
<p>Zookeeer提供了一个Leader Election机制，利用这个机制可以保证虽然集群存在多个Master，但是只有一个是Active，其他的都是Standby。当Active的Master出现故障时，另外的一个Standy Master会被选举出来。由于集群的信息，及woker、driver和Application的信息都已经持久化到文件系统，因此在切换的过程中只会影响新Job的提交，对于正在进行的Job没有任何影响。</p>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210163520123.png" alt="image-20221210163520123"></p>
<h2 id="Spark-On-YARN"><a href="#Spark-On-YARN" class="headerlink" title="Spark On YARN"></a>Spark On YARN</h2><p>多数情况下，各企业都会有Haddop集群，为了节约服务器资源，将Spark运行到YARN集群中，不需要再单独部署Spark Standalone集群。</p>
<p>在Spark on YARN中，无需部署Spark集群，只要一个服务器充当Spark客户端，即可提交任务到YARN集群中运行。</p>
<h3 id="Spark-On-YARN的本质"><a href="#Spark-On-YARN的本质" class="headerlink" title="Spark On YARN的本质"></a>Spark On YARN的本质</h3><p>Master角色由YARN的ResourceManager担任。</p>
<p>Worker角色由YARN的NodeManager担任。</p>
<p>Driver角色运行在YARN容器内或提交任务的客户端进程中。</p>
<p>真正干活的Executor运行在YARN提供的容器内。</p>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210172323763.png" alt="image-20221210172323763"></p>
<h3 id="部署模式DeloyMode"><a href="#部署模式DeloyMode" class="headerlink" title="部署模式DeloyMode"></a>部署模式DeloyMode</h3><p><strong>Cluster模式</strong></p>
<p>Driver运行在YARN容器内部，和ApplicationMaster在同一个容器内</p>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210180238773.png" alt="image-20221210180238773"></p>
<p>详细流程：</p>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210182646551.png" alt="image-20221210182646551"></p>
<p>具体流程步骤：</p>
<ol>
<li>任务提交后会和ResourceManager通讯申请启动ApplicationMaster；</li>
<li>随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver；</li>
<li>Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后在合适的NodeManager上启动Executor进程；</li>
<li>Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数；</li>
<li>之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生成对应的TaskSet，之后将Task分发到各个Executor上执行</li>
</ol>
<p><strong>Client模式</strong></p>
<p>Driver运行在客户端进程中，比如Driver运行在spark-submit程序的进程中</p>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210180341609.png" alt="image-20221210180341609"></p>
<p>详细流程：</p>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210181813615.png" alt="image-20221210181813615"></p>
<p>具体流程步骤：</p>
<ol>
<li>Driver在任务提交的本地机器上运行，Driver启动后会和ResourceManager通讯申请启动ApplicationMaster；</li>
<li>随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster的功能相当于一个ExecutorLaucher，只负责向ResourceManager申请Executor内存；</li>
<li>ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程；</li>
<li>Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数；</li>
<li>之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生成对应的TaskSet，之后将Task分发到各个Executor上执行</li>
</ol>
<p><strong>两种模式的区别</strong></p>
<img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210180524025.png" alt="image-20221210180524025" style="zoom:80%;">



<p><strong>使用方式</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HOME = /export/server/spark</span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn -- deploy-mode client|cluster test.py</span></span><br></pre></td></tr></table></figure>





<h1 id="PySpark"><a href="#PySpark" class="headerlink" title="PySpark"></a>PySpark</h1><p>PySpark是Spark官方提供的一个Python类库，内置了完全的Spark API，可以通过PySpark类库来编写Spark应用程序，并将其提交到Spark集群中运行。</p>
<p>Python On Spark执行原理</p>
<p>PySark宗旨是在不破坏Spark已有的运行时架构，在Spark架构外层包装一层Python API，借助Py4j实现Python和Java的交互，进而实现通过Python编写Spark应用程序，其运行时架构如图所示。</p>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210202831245.png" alt="image-20221210202831245"></p>
<p><img src="/2022/02/01/Spark%E5%9F%BA%E7%A1%80/image-20221210202906475.png" alt="image-20221210202906475"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/01/13/%E5%81%9A%E4%B8%80%E4%B8%AA%E6%9C%89%E6%B8%A9%E5%BA%A6%E6%9C%89%E6%9D%A1%E7%90%86%E7%9A%84%E8%A1%A8%E8%BE%BE%E8%80%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/13/%E5%81%9A%E4%B8%80%E4%B8%AA%E6%9C%89%E6%B8%A9%E5%BA%A6%E6%9C%89%E6%9D%A1%E7%90%86%E7%9A%84%E8%A1%A8%E8%BE%BE%E8%80%85/" class="post-title-link" itemprop="url">做一个有温度有条理的表达者</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-13 23:12:46" itemprop="dateCreated datePublished" datetime="2022-01-13T23:12:46+08:00">2022-01-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="为什么讲故事很重要"><a href="#为什么讲故事很重要" class="headerlink" title="为什么讲故事很重要"></a>为什么讲故事很重要</h1><p>输出是掌握别人传授干货的重要一环。【费曼学习法】<br>故事一直是传播信息的最重要的载体没有之一。</p>
<h1 id="如何讲好一个故事"><a href="#如何讲好一个故事" class="headerlink" title="如何讲好一个故事"></a>如何讲好一个故事</h1><p>结构决定下限，其他(升华等)决定上限</p>
<ol>
<li>启(简单交代背景)</li>
<li>承(引发共鸣)</li>
<li>转(制造冲突，加强吸引注意。紧接反转，继续引发情绪共鸣)</li>
<li>合(闭合式或开放式结局)</li>
<li>升华(引出道理)</li>
</ol>
<h1 id="如何讲好一个道理"><a href="#如何讲好一个道理" class="headerlink" title="如何讲好一个道理"></a>如何讲好一个道理</h1><p>金字塔原理&#x2F;论证类比法</p>
<img src="/2022/01/13/%E5%81%9A%E4%B8%80%E4%B8%AA%E6%9C%89%E6%B8%A9%E5%BA%A6%E6%9C%89%E6%9D%A1%E7%90%86%E7%9A%84%E8%A1%A8%E8%BE%BE%E8%80%85/image-20230219231456527.png" alt="image-20230219231456527" style="zoom:67%;">

<ol>
<li>结论先行(引起疑惑和兴趣)</li>
<li>以上统下(从主论点引出多个分论点，分论点支撑主论点的成立)</li>
<li>管理分组(归类整理)</li>
<li>逻辑递进(控制分论点&#x2F;分组的逻辑关系，让听者不掉队)</li>
</ol>
<h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>借故事讲道理是传达思想最有效的方式。<br>讲好故事&#x2F;讲好道理&#x3D;超越80%地人。<br>讲好故事+讲好道理+有趣&#x3D;完美</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/01/12/Azkaban%E8%BF%9B%E9%98%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/12/Azkaban%E8%BF%9B%E9%98%B6/" class="post-title-link" itemprop="url">Azkaban进阶</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-12 22:02:03" itemprop="dateCreated datePublished" datetime="2022-01-12T22:02:03+08:00">2022-01-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="JavaProcess-作业类型案例"><a href="#JavaProcess-作业类型案例" class="headerlink" title="JavaProcess 作业类型案例"></a>JavaProcess 作业类型案例</h1><p>JavaProcess 类型可以运行一个自定义主类方法，type 类型为 javaprocess，可用的配置为：</p>
<blockquote>
<p>Xms：最小堆</p>
<p>Xmx：最大堆</p>
<p>classpath：类路径</p>
<p>java.class：要运行的 Java 对象，其中必须包含 Main 方法</p>
<p>main.args：main 方法的参数</p>
</blockquote>
<p>案例：</p>
<ol>
<li><p>新建一个 azkaban 的 maven 工程</p>
</li>
<li><p>创建包名：com.st</p>
</li>
<li><p>创建 AzTest 类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.st;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AzTest</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;This is for testing&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>打包成 jar 包 azkaban-1.0-SNAPSHOT.jar</p>
</li>
<li><p>新建 testJava.flow，内容如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: test_java</span><br><span class="line">    type: javaprocess</span><br><span class="line">    config:</span><br><span class="line">      Xms: 96M</span><br><span class="line">      Xmx: 200M</span><br><span class="line">      java.class: com.st.AzTest</span><br></pre></td></tr></table></figure>


</li>
<li><p>将 Jar 包、flow 文件和 project 文件打包成 javatest.zip</p>
</li>
<li><p>创建项目&#x3D;》上传 javatest.zip &#x3D;》执行作业&#x3D;》观察结果</p>
</li>
</ol>
<h1 id="条件工作流案例"><a href="#条件工作流案例" class="headerlink" title="条件工作流案例"></a>条件工作流案例</h1><p>条件工作流功能允许用户自定义执行条件来决定是否运行某些Job。条件可以由当前Job的父 Job 输出的运行时参数构成，也可以使用预定义宏。在这些条件下，用户可以在确定 Job执行逻辑时获得更大的灵活性，例如，只要父 Job 之一成功，就可以运行当前 Job。</p>
<h2 id="运行时参数案例"><a href="#运行时参数案例" class="headerlink" title="运行时参数案例"></a>运行时参数案例</h2><ol>
<li><p>基本原理<br> （1）父 Job 将参数写入 JOB_OUTPUT_PROP_FILE 环境变量所指向的文件<br> （2）子 Job 使用 ${jobName:param}来获取父 Job 输出的参数并定义执行条件</p>
</li>
<li><p>支持的条件运算符：<br> （1）&#x3D;&#x3D; 等于<br> （2）!&#x3D; 不等于<br> （3）&gt; 大于<br> （4）&gt;&#x3D; 大于等于<br> （5）&lt; 小于<br> （6）&lt;&#x3D; 小于等于<br> （7）&amp;&amp; 与<br> （8）|| 或<br> （9）! 非</p>
</li>
<li><p>案例：</p>
</li>
</ol>
<p>  需求：</p>
<p>  JobA 执行一个 shell 脚本。</p>
<p>  JobB 执行一个 shell 脚本，但 JobB 不需要每天都执行，而只需要每个周一执行。</p>
<p>  （1）新建 JobA.sh</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">echo &quot;do JobA&quot;</span><br><span class="line">wk=`date +%w`</span><br><span class="line">echo &quot;&#123;\&quot;wk\&quot;:$wk&#125;&quot; &gt; $JOB_OUTPUT_PROP_FILE</span><br></pre></td></tr></table></figure>

<p>  （2）新建 JobB.sh</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">echo &quot;do JobB&quot;</span><br></pre></td></tr></table></figure>

<p>  （3）新建 condition.flow</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">- name: JobA</span><br><span class="line"> type: command</span><br><span class="line"> config:</span><br><span class="line"> command: sh JobA.sh</span><br><span class="line">- name: JobB</span><br><span class="line"> type: command</span><br><span class="line"> dependsOn:</span><br><span class="line"> - JobA</span><br><span class="line"> config:</span><br><span class="line"> command: sh JobB.sh</span><br><span class="line"> condition: $&#123;JobA:wk&#125; == 1</span><br></pre></td></tr></table></figure>

<p>  （4）将 JobA.sh、JobB.sh、condition.flow 和 azkaban.project 打包成 condition.zip<br>  （5）创建 condition 项目&#x3D;》上传 condition.zip 文件&#x3D;》执行作业&#x3D;》观察结果<br>  （6）按照我们设定的条件，JobB 会根据当日日期决定是否执行。</p>
<p>  <img src="/2022/01/12/Azkaban%E8%BF%9B%E9%98%B6/image-20230227233252168.png" alt="image-20230227233252168"></p>
<h2 id="预定义宏案例"><a href="#预定义宏案例" class="headerlink" title="预定义宏案例"></a>预定义宏案例</h2><p>Azkaban 中预置了几个特殊的判断条件，称为预定义宏。<br>预定义宏会根据所有父 Job 的完成情况进行判断，再决定是否执行。可用的预定义宏如<br>下：<br>（1）all_success: 表示父 Job 全部成功才执行(默认)<br>（2）all_done：表示父 Job 全部完成才执行<br>（3）all_failed：表示父 Job 全部失败才执行<br>（4）one_success：表示父 Job 至少一个成功才执行<br>（5）one_failed：表示父 Job 至少一个失败才执行</p>
<p>案例<br>需求：<br>JobA 执行一个 shell 脚本<br>JobB 执行一个 shell 脚本<br>JobC 执行一个 shell 脚本，要求 JobA、JobB 中有一个成功即可执行<br>（1）新建 JobA.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">echo &quot;do JobA&quot;</span><br></pre></td></tr></table></figure>
<p>（2）新建 JobC.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">echo &quot;do JobC&quot;</span><br></pre></td></tr></table></figure>
<p>（3）新建 macro.flow</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">- name: JobA</span><br><span class="line"> type: command</span><br><span class="line"> config:</span><br><span class="line"> command: sh JobA.sh</span><br><span class="line">- name: JobB</span><br><span class="line"> type: command</span><br><span class="line"> config:</span><br><span class="line"> command: sh JobB.sh</span><br><span class="line">- name: JobC</span><br><span class="line"> type: command</span><br><span class="line"> dependsOn:</span><br><span class="line"> - JobA</span><br><span class="line"> - JobB</span><br><span class="line"> config:</span><br><span class="line"> command: sh JobC.sh</span><br><span class="line"> condition: one_success</span><br></pre></td></tr></table></figure>
<p>（4）JobA.sh、JobC.sh、macro.flow、azkaban.project 文件，打包成 macro.zip。<br>注意：没有 JobB.sh。<br>（5）创建 macro 项目&#x3D;》上传 macro.zip 文件&#x3D;》执行作业&#x3D;》观察结果</p>
<p><img src="/2022/01/12/Azkaban%E8%BF%9B%E9%98%B6/image-20230227233228872.png" alt="image-20230227233228872"></p>
<h1 id="定时执行案例"><a href="#定时执行案例" class="headerlink" title="定时执行案例"></a>定时执行案例</h1><p>需求：JobA 每间隔 1 分钟执行一次；</p>
<p>具体步骤：</p>
<p>1）Azkaban 可以定时执行工作流。在执行工作流时候，选择左下角 Schedule</p>
<p><img src="/2022/01/12/Azkaban%E8%BF%9B%E9%98%B6/image-20230228210029040.png" alt="image-20230228210029040"></p>
<p>2）右上角注意时区是上海，然后在左面填写具体执行事件，填写的方法和 crontab 配置定时任务规则一致。</p>
<p><img src="/2022/01/12/Azkaban%E8%BF%9B%E9%98%B6/image-20230228210157551.png" alt="image-20230228210157551"></p>
<p><img src="/2022/01/12/Azkaban%E8%BF%9B%E9%98%B6/image-20230228210241189.png" alt="image-20230228210241189"></p>
<p>3）观察结果</p>
<p><img src="/2022/01/12/Azkaban%E8%BF%9B%E9%98%B6/image-20230228210331444.png" alt="image-20230228210331444"></p>
<p><img src="/2022/01/12/Azkaban%E8%BF%9B%E9%98%B6/image-20230228210359458.png" alt="image-20230228210359458"></p>
<p>4）删除定时调度</p>
<p>点击 remove Schedule 即可删除当前任务的调度规则。</p>
<p><img src="/2022/01/12/Azkaban%E8%BF%9B%E9%98%B6/image-20230228210435344.png" alt="image-20230228210435344"></p>
<h1 id="Azkaban-多-Executor-模式注意事项"><a href="#Azkaban-多-Executor-模式注意事项" class="headerlink" title="Azkaban 多 Executor 模式注意事项"></a>Azkaban 多 Executor 模式注意事项</h1><p>Azkaban 多 Executor 模式是指，在集群中多个节点部署 Executor。在这种模式下，<br>Azkaban web Server 会根据策略，选取其中一个 Executor 去执行任务。<br>为确保所选的 Executor 能够准确的执行任务，我们须在以下两种方案任选其一，推荐使<br>用方案二。<br>方案一：指定特定的 Executor（node1）去执行任务。<br>1）在 MySQL 中 azkaban 数据库 executors 表中，查询 node1 上的 Executor 的 id。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use azkaban;</span><br><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; select * from executors;</span><br><span class="line">+----+-----------+-------+--------+</span><br><span class="line">| id | host | port | active |</span><br><span class="line">+----+-----------+-------+--------+</span><br><span class="line">| 1 | node1 | 12321 | 1 |</span><br><span class="line">| 2 | node2 | 12321 | 1 |</span><br><span class="line">| 3 | node3 | 12321 | 1 |</span><br><span class="line">+----+-----------+-------+--------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>
<p>2）在执行工作流程时加入 useExecutor 属性，如下</p>
<p><img src="/2022/01/12/Azkaban%E8%BF%9B%E9%98%B6/image-20230227233001173.png" alt="image-20230227233001173"></p>
<p>方案二：在 Executor 所在所有节点部署任务所需脚本和应用。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/01/09/Azkaban%E6%A1%88%E4%BE%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/09/Azkaban%E6%A1%88%E4%BE%8B/" class="post-title-link" itemprop="url">Azkaban案例</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-09 22:02:49" itemprop="dateCreated datePublished" datetime="2022-01-09T22:02:49+08:00">2022-01-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Work-Flow案例"><a href="#Work-Flow案例" class="headerlink" title="Work Flow案例"></a>Work Flow案例</h1><h2 id="Hello-World案例"><a href="#Hello-World案例" class="headerlink" title="Hello World案例"></a>Hello World案例</h2><ol>
<li><p>在 windows 环境， 新建 azkaban.project 文件， 编辑内容如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">azkaban-flow-version: 2.0</span><br></pre></td></tr></table></figure>

<p>注意：该文件作用，是采用新的 Flow-API 方式解析 flow 文件。</p>
</li>
<li><p>新建 basic.flow 文件，内容如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">\- name: jobA</span><br><span class="line">type: command</span><br><span class="line">config:</span><br><span class="line">command: echo &quot;Hello World&quot;</span><br></pre></td></tr></table></figure>

<p>（1） Name： job 名称<br>（2） Type： job 类型。 command 表示你要执行作业的方式为命令<br>（3） Config： job 配置</p>
</li>
<li><p>将 azkaban.project、 basic.flow 文件压缩到一个 zip 文件(first.zip)，文件名称必须是英文。  </p>
</li>
<li><p>在 WebServer 新建项目： <a target="_blank" rel="noopener" href="http://node1:8081/index">http://node1:8081/index</a>  </p>
</li>
<li><p>给项目名称命名和添加项目描述  </p>
</li>
<li><p>上传文件 first.zip  </p>
</li>
<li><p>执行任务流  </p>
</li>
<li><p>在日志文件中，查看运行结果</p>
</li>
</ol>
<h2 id="作业依赖案例"><a href="#作业依赖案例" class="headerlink" title="作业依赖案例"></a>作业依赖案例</h2><p>需求： JobA 和 JobB 执行完了，才能执行 JobC<br>具体步骤：  </p>
<ol>
<li><p>修改 basic.flow 为如下内容  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">- name: jobC</span><br><span class="line">type: command</span><br><span class="line"># jobC 依赖 JobA 和 JobB</span><br><span class="line">dependsOn:</span><br><span class="line">- jobA</span><br><span class="line">- jobB</span><br><span class="line">config:</span><br><span class="line">command: echo &quot;I’m JobC&quot;</span><br><span class="line">- name: jobA</span><br><span class="line">type: command</span><br><span class="line">config:</span><br><span class="line">command: echo &quot;I’m JobA&quot;</span><br><span class="line">- name: jobB</span><br><span class="line">type: command</span><br><span class="line">config:</span><br><span class="line">command: echo &quot;I’m JobB&quot;</span><br></pre></td></tr></table></figure>

<p>dependsOn：作业依赖  </p>
</li>
<li><p>将修改后的 basic.flow 和 azkaban.project 压缩成 second.zip 文件  </p>
</li>
<li><p>重复 HelloWorld 案例后续步骤</p>
</li>
</ol>
<h2 id="自动失败重试案例"><a href="#自动失败重试案例" class="headerlink" title="自动失败重试案例"></a>自动失败重试案例</h2><p>需求：如果执行任务失败，需要重试 3 次，重试的时间间隔 10000ms<br>具体步骤：</p>
<ol>
<li><p>编译配置流  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">- name: JobA</span><br><span class="line">type: command</span><br><span class="line">config:</span><br><span class="line">command: sh /not_exists.sh</span><br><span class="line">retries: 3</span><br><span class="line">retry.backoff: 10000</span><br></pre></td></tr></table></figure>

<p>参数说明：<br>retries：重试次数<br>retry.backoff：重试的时间间隔  </p>
</li>
<li><p>将修改后的 basic.flow 和 azkaban.project 压缩成 four.zip 文件  </p>
</li>
<li><p>重复 HelloWorld 案例后续步骤  </p>
</li>
<li><p>执行并观察到一次失败+三次重试  </p>
</li>
<li><p>也可以在 Flow 全局配置中添加任务失败重试配置，此时重试配置会应用到所有 Job。  </p>
<p>案例如下：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">config:</span><br><span class="line">retries: 3</span><br><span class="line">retry.backoff: 10000</span><br><span class="line">nodes:</span><br><span class="line">- name: JobA</span><br><span class="line">type: command</span><br><span class="line">config:</span><br><span class="line">command: sh /not_exists.sh</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="手动失败重试案例"><a href="#手动失败重试案例" class="headerlink" title="手动失败重试案例"></a>手动失败重试案例</h2><p>需求： JobA&#x3D;》 JobB（依赖于 A） &#x3D;》 JobC&#x3D;》 JobD&#x3D;》 JobE&#x3D;》 JobF。生产环境，任何 Job 都<br>有可能挂掉，可以根据需求执行想要执行的 Job。<br>具体步骤：</p>
<ol>
<li><p>编译配置流  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">- name: JobA</span><br><span class="line">type: command</span><br><span class="line">config:</span><br><span class="line">command: echo &quot;This is JobA.&quot;</span><br><span class="line">- name: JobB</span><br><span class="line">type: command</span><br><span class="line">dependsOn:</span><br><span class="line">- JobA</span><br><span class="line">config:</span><br><span class="line">command: echo &quot;This is JobB.&quot;</span><br><span class="line">- name: JobC</span><br><span class="line">type: command</span><br><span class="line">dependsOn:</span><br><span class="line">- JobB</span><br><span class="line">config:</span><br><span class="line">command: echo &quot;This is JobC.&quot;</span><br><span class="line">- name: JobD</span><br><span class="line">type: command</span><br><span class="line">dependsOn:</span><br><span class="line">- JobC</span><br><span class="line">config:</span><br><span class="line">command: echo &quot;This is JobD.&quot;</span><br><span class="line">- name: JobE</span><br><span class="line">type: command</span><br><span class="line">dependsOn:</span><br><span class="line">- JobD</span><br><span class="line">config:</span><br><span class="line">command: echo &quot;This is JobE.&quot;</span><br><span class="line">- name: JobF</span><br><span class="line">type: command</span><br><span class="line">dependsOn:</span><br><span class="line">- JobE</span><br><span class="line">config:</span><br><span class="line">command: echo &quot;This is JobF.&quot;</span><br></pre></td></tr></table></figure>


</li>
<li><p>将修改后的 basic.flow 和 azkaban.project 压缩成 five.zip 文件  </p>
</li>
<li><p>重复 HelloWorld 案例后续步骤</p>
<p>Enable 和 Disable 下面都分别有如下参数：<br>Parents：该作业的上一个任务<br>Ancestors：该作业前的所有任务<br>Children：该作业后的一个任务<br>Descendents：该作业后的所有任务<br>Enable All： 所有的任务  </p>
</li>
<li><p>可以根据需求选择性执行对应的任务。</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/01/06/Azkaban%E9%83%A8%E7%BD%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/06/Azkaban%E9%83%A8%E7%BD%B2/" class="post-title-link" itemprop="url">Azkaban部署</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-06 21:12:43" itemprop="dateCreated datePublished" datetime="2022-01-06T21:12:43+08:00">2022-01-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="集群模式安装"><a href="#集群模式安装" class="headerlink" title="集群模式安装"></a>集群模式安装</h1><h2 id="上传tar包"><a href="#上传tar包" class="headerlink" title="上传tar包"></a>上传tar包</h2><ol>
<li><p>将 azkaban-db-3.84.4.tar.gz， azkaban-exec-server-3.84.4.tar.gz， azkaban-webserver-3.84.4.tar.gz 上传到 node1的&#x2F;export&#x2F;software 路径 。</p>
</li>
<li><p>新建&#x2F;export&#x2F;server&#x2F;azkaban 目录，并将所有 tar 包解压到这个目录下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 server]# mkdir azkaban</span><br></pre></td></tr></table></figure>


</li>
<li><p>解压 azkaban-db-3.84.4.tar.gz、 azkaban-exec-server-3.84.4.tar.gz 和 azkabanweb-server-3.84.4.tar.gz到&#x2F;export&#x2F;server&#x2F;azkaban 目录下。  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 azkaban]# tar -zxvf azkaban-db-3.84.4.tar.gz -C ../../server/azkaban/</span><br><span class="line">(base) [root@node1 azkaban]# tar -zxvf azkaban-exec-server-3.84.4.tar.gz -C ../../server/azkaban/</span><br><span class="line">(base) [root@node1 azkaban]# tar -zxvf azkaban-web-server-3.84.4.tar.gz -C ../../server/azkaban/</span><br></pre></td></tr></table></figure>


</li>
<li><p>进入到&#x2F;export&#x2F;server&#x2F;azkaban 目录，依次修改名称。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 azkaban]# mv azkaban-db-3.84.4 azkaban-db</span><br><span class="line">(base) [root@node1 azkaban]# mv azkaban-exec-server-3.84.4 azkaban-exec</span><br><span class="line">(base) [root@node1 azkaban]# mv azkaban-web-server-3.84.4 azkaban-web</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="配置MySql"><a href="#配置MySql" class="headerlink" title="配置MySql"></a>配置MySql</h2><ol>
<li><p>登录MySql</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# mysql -u root -p</span><br></pre></td></tr></table></figure>


</li>
<li><p>创建数据库azkaban</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">create database azkaban;</span></span><br></pre></td></tr></table></figure>



<p>创建 Azkaban 用户，任何主机都可以访问 Azkaban，密码是 000000  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">CREATE USER <span class="string">&#x27;azkaban&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED BY <span class="string">&#x27;000000&#x27;</span>;</span></span><br></pre></td></tr></table></figure>

<p>赋予 Azkaban 用户增删改查权限  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">GRANT SELECT,INSERT,UPDATE,DELETE ON azkaban.* to <span class="string">&#x27;azkaban&#x27;</span>@<span class="string">&#x27;%&#x27;</span> WITH GRANT OPTION;</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>创建 Azkaban 表，完成后退出 MySQL;</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">use azkaban;</span></span><br><span class="line">Database changed</span><br><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash"><span class="built_in">source</span> /export/server/azkaban/azkaban-db/create-all-sql-3.84.4.sql</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>更改 MySQL 包大小；防止 Azkaban 连接 MySQL 阻塞  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# vim /etc/my.cnf</span><br></pre></td></tr></table></figure>

<p>在[mysqld]下面加一行 max_allowed_packet&#x3D;1024M  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">max_allowed_packet=1024M</span><br></pre></td></tr></table></figure>

<p>重启 MySQL  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# systemctl restart mysqld</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="配置Executor-Server"><a href="#配置Executor-Server" class="headerlink" title="配置Executor Server"></a>配置Executor Server</h2><p>Azkaban Executor Server 处理工作流和作业的实际执行。  </p>
<ol>
<li><p>编辑 azkaban.properties  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# vim /export/server/azkaban/azkaban-exec/conf/azkaban.properties</span><br></pre></td></tr></table></figure>

<p>修改如下的属性</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#...</span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">#...</span><br><span class="line">azkaban.webserver.url=http://node1:8081</span><br><span class="line">executor.port=12321</span><br><span class="line">#...</span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=node1</span><br><span class="line">mysql.database=azkaban</span><br><span class="line">mysql.user=azkaban</span><br><span class="line">mysql.password=000000</span><br><span class="line">mysql.numconnections=100</span><br></pre></td></tr></table></figure>


</li>
<li><p>同步 azkaban-exec 到所有节点  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node2 ~]# mkdir /export/server/azkaban</span><br><span class="line">(base) [root@node3 ~]# mkdir /export/server/azkaban</span><br><span class="line">(base) [root@node1 ~]# cd /export/server/azkaban/</span><br><span class="line">(base) [root@node1 azkaban]# scp -r azkaban-exec/ root@node2:$PWD</span><br><span class="line">(base) [root@node1 azkaban]# scp -r azkaban-exec/ root@node3:$PWD</span><br></pre></td></tr></table></figure>


</li>
<li><p>必须进入到&#x2F;export&#x2F;server&#x2F;azkaban&#x2F;azkaban-exec 路径，分别在三台机器上， 启动 executor server</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 azkaban-exec]# ./bin/start-exec.sh </span><br><span class="line">(base) [root@node2 azkaban-exec]# ./bin/start-exec.sh </span><br><span class="line">(base) [root@node3 azkaban-exec]# ./bin/start-exec.sh </span><br></pre></td></tr></table></figure>

<p>注意： 如果在&#x2F;opt&#x2F;module&#x2F;azkaban&#x2F;azkaban-exec 目录下出现 executor.port 文件，说明启动成功  </p>
</li>
<li><p>下面激活 executor，需要  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 azkaban-exec]# curl -G &quot;node1:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">(base) [root@node2 azkaban-exec]# curl -G &quot;node2:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">(base) [root@node3 azkaban-exec]# curl -G &quot;node3:12321/executor?action=activate&quot; &amp;&amp; echo</span><br></pre></td></tr></table></figure>

<p>如果三台机器都出现如下提示，则表示激活成功  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;status&quot;:&quot;success&quot;&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="配置Web-Server"><a href="#配置Web-Server" class="headerlink" title="配置Web Server"></a>配置Web Server</h2><p>Azkaban Web Server 处理项目管理，身份验证，计划和执行触发  </p>
<ol>
<li><p>编辑 azkaban.properties  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# vim /export/server/azkaban/azkaban-web/conf/azkaban.properties </span><br></pre></td></tr></table></figure>

<p>修改如下属性  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">...</span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=hadoop102</span><br><span class="line">mysql.database=azkaban</span><br><span class="line">mysql.user=azkaban</span><br><span class="line">mysql.password=000000</span><br><span class="line">mysql.numconnections=100</span><br><span class="line">...</span><br><span class="line">azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus</span><br></pre></td></tr></table></figure>

<p>说明：<br>#StaticRemainingFlowSize：正在排队的任务数；<br>#CpuStatus： CPU 占用情况<br>#MinimumFreeMemory：内存占用情况。 测试环境， 必须将 MinimumFreeMemory 删除掉， 否则它会认为集群资源不够，不执行。</p>
</li>
<li><p>修改 azkaban-users.xml 文件，添加 st用户  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# vim /export/server/azkaban/azkaban-web/conf/azkaban-users.xml </span><br><span class="line">&lt;azkaban-users&gt;</span><br><span class="line">  &lt;user groups=&quot;azkaban&quot; password=&quot;azkaban&quot; roles=&quot;admin&quot; username=&quot;azkaban&quot;/&gt;</span><br><span class="line">  &lt;user password=&quot;metrics&quot; roles=&quot;metrics&quot; username=&quot;metrics&quot;/&gt;</span><br><span class="line">  &lt;user password=&quot;123456&quot; roles=&quot;admin&quot; username=&quot;st/&gt;</span><br><span class="line"></span><br><span class="line">  &lt;role name=&quot;admin&quot; permissions=&quot;ADMIN&quot;/&gt;</span><br><span class="line">  &lt;role name=&quot;metrics&quot; permissions=&quot;METRICS&quot;/&gt;</span><br><span class="line">&lt;/azkaban-users&gt;</span><br></pre></td></tr></table></figure>


</li>
<li><p>必须进入到 node1的&#x2F;export&#x2F;server&#x2F;azkaban&#x2F;azkaban-web 路径， 启动 web server  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# cd /export/server/azkaban/azkaban-web/</span><br><span class="line">(base) [root@node1 azkaban-web]# ./bin/start-web.sh </span><br></pre></td></tr></table></figure>


</li>
<li><p>访问 <a href="http://node1:8081,并用">http://node1:8081,并用</a> st 用户登陆  </p>
<p><img src="/2022/01/06/Azkaban%E9%83%A8%E7%BD%B2/image-20230225235650708.png" alt="image-20230225235650708"></p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://fhclk.github.io/2022/01/05/Azkaban%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.png">
      <meta itemprop="name" content="fhclk">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒者">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/05/Azkaban%E4%BB%8B%E7%BB%8D/" class="post-title-link" itemprop="url">Azkaban介绍</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-05 22:10:41" itemprop="dateCreated datePublished" datetime="2022-01-05T22:10:41+08:00">2022-01-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>azkaban是LinkedIn开源的批量工作流调度器，可以通过编写类似properties的文件，在里面定义工作的顺序和依赖关系。配置好azkaban后将可以通过web ui的方式，来管理和查看追踪工作流。</p>
<p>官网<a target="_blank" rel="noopener" href="https://azkaban.github.io/">https://azkaban.github.io</a>。</p>
<p>azkaban提供了模块化的可插拔机制，支持command、java、hadoop和hive命令，另外它和任何版本的hadoop均兼容。</p>
<p>下图为官网展示的工作流形象图，定义了工作流后，在执行前可以通过图形化，更加直观的检查工作流地配置是否正确。</p>
<img src="/2022/01/05/Azkaban%E4%BB%8B%E7%BB%8D/1486105-20200203202740156-1491546770.png" alt="img" style="zoom:67%;">

<h1 id="工作流调度系统"><a href="#工作流调度系统" class="headerlink" title="工作流调度系统"></a>工作流调度系统</h1><h2 id="为什么需要工作流调度系统"><a href="#为什么需要工作流调度系统" class="headerlink" title="为什么需要工作流调度系统"></a>为什么需要工作流调度系统</h2><ol>
<li>一个完整的数据分析系统通常都是由大量任务单元组成：Shell 脚本程序，Java 程序，MapReduce 程序、Hive 脚本等。</li>
<li>各任务单元之间存在时间先后及前后依赖关系。</li>
<li>为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行；</li>
</ol>
<p><img src="/2022/01/05/Azkaban%E4%BB%8B%E7%BB%8D/image-20230222232607200.png" alt="image-20230222232607200"></p>
<h2 id="常见工作流调度系统"><a href="#常见工作流调度系统" class="headerlink" title="常见工作流调度系统"></a>常见工作流调度系统</h2><p>简单的任务调度：直接使用 Linux 的 Crontab 来定义；</p>
<p>复杂的任务调度：开发调度平台或使用现成的开源调度系统，比如 Ooize、Azkaban、Airflow、DolphinScheduler 等。</p>
<h2 id="Azkaban-与-Oozie-对比"><a href="#Azkaban-与-Oozie-对比" class="headerlink" title="Azkaban 与 Oozie 对比"></a>Azkaban 与 Oozie 对比</h2><p>Ooize 相比 Azkaban 是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不在意某些功能的缺失，轻量级调度器 Azkaban 是很不错的候选对象。</p>
<h1 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h1><p>azkaban主要由三部分组成，它们之间协同工作。</p>
<p><strong>azkaban web server</strong>：</p>
<p>提供web ui，可以通过web实现对工作流的创建、执行、追踪等，安装后有单独的文件夹对应web server。</p>
<p><strong>azkaban executor server</strong>：</p>
<p>负责工作流和任务的调度提交，安装后也有单独的文件夹对应executor server。</p>
<p><strong>mysql</strong>：</p>
<p>解压对应的安装压缩文件后，需要执行相应脚本创建多张表，用于保存日志、执行计划和项目相关的信息。</p>
<img src="/2022/01/05/Azkaban%E4%BB%8B%E7%BB%8D/1486105-20200203204509602-1836562992.png" alt="img" style="zoom: 50%;">



<h1 id="运行模式"><a href="#运行模式" class="headerlink" title="运行模式"></a>运行模式</h1><p>azkaban有三种运行模式。</p>
<p><strong>solo server mode</strong>：</p>
<p>executor server和web server运行在同一个进程里，数据库使用内置的H2数据库，任务量不大的项目可以使用此模式。</p>
<p><strong>two server mode</strong>：</p>
<p>executor server和web server运行在不同的进程里，数据库使用mysql。</p>
<p><strong>multiple executor mode</strong>： </p>
<p>executor server和web server运行在不同的进程里，但是executor server可以有多个。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  <div>
  
  </div>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="fhclk"
      src="/images/avatar1.png">
  <p class="site-author-name" itemprop="name">fhclk</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">160</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/fhclk" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fhclk" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">fhclk</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/src/clicklove.js"></script>
</body>
</html>
